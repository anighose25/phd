import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
import os
import cPickle as pickle
import json
import sys
import networkx as nx
import numpy as np
import random
import Queue as Q
import collections
import plotly.figure_factory as ff
import plotly.graph_objs as go
import plotly.plotly as py
import datetime
import heapq
from copy import deepcopy
from bokeh.io import show, output_file
from bokeh.plotting import figure
from bokeh.models.graphs import from_networkx
from sklearn import linear_model
from math import sqrt, log, exp



# Auxillary functions and variables
###########################################################################################
# py.sign_in('anighose25', 'nrJZ4ZwpuHlTRV2zlAD1')
CPU_FLOPS = 86.4
# CPU_FLOPS = 179.2
GPU_FLOPS = 515.0
BW = 144.0

dispatch_step = 0

ALL_COLORS = ['rgb(31, 119, 180)', 'rgb(255, 127, 14)', 'rgb(44, 160, 44)', 'rgb(214, 39, 40)', 'rgb(148, 103, 189)',
              'rgb(140, 86, 75)', 'rgb(227, 119, 194)', 'rgb(127, 127, 127)', 'rgb(188, 189, 34)', 'rgb(23, 190, 207)',
              'rgb(217,217,217)', 'rgb(240,2,127)', 'rgb(253,205,172)', 'rgb(179,205,227)', 'rgb(166,86,40)',
              'rgb(51,160,44)', 'rgb(247,129,191)', 'rgb(253,191,111)', 'rgb(190,186,218)', 'rgb(231,41,138)',
              'rgb(166,216,84)', 'rgb(153,153,153)', 'rgb(166,118,29)', 'rgb(230,245,201)', 'rgb(255,255,204)',
              'rgb(102,102,102)', 'rgb(77,175,74)', 'rgb(228,26,28)', 'rgb(217,95,2)', 'rgb(255,255,179)',
              'rgb(178,223,138)', 'rgb(190,174,212)', 'rgb(253,180,98)', 'rgb(255,217,47)', 'rgb(31,120,180)',
              'rgb(56,108,176)', 'rgb(229,216,189)', 'rgb(251,154,153)', 'rgb(222,203,228)', 'rgb(203,213,232)',
              'rgb(188,128,189)', 'rgb(55,126,184)', 'rgb(231,138,195)', 'rgb(244,202,228)', 'rgb(191,91,23)',
              'rgb(128,177,211)', 'rgb(27,158,119)', 'rgb(229,196,148)', 'rgb(253,218,236)', 'rgb(102,166,30)',
              'rgb(241,226,204)', 'rgb(255,127,0)', 'rgb(252,141,98)', 'rgb(227,26,28)', 'rgb(254,217,166)',
              'rgb(141,160,203)', 'rgb(204,235,197)', 'rgb(117,112,179)', 'rgb(152,78,163)', 'rgb(202,178,214)',
              'rgb(141,211,199)', 'rgb(106,61,154)', 'rgb(253,192,134)', 'rgb(255,255,51)', 'rgb(179,226,205)',
              'rgb(127,201,127)', 'rgb(251,128,114)', 'rgb(255,242,174)', 'rgb(230,171,2)', 'rgb(102,194,165)',
              'rgb(255,255,153)', 'rgb(179,179,179)', 'rgb(179,222,105)', 'rgb(252,205,229)', 'rgb(204,204,204)',
              'rgb(242,242,242)', 'rgb(166,206,227)', 'rgb(251,180,174)']

AC = ALL_COLORS

def lineno():
    import inspect
    """Returns the current line number in our program."""
    return inspect.currentframe().f_back.f_lineno

def get_random_integer_around(x, r):
    """
    Returns an integer selected randomly between r*x and (2-r)*x where r takes a value between 0 and 1

    Args:
        x (int): The integer around which we want to generate a random integer 
        r (float): Controls the degree to which we want to

    """

    a = r * x
    b = (2.0 - r) * x
    # print r, x, a, b
    return max(1,np.random.randint(a, b))


def load_regression_model(filename):
    """
    Loads and returns the regression model for predicting execution times for different buffer sizes

    Args:
        filename (str): Name of file containing pickle file of regression model generated by scipy

    Returns:
        pickle object: Pickle object containing the regression model
    """
    loaded_model = pickle.load(open(filename, 'rb'))
    return loaded_model

# regression_model = load_regression_model("Stats/buffer_regression_model.sav")


def extract_dict_from_pickle(dictionary):
    """
    Create dictionary pertaining to some statistics from a pickle file
    Eg: Map between kernel names and execution times
    """
    with open(dictionary, 'rb') as handle:
        test_map = pickle.load(handle)
        return test_map


def make_dict(lines):
    x = dict()
    for line in lines:
        kvs = line.split(' ')
        x[kvs[0]] = kvs[1].strip("\n")
    return x


# print os.path.join(os.path.dirname(__file__), "/Stats/buffer_times.txt")
buffer_times = make_dict(open(os.path.join(os.path.dirname(__file__), 'Stats/buffer_times.txt')).readlines())


def create_feat_dict(key, global_map):
    """
    Returns a dictionary of feature names and feature values for a kernel and worksize pair
    """
    feature_source = "Stats/PART/" + global_map[key][:-6] + "_core2.stats"
    feature_source = os.path.join(os.path.dirname(__file__), feature_source)
    feature_list = open(feature_source, "r").readlines()
    feature_dict = make_dict(feature_list)
    return feature_dict


def get_sizeof(type_name):
    """

    :param type_name:
    :type str:
    :return: size in bytes
    :rtype:
    """
    if type_name == "int":
        return 2.0
    if type_name == "float":
        return 4.0
    if type_name == "double":
        return 8.0


def value_int(partition):
    """
    Returns integer equivalent of partition class value
    """
    if partition == "ZERO":
        return 0
    if partition == "ONE":
        return 1
    if partition == "TWO":
        return 2
    if partition == "THREE":
        return 3
    if partition == "FOUR":
        return 4
    if partition == "FIVE":
        return 5
    if partition == "SIX":
        return 6
    if partition == "SEVEN":
        return 7
    if partition == "EIGHT":
        return 8
    if partition == "NINE":
        return 9
    if partition == "TEN":
        return 10


def partition_class_value(class_string):
    if (value_int(class_string) > 5):
        return "gpu"
    else:
        return "cpu"



def partition_class_absolute(task):
    return value_int(task.Class)

def partition_class(task):
    """
    Returns partition class value of task
    """
    # print task.Class
    if (value_int(task.Class) > 5):
        return "gpu"
    else:
        return "cpu"


def all_processed(dags):
    """
    Checks whether all dags have finished processing
    """
    flag = True
    for dag in dags:
        flag = flag & dag.is_processed()
    return flag


def generate_unique_id():
    """
    Generates and returns a unique id string.
    """
    import uuid
    return str(uuid.uuid1())


def obtain_kernel_info(key):
    """
    Returns kernel object for a kernel name and worksize pair
    """
    # print key
    kernel_info = key.split("_")
    kernelName = kernel_info[0]
    worksize = kernel_info[1].strip("\n")
    if kernelName == 'uncoalesced' or kernelName == 'shared':
        kernelName = kernelName + "_copy"
        worksize = kernel_info[2].strip("\n")
    if kernelName == 'transpose':
        kernelName = kernelName + "_naive"
        worksize = kernel_info[2].strip("\n")
    dataset = int(worksize)
    kernel_info_source = "info/" + kernelName + ".json"
    kernel_info_source = os.path.join(os.path.dirname(__file__), kernel_info_source)
    info = json.loads(open(kernel_info_source).read())
    return Kernel(info, dataset)

def obtain_kernel_dimension(key):
    k = obtain_kernel_info(key)
    return k.work_dimension


def obtain_SimTask_object(key, global_map, ex_map):

    ex_cpu, ex_gpu = ex_map
    feat_dict = create_feat_dict(key, global_map)
    extime = 0.0
    if value_int(feat_dict['Class']) > 5:
        extime = ex_cpu[key]
    else:
        extime = ex_gpu[key]
    simtask = SimTask(key, 0, 0, feat_dict, extime)
    return simtask


#################################################################################################


class Kernel(object):
    """
    Class to handle all operations perfomed on kernel. Same as PySchedCL
    """

    def __init__(self, src, dataset=1024, partition=None, identifier=None):
        self.dataset = dataset
        if 'id' in src:
            self.id = src['id']
        else:
            self.id = generate_unique_id()
        if identifier is not None:
            self.id = identifier
        if 'ecos' in src and str(dataset) in src['ecos']:
            self.eco = src['ecos'][str(dataset)]
        elif 'eco' in src:
            self.eco = src['eco']
        else:
            self.eco = 1
        self.name = src['name']
        self.src = src['src']
        self.partition = src['partition']
        if partition is not None:
            self.partition = partition
        else:
            partition = self.partition
        self.work_dimension = src['workDimension']
        self.global_work_size = src['globalWorkSize']
        if type(self.global_work_size) in [str, unicode]:
            self.global_work_size = eval(self.global_work_size)
        if type(self.global_work_size) is int:
            self.global_work_size = [self.global_work_size]
        if 'localWorkSize' in src:
            self.local_work_size = src['localWorkSize']
        else:
            self.local_work_size = []
        if type(self.local_work_size) in [str, unicode]:
            self.local_work_size = eval(self.local_work_size)
        elif type(self.local_work_size) is int:
            self.local_work_size = [self.local_work_size]
        self.buffer_info = dict()
        if 'inputBuffers' in src:
            self.buffer_info['input'] = src['inputBuffers']
        else:
            self.buffer_info['input'] = []
        if 'outputBuffers' in src:
            self.buffer_info['output'] = src['outputBuffers']
        else:
            self.buffer_info['output'] = []
        if 'ioBuffers' in src:
            self.buffer_info['io'] = src['ioBuffers']
        else:
            self.buffer_info['io'] = []
        self.input_buffers = {'gpu': dict(), 'cpu': dict()}
        self.output_buffers = {'gpu': dict(), 'cpu': dict()}
        self.io_buffers = {'gpu': dict(), 'cpu': dict()}
        self.data = {}
        self.buffer_deps = {}
        if 'varArguments' in src:
            self.variable_args = deepcopy(src['varArguments'])
            self.vargs = src['varArguments']
        else:
            self.variable_args = []
            self.vargs = []
        if 'cpuArguments' in src:
            self.cpu_args = src['cpuArguments']
            print "Ignoring CPU Arguments"
        if 'gpuArguments' in src:
            self.gpu_args = src['gpuArguments']
            print "Ignoring GPU Arguments"
        if 'localArguments' in src:
            self.local_args = src['localArguments']
            for i in range(len(self.local_args)):
                self.local_args[i]['size'] = eval(self.local_args[i]['size'])
        else:
            self.local_args = []
            # self.buffer_info['local'] = deepcopy(self.local_args)
        self.kernel_objects = dict()
        for btype in ['input', 'output', 'io']:
            for i in range(len(self.buffer_info[btype])):
                if type(self.buffer_info[btype][i]['size']) in [str, unicode]:
                    self.buffer_info[btype][i]['size'] = eval(self.buffer_info[btype][i]['size'])
                if 'chunk' in self.buffer_info[btype][i] and type(self.buffer_info[btype][i]['chunk']) in [str,
                                                                                                           unicode]:
                    self.buffer_info[btype][i]['chunk'] = eval(self.buffer_info[btype][i]['chunk'])
                self.buffer_info[btype][i]['create'] = True
                self.buffer_info[btype][i]['enq_write'] = True
                self.buffer_info[btype][i]['enq_read'] = True
                if 'from' in self.buffer_info[btype][i]:
                    self.buffer_deps[self.buffer_info[btype][i]['pos']] = (self.buffer_info[btype][i]['from']['kernel'],
                                                                           self.buffer_info[btype][i]['from']['pos'])

class Cluster(object):
    """
    Class to handle operations over a linear cluster of tasks
    """

    def __init__(self, H, chain_type, task, log):
        self.frontier_task = False
        self.H = H
        self.log=log
        self.tasks = [task]
        self.device_type = ""
        self.device_id = -1
        self.gain = {"cpu": 1.0, "gpu": 1.0}
        self.gain_speedup_times = {"cpu": [], "gpu": []}
        exec_cpu = task.get_first_kernel().ECO/CPU_FLOPS
        exec_gpu = task.get_first_kernel().ECO / GPU_FLOPS + task.get_first_kernel().DT/BW
        self.SL = {"gpu" : [], "cpu": [], "original": []}
        if partition_class_absolute(task) <= 5:
            # print "Class of task is CPU"
            self.SL["original"].append(exec_cpu)
            self.SL["cpu"].append(exec_cpu)
            self.gain_speedup_times["cpu"].append(1.0)
            if exec_cpu < exec_gpu:
                self.SL["gpu"].append(exec_gpu)
                self.gain_speedup_times["gpu"].append(exec_cpu/exec_gpu)
            else:
                 # FV BLOCK
                # fv = task.get_first_kernel().feature_vector
                # if fv in self.log.keys():
                #     if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                #         speedup_factor = self.log[fv]["cpu"]/self.log[fv]["gpu"]
                #         self.SL["gpu"].append(speedup_factor * exec_cpu)
                #         self.gain_speedup_times["gpu"].append(speedup_factor)
                #     else:
                #         self.SL["gpu"].append(-1.0)
                #         self.gain_speedup_times["gpu"].append(-1.0)        
                        
                # #FV BLOCK
                # else:
                self.SL["gpu"].append(-1.0)
                self.gain_speedup_times["gpu"].append(-1.0)
        else:
            # print "Class of task is GPU"
            self.SL["original"].append(exec_gpu)
            self.SL["gpu"].append(exec_gpu)
            self.gain_speedup_times["gpu"].append(1.0)
            if exec_gpu < exec_cpu:
                self.SL["cpu"].append(exec_cpu)
                self.gain_speedup_times["cpu"].append(exec_gpu/exec_cpu)
            else:
                #FV BLOCK
                # fv = task.get_first_kernel().feature_vector
                # if fv in self.log.keys():
                #     if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                #         speedup_factor = self.log[fv]["gpu"]/self.log[fv]["cpu"]
                #         self.SL["cpu"].append(speedup_factor * exec_gpu)
                #         self.gain_speedup_times["cpu"].append(speedup_factor)
                #     else:
                #         self.SL["cpu"].append(-1.0)
                #         self.gain_speedup_times["cpu"].append(-1.0)        


                #FV BLOCK
                # else:
                self.SL["cpu"].append(-1.0)
                self.gain_speedup_times["cpu"].append(-1.0)

        self.gain_history = {"cpu": [], "gpu": []}
      
        if partition_class_absolute(task) <= 5 :
            exec_cpu = task.get_first_kernel().ECO/CPU_FLOPS
            exec_gpu = task.get_first_kernel().ECO / GPU_FLOPS + task.get_first_kernel().DT/BW
            if exec_gpu < exec_cpu:
                self.gain["gpu"] = float("inf")
            else:
                self.gain["gpu"] = exec_cpu - task.get_first_kernel().ECO / GPU_FLOPS

        elif partition_class_absolute(task) > 5:
            exec_cpu = task.get_first_kernel().ECO / CPU_FLOPS
            exec_gpu = task.get_first_kernel().ECO / GPU_FLOPS + task.get_first_kernel().DT / BW
            if exec_gpu > exec_cpu:
                self.gain["cpu"] = float("inf")

            else:
                self.gain["cpu"] = task.get_first_kernel().ECO / GPU_FLOPS - exec_cpu

        self.gain_history["cpu"].append(self.gain["cpu"])
        self.gain_history["gpu"].append(self.gain["gpu"])
        self.dispersion = 0.0
        self.partition_class_values = [partition_class_absolute(task)]
        self.chain_type = chain_type
        self.dag_id = task.dag_id
        self.gain["cpu"] = self.gain_speedup_times["cpu"][-1]
        self.gain["gpu"] = self.gain_speedup_times["gpu"][-1]


    
    def gain_cpu_admissible(self, gain_cpu, gain_type):
        if gain_type == "gain_speedup":
            if gain_cpu > 1.0 and gain_cpu >= self.gain_speedup_times["cpu"][-1]:
                return True
            else:
                return False
        elif gain_type =="simple_gain":
            if gain_cpu < 0:
                return True
            else:
                return False
    
    def gain_gpu_admissible(self, gain_gpu, gain_type):
        if gain_type == "gain_speedup":
            if gain_gpu > 1.0 and gain_gpu >= self.gain_speedup_times["gpu"][-1]:
                return True
            else:
                return False
        elif gain_type =="simple_gain":
            if gain_gpu < 0:
                return True
            else:
                return False

    def calculate_SL_speedup(self, next_task, **kwargs):
        # print lineno(),"CLUSTER SIZE", len(self.tasks)
        # print lineno(),self.get_task_ids()
        # print lineno(),self.SL
        last_task = None
        if self.chain_type == "succ":
            last_task = self.tasks[-1]
        else:
            last_task = self.tasks[0]
        if kwargs:
            if kwargs['merge'] == True:
                last_task = self.tasks[-1]

        nodes = [last_task.get_first_kernel(), next_task.get_first_kernel()]
        kernel_ids = map(lambda k: k.id, nodes)
        # print "NODE ", kernel_ids
        subgraph = self.H.subgraph(kernel_ids)
        u, v = subgraph.edges()[0]
        DT = subgraph[u][v]['weight'] / BW

        exec_cpu_last_task = last_task.get_first_kernel().ECO/CPU_FLOPS
        exec_gpu_last_task = last_task.get_first_kernel().ECO / GPU_FLOPS + last_task.get_first_kernel().DT/BW
        exec_cpu_next_task = next_task.get_first_kernel().ECO/CPU_FLOPS
        exec_gpu_next_task = next_task.get_first_kernel().ECO / GPU_FLOPS + next_task.get_first_kernel().DT/BW
        
        sl_original = 0
        sl_cpu = -1.0
        sl_gpu = -1.0
        gain_cpu = -1.0
        gain_gpu = -1.0
        if partition_class_absolute(next_task) <= 5:
            sl_original = self.SL["original"][-1] + exec_cpu_next_task

        else:
            sl_original = self.SL["original"][-1] + exec_gpu_next_task
        
        if (partition_class_absolute(next_task) > 5 and exec_cpu_next_task > exec_gpu_next_task) or (partition_class_absolute(next_task) < 5 and exec_gpu_next_task > exec_cpu_next_task):
            if self.SL["gpu"][-1] > -1.0:
                sl_gpu = (self.SL["gpu"][-1] + exec_gpu_next_task -2 *DT/BW)
            if self.SL["cpu"][-1] > -1.0:
                sl_cpu = self.SL["cpu"][-1] + exec_cpu_next_task
        else:
            if partition_class_absolute(next_task) > 5 and exec_cpu_next_task < exec_gpu_next_task:
                # Check for feature-vector and execution time mapping
                # fv = next_task.get_first_kernel().feature_vector
                # if fv in self.log.keys():
                    
                #     if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                #         print "Extime Refinement GPU Class but GPU ex time is more", next_task.get_first_kernel().id, next_task.get_first_kernel().name,
                #         print partition_class_absolute(next_task), self.log[fv]["cpu"], self.log[fv]["gpu"]
                #         print "Original ECO MEASURES", exec_cpu_next_task, exec_gpu_next_task
                #         print "Modified ECO MEASURES for", len(self.tasks), self.log[fv]["cpu"]/self.log[fv]["gpu"] * exec_gpu_next_task 
                #         if self.SL["cpu"][-1] > -1.0 :
                #             sl_cpu = self.SL["cpu"][-1] + self.log[fv]["cpu"]/self.log[fv]["gpu"] * exec_gpu_next_task
                #         if self.SL["gpu"][-1] > -1.0:
                #             sl_gpu = (self.SL["gpu"][-1] + exec_gpu_next_task -2 *DT/BW)
                #         print "SL Lengths for cluster of ", len(self.tasks), sl_original, sl_cpu, sl_gpu
                        
                #     else:
                #         sl_cpu = -1.0
                # else:
                sl_cpu = -1.0
                # sl_gpu = -1.0

                
            else:
                
                # fv = next_task.get_first_kernel().feature_vector
                # if fv in self.log.keys():
                    
                    # if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                        
                    #     print "Extime Refinement CPU Class but CPU ex time is more", next_task.get_first_kernel().id, next_task.get_first_kernel().name,
                    #     print partition_class_absolute(next_task), self.log[fv]["cpu"], self.log[fv]["gpu"] 
                    #     print "Original ECO MEASURES", exec_cpu_next_task, exec_gpu_next_task
                    #     print "Modified ECO MEASURES", exec_cpu_next_task, self.log[fv]["gpu"]/self.log[fv]["cpu"] * exec_cpu_next_task - 2*DT/BW
                    #     if self.SL["gpu"][-1] > -1.0 :
                    #         sl_gpu = self.SL["gpu"][-1] + self.log[fv]["gpu"]/self.log[fv]["cpu"] * exec_cpu_next_task - 2*DT/BW
                    #     if self.SL["cpu"][-1] > -1.0 :
                    #         sl_cpu = self.SL["cpu"][-1] + exec_cpu_next_task
                    #     print "SL Lengths for cluster of ", len(self.tasks), sl_original, sl_cpu, sl_gpu
                    # else:
                    #     sl_gpu = -1.0
                # else:
                sl_gpu = -1.0
                # sl_cpu = -1.0
        
        if sl_gpu == -1.0:
            gain_gpu = -1.0
        else:
            gain_gpu = sl_original/sl_gpu
        
        if sl_cpu == -1.0:
            gain_cpu = -1.0
        else:
            gain_cpu = sl_original/sl_cpu
        # print "GAINS", gain_cpu, gain_gpu
        return gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu




    def check_dispersion(self, next_task):
        pc_values = deepcopy(self.partition_class_values)
        pc_values.append(partition_class_absolute(next_task))
        if np.std(pc_values) < 1.5:
            return True
        return False

    def cluster_size(self):
        return len(self.tasks)

    def print_information(self):
        task_ids = []
        for task in self.tasks:
            task_ids.append(task.get_first_kernel().id)
        print self.chain_type, task_ids, self.device_type, self.device_id, self.SL

    def assign_values(self, gain_cpu, gain_gpu, task, **kwargs):
        
        # print lineno(), "Before adding"
        # print lineno(), self.get_task_ids()
        # print lineno(), self.SL
        if kwargs:
            sl_original =kwargs['sl_original']
            self.SL["original"].append(sl_original)
            sl_cpu =kwargs['sl_cpu']
            self.SL["cpu"].append(sl_cpu)
            sl_gpu =kwargs['sl_gpu']
            self.SL["gpu"].append(sl_gpu)
        self.gain["cpu"] = gain_cpu
        self.gain["gpu"] = gain_gpu
        self.gain_history["cpu"].append(gain_cpu)
        self.gain_history["gpu"].append(gain_gpu)
        self.gain_speedup_times["cpu"].append(gain_cpu)
        self.gain_speedup_times["gpu"].append(gain_gpu)

        
        if self.chain_type == "succ":
            self.tasks.append(task)
        else:
            self.tasks.insert(0, task)
        self.partition_class_values.append(partition_class_absolute(task))
        import numpy as np
        self.dispersion = np.std(self.partition_class_values)
        # print lineno(), "After adding"
        # print lineno(), self.get_task_ids()
        # print lineno(), self.SL

    def get_last_task(self):
        return self.tasks[-1]
        '''
        if self.chain_type == "succ":
            return self.tasks[-1]
        else:
            return self.tasks[0]
        '''
    def remove_last_task(self):
        if self.chain_type =="succ":
            if len(self.tasks) > 0:
                self.tasks.pop()
                self.gain_history["cpu"].pop()
                self.gain_history["gpu"].pop()
                self.SL["original"].pop()
                self.SL["cpu"].pop()
                self.SL["gpu"].pop()
                self.gain_speedup_times["cpu"].pop()
                self.gain_speedup_times["gpu"].pop()
                if len(self.tasks) != 0:
                    self.gain["cpu"] = self.gain_history["cpu"][-1]
                    self.gain["gpu"] = self.gain_history["gpu"][-1]
        else:
            if len(self.tasks) > 0:
                # self.tasks.pop(0)
                self.tasks.pop()
                self.gain_history["cpu"].pop()
                self.gain_history["gpu"].pop()
                self.SL["original"].pop()
                self.SL["cpu"].pop()
                self.SL["gpu"].pop()
                self.gain_speedup_times["cpu"].pop()
                self.gain_speedup_times["gpu"].pop()
                if len(self.tasks) != 0:
                    self.gain["cpu"] = self.gain_history["cpu"][-1]
                    self.gain["gpu"] = self.gain_history["gpu"][-1]





    def get_task_ids(self):
        task_ids = []
        for task in self.tasks:
            task_ids.append(task.get_first_kernel().id)
        return task_ids

    def calculate_gain_value(self,gain_type, next_task, **kwargs):
        # self.print_information()
        gain_cpu = -1.0
        gain_gpu = -1.0
        if gain_type == "gain_speedup":
            if kwargs:
                gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu = self.calculate_SL_speedup(next_task, merge=True)
            else:
                gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu = self.calculate_SL_speedup(next_task)
            return gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu
        else:
            gain_cpu, gain_gpu = self.calculate_gain(next_task)
            return gain_cpu, gain_gpu

    def calculate_gain_merge_chain(self, next_task):
        last_task = self.tasks[0]
        nodes = [last_task.get_first_kernel(), next_task.get_first_kernel()]
        kernel_ids = map(lambda k: k.id, nodes)
        subgraph = self.H.subgraph(kernel_ids)
        u, v = subgraph.edges()[0]
        DT = subgraph[u][v]['weight'] / BW
        gain_cpu = self.gain["cpu"]
        gain_gpu = self.gain["gpu"]

        # Compute gain for cpu

        if partition_class_absolute(next_task) > 5:
            exec_cpu = next_task.get_first_kernel().ECO / CPU_FLOPS
            exec_gpu = next_task.get_first_kernel().ECO / GPU_FLOPS + next_task.get_first_kernel().DT / BW
            if exec_gpu > exec_cpu:
                gain_cpu += float("inf")
            else:
                gain_cpu += next_task.get_first_kernel().ECO / GPU_FLOPS - exec_cpu

            if partition_class_absolute(last_task) >= 5:
                gain_cpu -= 2 * DT / BW
            else:
                gain_cpu -= DT / BW
        else:
            if partition_class_absolute(last_task) >= 5:
                gain_cpu -= DT / BW

        # Compute gain for gpu

        if partition_class_absolute(next_task) < 5:
            exec_cpu = next_task.get_first_kernel().ECO / CPU_FLOPS
            exec_gpu = next_task.get_first_kernel().ECO / GPU_FLOPS + next_task.get_first_kernel().DT / BW
            if exec_gpu < exec_cpu:
                gain_gpu += float("inf")
            else:
                gain_gpu += exec_cpu - next_task.get_first_kernel().ECO / GPU_FLOPS

            if partition_class_absolute(last_task) >= 5:
                gain_gpu -= DT / BW

        else:
            if partition_class_absolute(last_task) >= 5:
                gain_gpu -= 2 * DT / BW
            else:
                gain_gpu -= DT / BW

        return gain_cpu, gain_gpu

    def calculate_gain(self, next_task):
        
        last_task = None
        if self.chain_type == "succ":
            last_task = self.tasks[-1]
        else:
            last_task = self.tasks[0]
        # print "LC_DEBUG", self.tasks, next_task
        nodes = [last_task.get_first_kernel(), next_task.get_first_kernel()]
        # print lineno(), "NODES: ",last_task.get_first_kernel().id, next_task.get_first_kernel().id
        kernel_ids = map(lambda k: k.id, nodes)
        subgraph = self.H.subgraph(kernel_ids)
        u,v = subgraph.edges()[0]
        DT = subgraph[u][v]['weight']/BW
        gain_cpu = self.gain["cpu"]
        gain_gpu = self.gain["gpu"]

        # Compute gain for cpu

        if partition_class_absolute(next_task) > 5:
            exec_cpu = next_task.get_first_kernel().ECO / CPU_FLOPS
            exec_gpu = next_task.get_first_kernel().ECO / GPU_FLOPS + next_task.get_first_kernel().DT / BW

            if exec_gpu > exec_cpu:
                fv = next_task.get_first_kernel().feature_vector
                if fv in self.log.keys():
                    if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                        speedup = self.log[fv]["cpu"]/self.log[fv]["gpu"]
                        exec_gpu = speedup * exec_cpu
                        gain_cpu += exec_gpu - next_task.get_first_kernel().DT - exec_cpu
                    else:
                        gain_cpu += float("inf")

                else:
                    gain_cpu += float("inf")
            else:
                gain_cpu += next_task.get_first_kernel().ECO / GPU_FLOPS - exec_cpu

            if partition_class_absolute(last_task) >= 5:
                gain_cpu -= 2*DT/BW
            else:
                gain_cpu -= DT/BW
        else:
            if partition_class_absolute(last_task) >= 5:
                gain_cpu -= DT/BW

        # Compute gain for gpu

        if partition_class_absolute(next_task) < 5:
            exec_cpu = next_task.get_first_kernel().ECO / CPU_FLOPS
            exec_gpu = next_task.get_first_kernel().ECO / GPU_FLOPS + next_task.get_first_kernel().DT / BW
            if exec_gpu < exec_cpu:
                fv = next_task.get_first_kernel().feature_vector
                if fv in self.log.keys():
                    if self.log[fv]["cpu"] > 0.0 and self.log[fv]["gpu"] > 0.0:
                        speedup = self.log[fv]["gpu"] / self.log[fv]["cpu"]
                        exec_cpu = speedup * exec_gpu
                        gain_gpu += exec_cpu - next_task.get_first_kernel().DT - exec_gpu
                    else:
                        gain_gpu += float("inf")
                else:
                    gain_gpu += float("inf")
            else:
                gain_gpu += exec_cpu - next_task.get_first_kernel().ECO / GPU_FLOPS

            if partition_class_absolute(last_task) >= 5:
                gain_gpu -= DT/BW

        else:
            if partition_class_absolute(last_task) >= 5:
                gain_gpu -= 2 * DT/BW
            else:
                gain_gpu -= DT / BW

        return gain_cpu, gain_gpu



class SimTask(object):
    """
    Class to handle all operations perfomed on simulation tasks.
    """

    def __init__(self, key="", uid=-1, dag_id=-1, feature_dict=dict(), extime=-1):
        self.id = uid
        self.dispatch_step = 0
        self.dag_id = dag_id
        self.name = key
        self.execution_time = extime
        self.projected_ex_time = extime
        self.estimated_ex_time = 0.0
        if any(feature_dict):
            self.Float = float(feature_dict['Float16'])
            self.Float4 = float(feature_dict['Float32'])
            self.Float4 = float(feature_dict['Float32'])
            self.Int = float(feature_dict['Int16'])
            self.Int4 = float(feature_dict['Int32'])
            # self.DT = float(feature_dict['DataTransfer'])
            self.DT = 0.0
            self.Barrier = float(feature_dict['Barrier'])
            self.ComputePerDT = float(feature_dict['ComputePerDataTransfer'])
            self.Branches = float(feature_dict['TotalBranches'])
            self.Memory = float(feature_dict['TotalMemory'])
            self.WorkItems = float(feature_dict['NumberOfWorkItems'])
            self.Memory = float(feature_dict['TotalMemory'])
            self.CMRatio = float(feature_dict['ComputeToMemoryRatio'])
            # self.ECO = self.WorkItems*((self.Int + self.Int4 + self.Float + self.Float4) * self.DT/ self.WorkItems)
            # self.ECO = ((self.Int + self.Int4 + self.Float + self.Float4)) * self.WorkItems

            self.Class = feature_dict['Class']
        self.rank = 0
        self.start_time = 0
        self.finish_time = 0
        self.device_type = ""
        self.device_id = 0
        self.is_finished = False
        self.in_frontier = False
        self.contract = True
        if key is not "dummy" and key is not "":
            self.Kernel_Object = obtain_kernel_info(key)
            for buf in self.Kernel_Object.buffer_info['output']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                self.DT += buf_size
                # print "OUTPUT_DT_CALCULATION_"+key, buf_size
            for buf in self.Kernel_Object.buffer_info['input']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                self.DT += buf_size
                # print "INPUT_DT_CALCULATION_" + key, buf_size
            for buf in self.Kernel_Object.buffer_info['io']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                self.DT += buf_size
                # print "IO_DT_CALCULATION_" + key, buf_size
            self.ECO = self.WorkItems * ((self.Int + self.Int4 + self.Float + self.Float4) * self.DT / self.WorkItems)
        self.levels = dict()
        self.rank_values = {'DT_next_level': 0.0, 'upward_rank_ECO_DT': 0.0, 'downward_rank_ECO_DT': 0.0,
                            'upward_rank_EXTime': 0.0, 'downward_rank_EXTime': 0.0, 'percentage_ECO_remaining': 0.0,
                            'percentage_DT_remaining': 0.0, 'blevel': 0.0, 'tlevel': 0.0, 'oct': 0.0}
        self.latest_start_time = 0.0
        # self.feature_vector = []
        # if any(feature_dict):
        #     for feat in feature_dict.keys():
        #         if feat!= "Class":
        #             self.feature_vector.append(float(feature_dict[feat]))
        self.feature_vector = 0.0
        if any(feature_dict):
            for feat in feature_dict.keys():
                if feat != "Class":
                    self.feature_vector += (float(feature_dict[feat]) * float(feature_dict[feat]))

    def __cmp__(self, other):
        return cmp(self.rank, other.rank)

    def get_dataset(self):
        return self.Kernel_Object.dataset

    def get_dimension(self):
        return self.Kernel_Object.work_dimension

    def get_num_input_buffers(self):
        return len(self.Kernel_Object.buffer_info['input'])

    def get_num_output_buffers(self):
        return len(self.Kernel_Object.buffer_info['output'])

    def get_input_buffer_sizes(self):
        input_buf_sizes = []
        for buf in self.Kernel_Object.buffer_info['input']:
            input_buf_sizes.append(buf['size'])
        return input_buf_sizes


    def get_output_buffer_sizes(self):
        output_buf_sizes = []
        for buf in self.Kernel_Object.buffer_info['output']:
            output_buf_sizes.append(buf['size'])
        return output_buf_sizes



    def get_output_buffer_dimension(self):
        dataset = float(self.Kernel_Object.dataset)
        output_buf_size = float(self.Kernel_Object.buffer_info['output']['size'])
        if output_buf_size/dataset > 1:
            return 2
        else:
            return 1




class SimTaskComponent(object):
    """
    Class to handle all operations perfomed on simulation task components.
    """

    def __init__(self, kernel):
        self.id = generate_unique_id()
        self.kernels = set()
        self.sorted_kernels = list()
        self.kernels.add(kernel)
        self.is_horizontal_component = False
        self.is_vertical_component = False
        self.rank_values = {'DT_next_level': 0.0, 'upward_rank_ECO_DT': 0.0, 'downward_rank_ECO_DT': 0.0,
                            'upward_rank_EXTime': 0.0, 'downward_rank_EXTime': 0.0, 'percentage_ECO_remaining': 0.0,
                            'percentage_DT_remaining': 0.0, 'blevel': 0.0, 'tlevel': 0.0, 'oct': 0.0}
        self.rank_name = ""
        self.projected_ex_time = kernel.execution_time
        self.start_time = 0
        self.finish_time = 0
        self.to_be_scheduled = False
        self.future_device_type = ""
        self.future_device_id = -1
        self.device_type = ""
        self.device_id = -1
        self.is_finished = False
        self.dag_id = kernel.dag_id
        self.Class = kernel.Class
        self.contract = True
        self.local_frontier = collections.deque()
        self.number_of_tasks = 0
        self.rank=0
        self.dispersion = 0.0
        self.is_dispatched = False



    # def __gt__(self, other):
    #     return (self.rank_value[self.rank_name] < other.rank_value[other.rank_name])

    def __gt__ (self, other):
        if self is None or other is None:
            return False
        return self.get_first_kernel().rank > other.get_first_kernel().rank

    def is_supertask(self):
        """
        Returns a boolean value indicating whether task component has more than one kernel
        :return:
        :rtype:
        """
        return len(self.get_kernels()) > 1


    def get_first_kernel(self):
        """
        Returns the first kernel (SimTask object)in the task component (indegree zero)
        :return:
        :rtype:
        """
        return list(self.kernels)[0]

    def get_kernels(self):
        """
        Returns all kernels (SimTask objects)
        :return:
        :rtype:
        """
        return self.kernels

    def get_component_time(self,ex_map,device):
        ex_cpu,ex_gpu = ex_map
        t=0.0
        for k in self.kernels:
            if device=="cpu":
                t+=ex_cpu[k.name]
            else:
                t+=ex_gpu[k.name]
        return t

    def get_kernel_ids(self):
        """
        Return list of ids pertaining to each kernel (SimTask Object)
        :return:
        :rtype:
        """
        return map(lambda k: k.id, self.get_kernels())

    def get_kernel_classes(self):
        """
        Returns list of Partition Classes pertaining to each kernel (SimTask Object)
        :return:
        :rtype:
        """
        return map(lambda k: k.Class, self.get_kernels())

    def get_kernel_pvalues(self):
        """
        Returns list of Partition Classes pertaining to each kernel (SimTask Object)
        :return:
        :rtype:
        """
        return map(lambda k: partition_class_absolute(k), self.get_kernels())

    def get_kernel_names(self):
        """
        Returns list of names pertaining to each kernel (SimTask Object)
        :return:
        :rtype:
        """
        return map(lambda k: k.name, self.get_kernels())

    def get_kernels_sorted(self, dag):
        """
        Returns list of kernels (SimTask objects) in topologically sorted order
        :param dag:
        :type dag:
        :return:
        :rtype:
        """
        return map(lambda kid: dag.tasks[kid],
                   nx.algorithms.topological_sort(dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))))

    def get_free_kernels(self, dag):
        subgraph = dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))
        free_task_ids = []
        for i in subgraph.nodes():
            if subgraph.in_degree(i) == 0:
                free_task_ids.append(i)
        free_tasks = []
        for task_id in free_task_ids:
            free_tasks.append(dag.tasks[task_id])
        return free_tasks


    def top(self, dag):
        subgraph = dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))
        free_task_ids = []
        for i in subgraph.nodes():
            if subgraph.in_degree(i) == 0:
                free_task_ids.append(i)
        free_tasks = []
        for task_id in free_task_ids:
            free_tasks.append(dag.tasks[task_id])
        return free_tasks

    def btm(self, dag):
        subgraph = dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))
        free_task_ids = []
        for i in subgraph.nodes():
            if subgraph.out_degree(i) == 0:
                free_task_ids.append(i)
        free_tasks = []
        for task_id in free_task_ids:
            free_tasks.append(dag.tasks[task_id])
        return free_tasks

    def TL(self, dag):
        subgraph = dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))
        max_start_time = 0.0
        free_task_ids = []
        for i in subgraph.nodes():
            if subgraph.in_degree(i) == 0:
                free_task_ids.append(i)
        for task_id in free_tasks:
            max_start_time = max(max_start_time, dag.tasks[task_id].rank_values['tlevel'])
        return max_start_time

    def BL(self, dag):
        subgraph = dag.get_skeleton_subgraph(map(lambda k: k.id, self.get_kernels()))
        max_start_time = 0.0
        free_task_ids = []
        for i in subgraph.nodes():
            if subgraph.out_degree(i) == 0:
                free_task_ids.append(i)
        for task_id in free_tasks:
            max_start_time = max(max_start_time, dag.tasks[task_id].rank_values['blevel'])
        return max_start_time

    def LV(self, dag):
        return self.LV(dag) + self.BL(dag)


    def get_kernel_ids_sorted(self, dag):
        """
        Returns list of kernel ids (SimTask objects) in topologically sorted order
        :param dag:
        :type dag:
        :return:
        :rtype:
        """
        return map(lambda k: k.id, self.get_kernels_sorted(dag))

    def add_kernels_from_task(self, task):
        """
        Merges a child task into itself.
        """
        self.kernels.update(task.get_kernels())

    def remove_kernel(self, kernel):
        """
        Removes given kernel from this task.
        """
        if kernel in self.kernels:
            self.kernels.remove(kernel)
        else:
            raise Exception("Given kernel is not a subset of this task")


class SimTaskDAG(object):
    """
    Class to handle all operations on Simulation DAG
    """

    def __init__(self, task_dict, dag, dag_id, ex_map, ml_classifier=None,name=None,deadline=False):
        self.starting_time = 0.0
        self.finishing_time = 0.0
        self.ex_map = ex_map
        self.dag_id = dag_id
        self.tasks = task_dict
        self.task_components = dict()
        self.skeleton = dag
        self.finished_tasks = list()
        self.free_task_components = list()
        self.processing_tasks = list()
        self.ranks = dict()
        self.task_component_mappings = dict()
        self.task_component_id_map = dict()
        self.num_nodes = nx.number_of_nodes(dag)
        self.accuracy = 0.0
        self.job_id = 0.0
        self.period=0.0
        self.rate=0.0
        self.rates = []
        self.release=0.0
        self.deadline=0.0
        self.wcet=0.0
        self.phase=0.0
        self.fused_kernel_timings={}
        self.adas=False
        # print "TOTAL_NODES ", self.skeleton.nodes()
        self.available_device_lookahead = {}
        self.currently_executing = {}
        for edge in self.skeleton.edges():
            global buffer_times
            u, v = edge
            k_source = self.tasks[u].Kernel_Object
            k_target = self.tasks[v].Kernel_Object
            size_output_buf = 0
            time_output_buf = 0
            for buf in k_source.buffer_info['output']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                size_output_buf += buf_size
                time_output_buf += float(buffer_times[str(int(buf_size))])
            size_input_buf = 0
            time_input_buf = 0
            for buf in k_target.buffer_info['input']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                size_input_buf += buf_size
                time_input_buf += float(buffer_times[str(int(buf_size))])





            # if self.tasks[v].name == "bicg1_128" and self.tasks[u].name == "bicg1_128":
            #     print "GRAPH_EDGE_WEIGHT_TARGET ","bicg1_128", size_output_buf, size_input_buf


            self.skeleton[u][v]['weight'] = min(size_output_buf, size_input_buf)

            self.skeleton[u][v]['time'] = min(time_output_buf, time_input_buf)
            # print self.skeleton[u][v]['weight']
            # print self.skeleton[u][v]['time']

        # mapping = lambda s: SimTaskComponent(self.tasks[s])
        self.G = nx.relabel_nodes(self.skeleton, lambda s: SimTaskComponent(self.tasks[s]), copy=True)
        for task_component in self.G.nodes():
            for kid in task_component.get_kernel_ids():
                self.task_components[kid] = task_component
            if not self.get_task_component_parents(task_component):
                self.free_task_components.append(task_component)
        for task_component in self.G.nodes():
            self.task_component_id_map[task_component.id] = task_component

        self.name = None

        if name:
            self.name = name

        if deadline:
            self.calculate_wcet()
            self.get_available_rates()
            self.deadline=self.release+self.wcet
            self.adas=True

        if ml_classifier:
            ml_classifier.train_classifier("RandomForest")
            
            target_class_map = ml_classifier.predict_partition_classes()
            for node in self.skeleton.nodes():
                simtask = self.tasks[node]
                # print "Changing class of ", simtask.name, "from ", simtask.Class ,"to",
                if target_class_map[simtask.name] == "CPU":
                    simtask.Class = "ZERO"
                    self.task_components[node].Class = "ZERO"
                    # print simtask.Class, self.task_components[node].Class
                else:
                    simtask.Class = "TEN"
                    self.task_components[node].Class = "TEN"
                    # print simtask.Class,self.task_components[node].Class
            self.accuracy = ml_classifier.accuracy
        
    
    def modify_partition_classes(self,accuracy_file):
        # print "Modifying ",accuracy_file
        target_class_map = {}
        # file_name = open("GraphsNewAccuracy/accuracy_" + self.name + ".stats" ,'r').readlines()
        file_name = open(accuracy_file,'r')
        for line in file_name:
            k, p = line.strip("\n").split(":")
            target_class_map[k] = p

        for node in self.skeleton.nodes():
            simtask = self.tasks[node]
            # print "Changing class of ", simtask.name, "from ", simtask.Class ,"to",
            if target_class_map[simtask.name] == "CPU":
                simtask.Class = "ZERO"
                self.task_components[node].Class = "ZERO"
                # print simtask.Class, self.task_components[node].Class
            else:
                simtask.Class = "TEN"
                self.task_components[node].Class = "TEN"
                # print simtask.Class,self.task_components[node].Class
        
    
    @staticmethod
    def make_levels(G):
        """
        :param G:
        :type G: nx.Digraph
        :return levels: list
        """
        import networkx as nx
        node_level = dict()
        levels = dict()
        for node in nx.algorithms.topological_sort(G):
            pred = G.predecessors(node)
            if not pred:
                node_level[node] = 0
            else:
                node_level[node] = min(map(lambda x: node_level[x], pred)) + 1
            try:
                levels[node_level[node]].append(node)
            except KeyError:
                levels[node_level[node]] = [node]
        return map(lambda x: x[1], sorted(levels.items(), key=lambda x: x[0]))

    def contract(self):
        import networkx as nx
        mapping = lambda s: s.id
        G = nx.relabel_nodes(self.G, mapping, copy=True)
        levels = self.make_levels(G)
        n = len(levels)
        for i in xrange(n-1):
            g = G.subgraph(levels[i] + levels[i+1])
            # print g.nodes()
            # g.to_undirected().nodes()
            for component in nx.algorithms.components.connected_components(g.to_undirected()):
                level = self.make_levels(G.subgraph(list(component)))
                r = [self.task_id_map[t] for t in list(component) if not self.G.predecessors(self.task_id_map[t])]
                s = set()
                dt = dict()
                for task in r:
                    s.update(self.get_task_children(task))
                ft = dict()
                s = list(s)
                total_ecos = 0
                for task in r + s:
                    if not task.is_supertask():
                        k = task.get_first_kernel()
                        pt = k.partition
                        ft[task] = max(pt/GPU_FLOPS, (10-pt)/CPU_FLOPS)*k.eco*k.get_num_global_work_items()/10.0
                        total_ecos += k.eco * k.get_num_global_work_items()
                for tr in r:
                    for ts in self.get_task_children(tr):
                        dt[(tr, ts)] = self.task_data_transfer_size(tr, ts)/GPU_BANDWIDTH
                DT_GPU_diff = sum(
                    [dt[(tr, ts)] * (ts.partition) / 5.0 for tr in r for ts in self.get_task_children(tr)])
                DT_CPU_diff = sum(
                    [dt[(tr, ts)] * (ts.partition - 5) / 5.0 for tr in r for ts in self.get_task_children(tr)])
                fmax = max([ft[ts] + max([ft[tr] for tr in self.get_task_parents(ts)]) for ts in s])
                EX_CPU = total_ecos / CPU_FLOPS
                EX_GPU = total_ecos / GPU_FLOPS
                EX_CPU_diff = EX_CPU - fmax
                EX_GPU_diff = EX_GPU - fmax
                if DT_CPU_diff > EX_CPU_diff and DT_GPU_diff > EX_GPU_diff:
                    DT = sum([dt[(tr, ts)] for tr in r for ts in self.get_task_children(tr)])
                    if DT + EX_GPU > EX_CPU:
                        t1 = r[0]
                        for t2 in r[1:] + s:
                            self.merge_tasks(t1, t2)
                        t1.set_partition(0)
                    else:
                        t1 = r[0]
                        for t2 in r[1:] + s:
                            self.merge_tasks(t1, t2)
                        t1.set_partition(10)
                elif DT_CPU_diff > EX_CPU_diff:
                    t1 = r[0]
                    for t2 in r[1:] + s:
                        self.merge_tasks(t1, t2)
                    t1.set_partition(0)
                elif DT_GPU_diff > EX_GPU_diff:
                    t1 = r[0]
                    for t2 in r[1:] + s:
                        self.merge_tasks(t1, t2)
                    t1.set_partition(10)

    
    def get_max_dt(self):
        dt = 0.0
        for edge in self.skeleton.edges():
            u,v = edge
            dt = max(dt, self.skeleton[u][v]['weight'])
        return dt

    def get_unique_tasks(self):
        unique_tasks = []
        for key in self.tasks.keys():
            unique_tasks.append(self.tasks[key].name) 
        return list(set(unique_tasks))

    
    def get_num_unique_tasks(self):
        return len(self.get_unique_tasks())
   
    @staticmethod

  

    def make_levels(G):
        # G = self.skeleton
        node_level = dict()
        levels = dict()
        for node in nx.algorithms.topological_sort(G):
            pred = G.predecessors(node)
            if not pred:
                node_level[node] = 0
            else:
                node_level[node] = min(map(lambda x: node_level[x], pred)) + 1
            try:
                levels[node_level[node]].append(node)
            except KeyError:
                levels[node_level[node]] = [node]
        return map(lambda x: x[1], sorted(levels.items(), key=lambda x: x[0]))

    def print_device_preferences(self):
        for t in self.tasks:
            print self.tasks[t].name, self.tasks[t].Class
    
    def print_information(self, rank_name):
        print self.skeleton.nodes()
        print self.skeleton.edges()
        for t in self.tasks:
            print str(t) + " " + self.tasks[t].name
        for node in self.G.nodes():
            print node.get_kernel_ids()
            # print node.rank_values[rank_name]
            # print "Ex Time: " + str(node.projected_ex_time)
        for edge in self.G.edges():
            u, v = edge
            print u.get_kernel_ids(),
            print "--->",
            print v.get_kernel_ids()

    def print_cc_information(self, cc):
        i = 1
        for c in cc:
            print "### Component " + str(i) + "###"
            for node in c.nodes():
                print node.get_kernel_ids(),
            print ""
            for edge in c.edges():
                u, v = edge
                print u.get_kernel_ids(),
                print "--->",
                print v.get_kernel_ids()
            i = i + 1

    def calculate_wcet(self):
        wcet= 0.0
        cpu_time = 0.0
        gpu_time = 0.0
        for node in self.skeleton.nodes():
            task=self.tasks[node].name
            ex_cpu,ex_gpu=self.ex_map
            cpu_time +=ex_cpu[task]
            gpu_time +=ex_gpu[task]

            if ex_cpu[task] > ex_gpu[task]:
                # print "CPU",task,ex_cpu[task]
                wcet += ex_cpu[task]
            else:
                # print "GPU",task,ex_gpu[task]
                wcet += ex_gpu[task]
        self.wcet=wcet
        return cpu_time,gpu_time
        # print "CALCULATED WCET", wcet

    
    def get_available_rates(self):
        periods = []
        for r in [0.75,1,1.5]:
            periods.append(6*self.wcet*r)
        rates=[1/p for p in periods]
        self.rates=rates

    def assign_device_lookahead(self, dev_lookahead):
        self.available_device_lookahead = dev_lookahead

    def assign_currently_executing(self, current_execution_status):
        self.currently_executing = current_execution_status

    def print_ranks(self):
        for i in self.skeleton.nodes():
            print  str(i) + " --> " + str(self.ranks[i])
    
    def print_rank_info(self):
        for i in self.skeleton.nodes():
            print "DAG", self.dag_id, "WCET", self.wcet, "DAG RELEASE", self.release, "DAG DEADLINE", self.deadline, "TASK ", str(i) + " LOCAL DEADLINE --> " + str(self.tasks[i].rank)

    def print_task_info(self):
        for i in self.skeleton.nodes():
            print "DAG", self.dag_id, "TASK", self.tasks[i].id, self.tasks[i].start_time, self.tasks[i].finish_time

    def get_source_nodes(self):
        source_nodes = []
        for i in self.skeleton.nodes():
            if (self.skeleton.in_degree(i) == 0):
                source_nodes.append(i)
        return source_nodes

    def get_sink_nodes(self):
        sink_nodes = []
        for i in self.skeleton.nodes():
            if (self.skeleton.out_degree(i) == 0):
                sink_nodes.append(i)
        return sink_nodes

    def add_dummy_node_source(self):
        source_nodes = self.get_source_nodes()
        n = self.num_nodes
        feat_dict = {'Float16': 0.0, 'Float32': 0.0, 'Int16': 0.0, 'Int32': 0.0, 'DataTransfer': 0.0, 'Barrier': 0.0,
                     'ComputePerDataTransfer': 0.0, 'TotalBranches': 0.0, 'TotalMemory': 0.0, 'NumberOfWorkItems': 1.0,
                     'ComputeToMemoryRatio': 0.0, 'Class': 0.0}
        self.tasks[n] = SimTask("dummy", n, self.dag_id, feat_dict, 0)
        self.skeleton.add_node(n)
        for s in source_nodes:
            self.skeleton.add_edge(n, s)

    def remove_dummy_node_source(self):
        n = self.num_nodes
        self.skeleton.remove_node(n)
        del self.tasks[n]

    def remove_dummy_node_exit(self):
        n = self.num_nodes + 1
        self.skeleton.remove_node(n)
        del self.tasks[n]

    def add_dummy_node_exit(self):
        sink_nodes = self.get_sink_nodes()
        n = self.num_nodes + 1
        feat_dict = {'Float16': 0.0, 'Float32': 0.0, 'Int16': 0.0, 'Int32': 0.0, 'DataTransfer': 0.0, 'Barrier': 0.0,
                     'ComputePerDataTransfer': 0.0, 'TotalBranches': 0.0, 'TotalMemory': 0.0, 'NumberOfWorkItems': 1.0,
                     'ComputeToMemoryRatio': 0.0, 'Class': 0.0}
        self.tasks[n] = SimTask("dummy", n, self.dag_id, feat_dict, 0)
        self.skeleton.add_node(n)
        for s in sink_nodes:
            self.skeleton.add_edge(s, n)

    def is_processed(self):
        # print "Finished " + str(len(self.finished_tasks))
        # print "Number of nodes: " + str(len(self.skeleton.nodes()))
                
        rev_top_list = list(nx.topological_sort(self.skeleton))
        rev_top_list.reverse()
        finish_timestamp = 0.0
        if len(self.finished_tasks) == len(self.skeleton.nodes()):
            for node in rev_top_list:
                if self.skeleton.out_degree(node) == 0:
                    finish_timestamp = max(finish_timestamp,self.tasks[node].finish_time)
            self.finishing_time = finish_timestamp    
            return True
        
        return False
        # return len(self.finished_tasks) == len(self.skeleton.nodes())

    def update_rank_values(self, rank_name):
        for node in self.skeleton.nodes():
            self.tasks[node].rank_values[rank_name] = self.tasks[node].rank

    def task_component_data_transfer_size(self, task_component):
        tdt = 0
        k_set = task_component.get_kernels()
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['weight']
        return tdt

    def task_component_data_transfer_time(self, task_component):
        tdt = 0
        k_set = task_component.get_kernels()
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['time']
        return tdt


    def reduce_adas_execution_time(self,task,task_component):
        ex_cpu,ex_gpu=self.ex_map
        if task_component.is_supertask():
            
            task_ids=task_component.get_kernel_ids()
            
            t_gpu=task_component.get_component_time(self.ex_map,"gpu")
            t_fused=self.fused_kernel_timings[tuple(sorted(tuple(task_ids)))]
            factor=ex_gpu[task.name]/float(t_gpu)
            diff=(t_gpu-t_fused)*factor
            # print t_gpu,t_fused,factor,diff
            task.projected_ex_time=ex_gpu[task.name]-diff
            task.execution_time = task.projected_ex_time
            # print "REDUCTION",task.id, task.dag_id, task.projected_ex_time
        else:
            if partition_class_value(task_component.Class) is "cpu":
                task.projected_ex_time=ex_cpu[task.name]
                task.execution_time = task.projected_ex_time
            else:
                task.projected_ex_time=ex_gpu[task.name]
                task.execution_time = task.projected_ex_time
            # print "NO REDUCTION", task.id, task.dag_id, task.projected_ex_time

    def reduce_execution_time(self, task, task_component):

        ex_cpu, ex_gpu = self.ex_map
        # print "EX_TIME_"+str(task.id),ex_cpu[task.name],ex_gpu[task.name]
        predecessors = self.skeleton.predecessors(task.id)
        successors = self.skeleton.successors(task.id)
        ex_time = 0.0
        if partition_class_value(task_component.Class) is "gpu":
            ex_time = ex_gpu[task.name]
        else:
            ex_time = ex_cpu[task.name]

        '''
        if partition_class_value(task_component.Class) is "gpu":

            if value_int(task.Class) > 5:
                ex_time = task.execution_time
            else:
                ex_time = ex_gpu[task.name]
            # ex_time = ex_gpu[task.name]

        else:
            if value_int(task.Class) < 5:
                ex_time = task.execution_time
            else:
                ex_time = ex_cpu[task.name]
        '''
            # ex_time = ex_gpu[task.name]
        dt_time = 0.0
        task_ids = task_component.get_kernel_ids()
        # print "LOOKAHEAD_ESTIMATE_GPU_" +task.name , task.id, task_ids, predecessors, successors
        dt_size = 0.0
        pred_dt_size = 0.0
        succ_dt_size = 0.0
        pred_dt_time = 0.0
        succ_dt_time = 0.0
        for pred in predecessors:
            if pred in task_ids:
                pred_dt_time += self.skeleton[pred][task.id]['time']
                pred_dt_size += self.skeleton[pred][task.id]['weight']
        # print "PRED_LOOKAHEAD_ESTIMATE_GPU_" + task.name, task.id, pred_dt_size
        k_source = self.tasks[task.id].Kernel_Object
        max_dt_size = 0.0
        max_dt_time = 0.0
        for buf in k_source.buffer_info['output']:
            buf_size = float(buf['size']) * get_sizeof(buf['type'])
            max_dt_size += buf_size
            max_dt_time += float(buffer_times[str(int(buf_size))])


        for succ in successors:
            if succ in task_ids:
                succ_dt_time += self.skeleton[task.id][succ]['time']
                succ_dt_size += self.skeleton[task.id][succ]['weight']
                if succ_dt_size >= max_dt_size:
                    succ_dt_size = max_dt_size
                    succ_dt_time = max_dt_time
                    break


        # print "SUCC_LOOKAHEAD_ESTIMATE_GPU_" + task.name, task.id, succ_dt_size
        dt_size = pred_dt_size + succ_dt_size
        dt_time = pred_dt_time + succ_dt_time
        # print "execution time",
        # print task.execution_time
        # print "data transfer time",
        # print dt_time
        # print "Reducing execution time of ",
        # print task.name,
        # print " to ",

        if partition_class_value(task_component.Class) is "gpu":
            task.projected_ex_time = ex_time - dt_time
        else:
            task.projected_ex_time = ex_time


        if partition_class_value(task_component.Class) is "gpu":
            task.estimated_ex_time = task.ECO/GPU_FLOPS + task.DT/BW - dt_size/BW
            # print "EXEC_LOOKAHEAD_ESTIMATE_GPU_" + task.name, task.estimated_ex_time, task.projected_ex_time
            # print "LOOKAHEAD_ESTIMATE_GPU_"+ task.name," Breakup ", task.ECO/GPU_FLOPS, task.DT ,dt_size
        else:
            task.estimated_ex_time = task.ECO/CPU_FLOPS
            # print "EXEC_LOOKAHEAD_ESTIMATE_CPU", task.name,task.estimated_ex_time, task.projected_ex_time


        # print task.projected_ex_time

    def calculate_projected_execution_time(self, task_component):
        global regression_model
        # global buffer_times
        k_set = task_component.get_kernels()
        total_execution_time = sum(map(lambda k: k.execution_time, list(k_set)))
        # print "total execution time " + str(total_execution_time)
        # total_data_transfer = self.task_component_data_transfer_size(task_component)
        # print "total data transfer " + str(total_data_transfer)
        # total_transfer_execution_time = regression_model.predict(total_data_transfer)[0][0]
        total_transfer_execution_time = self.task_component_data_transfer_time(task_component)
        # print "total_transfer_execution_time " + str(total_transfer_execution_time)
        task_component.projected_ex_time = total_execution_time - 2 * total_transfer_execution_time

    # Deprecated

    def task_data_transfer_size(self, task_r, task_s):
        tdt = 0
        k_r = task_r.get_kernels()
        k_s = task_s.get_kernels()
        kernel_ids = map(lambda k: k.id, list(k_r) + list(k_s))
        subgraph = self.get_skeleton_subgraph(kernel_ids)
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['weight']
        return tdt

    def get_task_parents(self, task):
        task_id = task.id
        t_parents = []
        for t in self.skeleton.predecessors(task_id):
            t_parents.append(self.tasks[t])
        return t_parents

    def get_task_children(self, task):
        task_id = task.id
        t_children = []
        for t in self.skeleton.successors(task_id):
            t_children.append(self.tasks[t])
        return t_children

    def get_remaining_tasks(self):
        for id in self.skeleton.nodes():
            if self.tasks[id].is_finished == False:
                print id,
        print "."



    def are_parents_finished(self, task):
        t_parents = self.get_task_parents(task)
        flag = True
        # print "Checking parents of " + str(task.id) + " of dag " + str(self.dag_id) + ": ",
        for t in t_parents:
            # print t.id,
            # print " ",
            # print t.is_finished,
            # print " ",
            flag = flag and t.is_finished
        return flag

    def are_task_component_parents_finished(self, task_component):
        t_parents = self.get_task_component_parents(task_component)
        flag = True
        for t in t_parents:
            flag = flag and t.is_finished
        return flag

    def get_kernel_parent_ids(self, kid):
        """
        Should return a list of kernel ids that are predecessors to given kernel.
        """
        return self.skeleton.predecessors(kid)

    def get_kernel_children_ids(self, kid):
        """
        Should return a list of kernel ids that are successors to given kernel.
        """
        return self.skeleton.successors(kid)

    def get_skeleton_subgraph(self, kernel_ids):
        return self.skeleton.subgraph(kernel_ids)

    def get_task_component_parents(self, task_component):
        return self.G.predecessors(task_component)

    def get_task_component_children(self, task_component):

        # print [(k.get_kernel_ids(),k) for k in self.G.nodes()]
        # print task_component.get_kernel_ids(), task_component
        return self.G.successors(task_component)

    def update_dependencies(self, task_component):
        """
        Updates task dependencies. Call this whenever a task is modified. Adds or remove edges to task dag based on
        skeleton kernel dag for the given task.
        :param task:
        :return:
        """
        p, c = set(self.get_task_component_parents(task_component)), set(
            self.get_task_component_children(task_component))
        pt, ct = set(), set()
        for kid in task_component.get_kernel_ids():
            for pkid in self.get_kernel_parent_ids(kid):
                pt.add(self.task_components[pkid])
            for ckid in self.get_kernel_children_ids(kid):
                ct.add(self.task_components[ckid])
        pt -= set([task_component])
        ct -= set([task_component])
        for t in pt - p:
            self.G.add_edge(t, task_component)
        for t in ct - c:
            self.G.add_edge(task_component, t)
        for t in p - pt:
            self.G.remove_edge(t, task_component)
        for t in c - ct:
            self.G.remove_edge(task_component, t)

    def merge_task_components(self, t1, t2):
        dependencies = set().union(*[set(self.get_kernel_parent_ids(kid)) for kid in t2.get_kernel_ids()])
        # print set(t1.get_kernel_ids()), dependencies
        if set(t1.get_kernel_ids()) >= dependencies:
            t1.add_kernels_from_task(t2)
        else:
            raise Exception('Some dependent kernels are not part of this task.')
        for kid in t2.get_kernel_ids():
            self.task_components[kid] = t1
        self.update_dependencies(t1)
        self.G.remove_node(t2)
        self.task_component_id_map.pop(t2.id)

    def merge_independent_task_components(self, t1, t2):
        # print t2.get_kernel_ids(), t2
        # for node in self.G.nodes():
        #     print node.get_kernel_ids(),node
        t1.add_kernels_from_task(t2)
        for kid in t2.get_kernel_ids():
            self.task_components[kid] = t1
        self.update_dependencies(t1)
        # print "Removing ",t2.get_kernel_ids(), t2
        self.G.remove_node(t2)
        self.task_component_id_map.pop(t2.id)

    def split_kernel_from_task_component(self, kernel, task_component):
        """
        Remove the given kernel from the given task and create a new task from that kernel, update task
        dependencies accordingly. Returns the newly created task.
        """
        task_component.remove_kernel(kernel)
        t = SimTaskComponent(kernel)
        self.G.add_node(t)
        self.task_components[kernel.id] = t
        self.update_dependencies(task_component)
        self.update_dependencies(t)
        return t

    def merge_task_list(self, t):
        # print "Tasks to be merged: ",
        # for task_component in t:
        #     print task_component.get_kernel_names(),
        t1 = t[0]
        for t2 in t[1:]:
            # print "T1 ",
            # print t1.get_kernel_names()
            # print "T2 ",
            # print t2.get_kernel_names()
            self.merge_independent_task_components(t1, t2)
        self.calculate_projected_execution_time(t1)
        return t1

    def get_connected_components_two_level(self, tasks):
        successors = set().union(*[set(self.get_task_component_children(task)) for task in tasks])
        subgraph_nodes = tasks + list(successors)
        print "Component Ids ",
        for t in subgraph_nodes:
            print t.get_kernel_ids(),
        print "\n"
        g = self.G.subgraph(subgraph_nodes)
        # print g.nodes()
        gid = nx.relabel_nodes(g, lambda s: s.id, copy=True)
        # cc = list(nx.connected_component_subgraphs(g))
        cc = []
        for component in nx.connected_components(gid.to_undirected()):
            cc.append(self.G.subgraph([self.task_component_id_map[c] for c in list(component)]))
        return cc

    def get_connected_components_k_level(self, tasks, k):
        subgraph_nodes = tasks
        for i in range(0, k):
            successors = set().union(*[set(self.get_task_component_children(task)) for task in tasks])
            subgraph_nodes = subgraph_nodes + list(successors)
            del tasks[:]
            tasks = list(successors)
        g = self.G.subgraph(subgraph_nodes)
        print g.nodes()
        gid = nx.relabel_nodes(g, lambda s: s.id, copy=True)
        # cc = list(nx.connected_component_subgraphs(g))
        cc = []
        for component in nx.connected_components(gid.to_undirected()):
            cc.append(self.G.subgraph([self.task_component_id_map[c] for c in list(component)]))
        return cc


    def get_next_parent(self, task, first_parent):
        parents = self.get_task_component_parents(task)
        # print "TC_GEN: Parents ",
        # print parents
        sorted(parents, key=lambda x: x.get_first_kernel().rank)
        for parent in parents:
            if parent.get_first_kernel().in_frontier and not parent.is_supertask() and parent is not first_parent:
                return parent



    def construct_component_dl(self, task, component, width, depth):

        if task is None:
            return

        if not task.is_supertask() and task not in component:
            print "TC_GEN : adding task of dag " + str(task.dag_id),
            print task.get_kernel_names(), task.get_kernel_ids()
            component.append(task)

        else:
            return

        for succ in self.get_task_component_children(task):

            t = succ.get_first_kernel()
            print "TC_GEN : investigate parents of successor " + str(t.id)
            if width > 0:
                if len(self.get_task_parents(t)) > 1:
                    parent = self.get_next_parent(succ, task)
                    print parent

                    self.construct_component_dl(parent, component, width-1, depth)
            if depth > 0:
                self.construct_component_dl(succ, component, width, depth-1)

    def construct_component_peek_var(self, task, component, width, depth):
        p_values = []
        component.append(task)
        blacklist_parents = []
        p_values.append(partition_class_absolute(task))
        L1 = [task]
        L2 = []
        level = 0
        d = depth
        while d > 0:

            T = []

            if level % 2 == 0:
                T = L1
            else:
                T = L2
            # print "Before Loop", [k.get_kernel_ids() for k in L1]
            for t in T:
                # print "Successors of ", t.get_kernel_ids()
                # print self.get_task_component_children(t)

                H = []
                mean = np.mean(p_values)
                for succ in self.get_task_component_children(t):
                    heapq.heappush(H, (abs(partition_class_absolute(succ) - mean), succ))
                # print H
                while (len(H) > 0):
                    priority, succ = heapq.heappop(H)

                    flag = 1
                    for parent in self.get_task_component_parents(succ):
                        if parent is not t and parent not in component:
                            if parent.get_first_kernel().in_frontier and not parent.is_supertask() and parent not in blacklist_parents:
                                # component.append(parent)
                                # print "TC_GEN : parent is ok to add ", parent.get_kernel_ids(), parent.dag_id
                                # print parent.get_kernel_names(), parent.get_kernel_ids()
                                pass
                            else:
                                flag = 0
                                # print "TC_GEN : parent is not ok to add (potential deadlock) successor shouldn't be added (parent,successor)", parent.get_kernel_ids(), succ.get_kernel_ids(), parent.dag_id

                    if flag:
                        for parent in self.get_task_component_parents(succ):
                            if parent not in component:
                                p_values.append(partition_class_absolute(parent))
                        p_values.append(partition_class_absolute(succ))
                        if np.std(p_values) >= 1.5:
                            blacklist_parents.append(succ)
                            flag = 0

                    if flag:
                        for parent in self.get_task_component_parents(succ):
                            if parent not in component:
                                component.append(parent)

                        if level % 2 == 0:
                            if succ not in L2:
                                L2.append(succ)
                        else:
                            if succ not in L1:
                                L1.append(succ)
                        if succ not in component:
                            component.append(succ)

            if level % 2 == 0:
                del L1[:]
                # print "After Loop", [k.get_kernel_ids() for k in L2]
            else:
                del L2[:]
                # print "After Loop", [k.get_kernel_ids() for k in L1]
            # print "Loop Component ",[k.get_kernel_ids() for k in component]
            level += 1
            d -= 1

    def construct_component_peek(self, task, component, width, depth):
        p_values = []
        component.append(task)
        L1 = [task]
        L2 = []
        level = 0
        d = depth
        while d > 0:

            T = []

            if level%2 == 0:
                T = L1
            else:
                T = L2
            print "Before Loop", [k.get_kernel_ids() for k in L1]
            for t in T:
                print "Successors of ", t.get_kernel_ids()
                print self.get_task_component_children(t)
                for succ in self.get_task_component_children(t):
                    flag = 1
                    for parent in self.get_task_component_parents(succ):
                        if parent is not t and parent not in component:
                            if parent.get_first_kernel().in_frontier and not parent.is_supertask():
                                # component.append(parent)
                                # print "TC_GEN : parent is ok to add ", parent.get_kernel_ids(), parent.dag_id
                                # print parent.get_kernel_names(), parent.get_kernel_ids()
                                pass
                            else:
                                flag = 0
                                # print "TC_GEN : parent is not ok to add (potential deadlock) successor shouldn't be added (parent,successor)", parent.get_kernel_ids(), succ.get_kernel_ids(), parent.dag_id

                    if flag:
                        for parent in self.get_task_component_parents(succ):
                            if parent not in component:
                                p_values.append(partition_class_absolute(parent))
                        p_values.append(partition_class_absolute(succ))
                        if np.std(p_values) >=1.5:
                            return

                        for parent in self.get_task_component_parents(succ):
                            if parent not in component:
                                component.append(parent)

                        if level%2 == 0:
                            if succ not in L2:
                                L2.append(succ)
                        else:
                            if succ not in L1:
                                L1.append(succ)
                        if succ not in component:
                            component.append(succ)

            if level%2 == 0:
                del L1[:]
                print "After Loop", [k.get_kernel_ids() for k in L2]
            else:
                del L2[:]
                print "After Loop", [k.get_kernel_ids() for k in L1]
            print "Loop Component ",[k.get_kernel_ids() for k in component]
            level +=1
            d -= 1




    def construct_component(self, task, component, width, depth):

        if task is None:
            return

        if not task.is_supertask() and task not in component:
            # print "TC_GEN : adding task of dag " + str(task.dag_id),
            # print task.get_kernel_names(), task.get_kernel_ids()
            p = []
            '''
            for t in component:
                p.append(partition_class_absolute(t))
            print "Dispersion: ", p, np.std(p)
            '''
            component.append(task)

        else:
            return
        # print "Successors of ", task.get_kernel_ids(), self.get_task_component_children(task)
        for succ in self.get_task_component_children(task):
            flag = 1
            for parent in self.get_task_component_parents(succ):
                if parent is not task and parent not in component:
                    if parent.get_first_kernel().in_frontier and not parent.is_supertask():
                        # component.append(parent)
                        # print "TC_GEN : parent is ok to add ", parent.get_kernel_ids(), parent.dag_id
                        # print parent.get_kernel_names(), parent.get_kernel_ids()
                        pass
                    else:
                        flag = 0
                        # print "TC_GEN : parent is not ok to add (potential deadlock) successor shouldn't be added (parent,successor)", parent.get_kernel_ids(), succ.get_kernel_ids(), parent.dag_id

            if flag:
                for parent in self.get_task_component_parents(succ):
                    if parent not in component:
                        component.append(parent)
                    # print "TC_GEN : adding parent task of dag ", parent.get_kernel_ids(), parent.dag_id
                    # print parent.get_kernel_names(), parent.get_kernel_ids()
            if depth > 0 and flag:
                self.construct_component(succ, component, width, depth-1)

    def construct_component_var(self, task, component, dt_max, percent, depth):

        if task is None:
            return

        if not task.is_supertask() and task not in component:
            # print "VAR_TC_GEN : adding task of dag " + str(task.dag_id),
            # print task.get_kernel_names(), task.get_kernel_ids()
            component.append(task)

        else:
            return
        depth_flag = 0
        for succ in self.get_task_component_children(task):
            flag = 1
            s = succ.get_kernel_ids()[0]

            for succ_prime in self.get_task_component_children(succ):
                s_prime = succ_prime.get_kernel_ids()[0]
                dt1 = self.skeleton[s][s_prime]['weight']
                # print "VAR_TC_GEN (s,s') ", dt_max, dt1, self.tasks[s].name, self.tasks[s].id, self.tasks[s_prime].name, self.tasks[s_prime].id
                if dt1 >= percent * dt_max:
                    for succ_prime_prime in self.get_task_component_children(succ_prime):
                        s_prime_prime = succ_prime_prime.get_kernel_ids()[0]
                        dt2 = self.skeleton[s_prime][s_prime_prime]['weight']
                        if dt2 >= dt1:
                            #flag = 0
                            depth += 1
                            depth_flag = 1
                            # print "VAR_TC_GEN (s,s'') ", dt_max, dt1, dt2, self.tasks[s_prime].name, self.tasks[s_prime].id, self.tasks[s_prime_prime].name, self.tasks[s_prime_prime].id
                            break

            for parent in self.get_task_component_parents(succ):
                if parent is not task and parent not in component:
                    if parent.get_first_kernel().in_frontier and not parent.is_supertask():
                        # component.append(parent)
                        # print "TC_GEN : parent is ok to add ", parent.get_kernel_ids(), parent.dag_id
                        # print parent.get_kernel_names(), parent.get_kernel_ids()
                        pass
                    else:
                        flag = 0
                        # print "TC_GEN : parent is not ok to add (potential deadlock) successor shouldn't be added (parent,successor)", parent.get_kernel_ids(), succ.get_kernel_ids(), parent.dag_id

            if flag:
                for parent in self.get_task_component_parents(succ):
                    if parent not in component:
                        component.append(parent)
                    # print "TC_GEN : adding parent task of dag ", parent.get_kernel_ids(), parent.dag_id
                    # print parent.get_kernel_names(), parent.get_kernel_ids()
            if depth > 0 and flag:
                self.construct_component_var(succ, component, dt_max, percent, depth - 1)
        if depth_flag:
            depth -= 1

    def construct_component_set(self, task, component, width, depth):
        print "adding task of dag " + str(task.dag_id),
        print task.get_kernel_names()
        if task not in component:
            component.append(task)
        else:
            return

        for succ in self.get_task_component_children(task):
            if succ not in component:
                t = succ.get_first_kernel()
                if len(self.get_task_parents(t)) > 1:
                    parents = self.get_task_component_parents(succ)
                    immediate_parent = next((p for p in parents if p.get_first_kernel().in_frontier == True and p != task), None)
                    if immediate_parent is not None:
                        print immediate_parent.get_kernel_names()
                    if width > 0 and immediate_parent is not None:
                        print "adding width based parent"
                        self.construct_component(immediate_parent, component, width - 1, depth)

                if depth > 0:
                    print "adding depth based child"

                    self.construct_component(succ, component, width, depth - 1)


    def construct_component_gpu(self, task, component, width, depth):
        print "adding task of dag " + str(task.dag_id),
        print task.get_kernel_names()
        if task in component:
            return

        if task.get_first_kernel().Class == "gpu":
            component.append(task)
            task.contract = False
        else:
            task.contract = False

        for succ in self.get_task_component_children(task):
            if succ not in component:
                t = succ.get_first_kernel()
                if len(self.get_task_parents(t)) > 1:
                    parents = self.get_task_component_parents(succ)
                    immediate_parent = next((p for p in parents if p.get_first_kernel().in_frontier == True), None)
                    if width > 0 and immediate_parent is not None:
                        self.construct_component(immediate_parent, component, width - 1, depth)

                if depth > 0:
                    self.construct_component(succ, component, width, depth - 1)




    def contract_component_extime(self, task, nCPU, mGPU):
        subgraph_nodes = [task]
        for succ in self.get_task_component_children(task):
            t = succ.get_first_kernel()
            if len(self.get_task_parents(t)) <= 1:
                subgraph_nodes.append(succ)
        cpu, gpu = self.get_component_device_affinity(subgraph_nodes)
        print "Device Affinity: CPU: " + str(cpu) + " GPU:  " + str(gpu)
        # percent = float(gpu) / (float(cpu) + float(gpu))
        if len(subgraph_nodes) > 1 and self.decide_contraction_execution_time(subgraph_nodes, nCPU, mGPU) is "gpu":
            subgraph_nodes[0].Class = "TEN"
            return self.merge_task_list(subgraph_nodes)
        elif len(subgraph_nodes) > 1 and self.decide_contraction_execution_time(subgraph_nodes, nCPU, mGPU) is "cpu":
            subgraph_nodes[0].Class = "ZERO"
            return self.merge_task_list(subgraph_nodes)
        else:
            return task





    def adaptive_naive_contraction(self, task, depth, frontier, nCPU, mGPU, dt_max_value, percent_tolerance):
        subgraph_nodes = []
        self.construct_component_var(task, subgraph_nodes, dt_max_value, percent_tolerance, 2)
        print "VAR_TC_TASK_COMPONENT_CREATED ", [k.get_kernel_ids() for k in subgraph_nodes]
        print "VAR_TC_TASK_COMPONENT_CREATED ", [k.get_kernel_names() for k in subgraph_nodes]
        cpu, gpu = self.get_component_device_affinity(subgraph_nodes)
        print "Device Affinity: CPU: " + str(cpu) + " GPU:  " + str(gpu)
        percent = float(gpu) / (float(cpu) + float(gpu))

        if len(subgraph_nodes) > 1 and percent >= 0.5:
            if mGPU == 0:
                return task
            subgraph_nodes[0].Class = "TEN"
            for x in frontier.queue:
                print x, x.get_first_kernel().name
            for node in subgraph_nodes:
                if node.get_first_kernel().in_frontier and node is not task:
                    print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                    frontier.queue.remove(node)
            return self.merge_task_list(subgraph_nodes)
        elif len(subgraph_nodes) > 1 and percent < 0.5:
            if nCPU == 0:
                return task
            subgraph_nodes[0].Class = "ZERO"
            for x in frontier.queue:
                print x, x.get_first_kernel().name
            for node in subgraph_nodes:
                if node.get_first_kernel().in_frontier and node is not task:
                    print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                    frontier.queue.remove(node)
            return self.merge_task_list(subgraph_nodes)
        else:
            return task



    def kernel_fusion(self,task,depth,frontier):
        subgraph_nodes = []
        if depth != -1:
            self.construct_component(task, subgraph_nodes, 0, depth)
        
        if len(subgraph_nodes) > 1:
            for node in subgraph_nodes:
                if node.get_first_kernel().in_frontier and node is not task:
                    # print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                    frontier.queue.remove(node)
            subgraph_nodes[0].Class="TEN"
            return self.merge_task_list(subgraph_nodes)
        else:
            return task


    def naive_contraction(self, task, width, depth, frontier, nCPU, mGPU, dt_max_value, percent_tolerance):
        subgraph_nodes = []
        if depth != -1:
            self.construct_component(task, subgraph_nodes, width, depth)
        else:
            self.construct_component_var(task, subgraph_nodes, dt_max_value, percent_tolerance, 2)
        self.construct_component(task, subgraph_nodes, width, depth)
        # print "Naive TASK_COMPONENT_CREATED ", [k.get_kernel_ids() for k in subgraph_nodes]
        cpu, gpu = self.get_component_device_affinity(subgraph_nodes)
        # print "Device Affinity: CPU: " + str(cpu) + " GPU:  " + str(gpu)
        percent = float(gpu) / (float(cpu) + float(gpu))

        if len(subgraph_nodes) > 1 and percent >= 0.5:
            if mGPU == 0:
                # print "Naive ZERO GPUs returning task"
                return task
            subgraph_nodes[0].Class = "TEN"
            # for x in frontier.queue:
            #     print x, x.get_first_kernel().name
            for node in subgraph_nodes:
                if node.get_first_kernel().in_frontier and node is not task:
                    # print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                    frontier.queue.remove(node)
            return self.merge_task_list(subgraph_nodes)
        elif len(subgraph_nodes) > 1 and percent < 0.5:
            if nCPU == 0:
                # print "Naive ZERO CPUs returning task"
                return task
            subgraph_nodes[0].Class = "ZERO"
            # for x in frontier.queue:
            #     print x, x.get_first_kernel().name
            for node in subgraph_nodes:
                if node.get_first_kernel().in_frontier and node is not task:
                    # print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                    frontier.queue.remove(node)
            return self.merge_task_list(subgraph_nodes)
        else:
            # print "Naive Length of TC is 1"
            return task


    def predecessor_assignment(self, task, task_chain_map, cluster_map, available_devices, log):

        def revert_clusters(cluster_map_predecessors, potential_clusters):

            for C in potential_clusters:
                # print "LC_REVERT_Removing tasks from ",
                # C.print_information()
                T = C.get_last_task()

                while T in cluster_map_predecessors:
                    # print "LC_REVERT_Removing task", T.get_first_kernel().id
                    # print "Before removing", C.SL
                    C.remove_last_task()
                    # print "After removing", C.SL
                    task_chain_map.pop(T)
                    if C.cluster_size() > 0:
                        T = C.get_last_task()
                    else:
                        break

                if C.cluster_size() == 0:
                    dtype = C.device_type
                    dev_id = C.device_id
                    # print "LC_REVERT_DEVUPDATE ", dtype,dev_id
                    available_devices[dtype].append(dev_id)


        # print "predecessor assignment called for", task.get_first_kernel().id

        if len(self.get_task_component_parents(task)) == 1:
            # print "Only one predecessor"
            return True

        cluster_map_predecessors = []
        potential_clusters = []
        predecessor_stack = []
        chain_predecessors = []

        existing_task_ids = []
        for t in task_chain_map.keys():
            existing_task_ids.append(t.get_first_kernel().id)
        # print "LC_DEBUG: Existing task ids in cluster chain maps ", existing_task_ids
        for t in self.get_task_component_parents(task):
            if t not in task_chain_map.keys() and not t.is_supertask() and not t.is_dispatched:
                predecessor_stack.append(t)
        pstack_ids = []
        for debug_p in predecessor_stack:
            pstack_ids.append(debug_p.get_kernel_ids())
        # print lineno(), "EXISTING CLUSTERS"
        # for c in cluster_map:
            # c.print_information()
        # print "LC_DEBUG: Removing Predecessor Stack ", pstack_ids


        while len(predecessor_stack) > 0:
            
            t = predecessor_stack.pop()
            # print "POPPED ", t.get_kernel_ids()
            gain_cpu = 0.0
            gain_gpu = 0.0
            sl_original = 0.0
            sl_cpu = 0.0
            sl_gpu = 0.0

            task_merge = False
            target_task = None
            target_cluster = None
            pred_chain_task = False
            succ_chain_task = False
            target_pred = None
            for pred in self.get_task_component_parents(t):
                if pred in task_chain_map.keys() and not pred.is_supertask():
                    if task_chain_map[pred].chain_type == "pred" and task_chain_map[pred].get_last_task() == pred:
                        # print "POSSIBLE PRED MERGE FOUND ",
                        target_task = t
                        target_pred = pred
                        target_cluster = task_chain_map[pred]
                        # target_cluster.print_information()
                        pred_chain_task = True
                    elif task_chain_map[pred].chain_type == "succ" and task_chain_map[pred].get_last_task() == pred:
                        # print "POSSIBLE SUCC MERGE FOUND ",
                        target_task = t
                        target_pred = pred
                        target_cluster = task_chain_map[pred]
                        # target_cluster.print_information()
                        succ_chain_task = True

                if target_task != None:
                    if pred_chain_task:
                        # gain_cpu, gain_gpu = target_cluster.calculate_gain_merge_chain(target_task)
                        gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu  = target_cluster.calculate_gain_value("gain_speedup", target_task, merge=True)
                    if succ_chain_task:
                        # gain_cpu, gain_gpu = target_cluster.calculate_gain(target_task)
                        gain_cpu, gain_gpu , sl_original, sl_cpu, sl_gpu   = target_cluster.calculate_gain_value("gain_speedup", target_task, merge=True)

                    # if gain_cpu < 0 and target_cluster.device_type == "cpu":
                    if target_cluster.gain_cpu_admissible(gain_cpu, "gain_speedup") and target_cluster.device_type == "cpu":

                        if pred_chain_task:
                            # target_cluster.tasks.insert(0, target_task)
                            target_cluster.tasks.append(target_task)
                        if succ_chain_task:
                            target_cluster.tasks.append(target_task)
                        target_cluster.gain["cpu"] = gain_cpu
                        target_cluster.gain["gpu"] = gain_gpu

                        target_cluster.gain_history["cpu"].append(gain_cpu)
                        target_cluster.gain_history["gpu"].append(gain_gpu)
                       
                        target_cluster.SL["cpu"].append(sl_cpu)
                        target_cluster.SL["gpu"].append(sl_gpu)
                        target_cluster.SL["original"].append(sl_original)
                        target_cluster.gain_speedup_times["cpu"].append(gain_cpu)
                        target_cluster.gain_speedup_times["gpu"].append(gain_gpu)

                        task_chain_map[target_task] = target_cluster
                        cluster_map_predecessors.append(target_task)
                        if target_cluster not in potential_clusters:
                            potential_clusters.append(target_cluster)
                        # print "BOOLEAN VALUES", pred_chain_task, succ_chain_task
                        # print "GAIN CPU,", 
                        # target_cluster.print_information()
                        task_merge = True

                    elif target_cluster.gain_gpu_admissible(gain_gpu, "gain_speedup") and target_cluster.device_type == "gpu":
                        if pred_chain_task:
                            # target_cluster.tasks.insert(0, target_task)
                            target_cluster.tasks.append(target_task)
                        if succ_chain_task:
                            target_cluster.tasks.append(target_task)
                        
                        target_cluster.gain["cpu"] = gain_cpu
                        target_cluster.gain["gpu"] = gain_gpu
                        target_cluster.gain_history["cpu"].append(gain_cpu)
                        target_cluster.gain_history["gpu"].append(gain_gpu)

                        target_cluster.SL["cpu"].append(sl_cpu)
                        target_cluster.SL["gpu"].append(sl_gpu)
                        target_cluster.SL["original"].append(sl_original)
                        target_cluster.gain_speedup_times["cpu"].append(gain_cpu)
                        target_cluster.gain_speedup_times["gpu"].append(gain_gpu)

                        task_merge = True

                        task_chain_map[target_task] = target_cluster
                        cluster_map_predecessors.append(target_task)
                        if target_cluster not in potential_clusters:
                            potential_clusters.append(target_cluster)
                        # print "BOOLEAN VALUES", pred_chain_task, succ_chain_task
                        # print "GAIN GPU,", 
                        # target_cluster.print_information()
                if task_merge:
                    # print "TASK MERGE SUCCESSFUL"
                    for pred in self.get_task_component_parents(t):
                        if pred not in task_chain_map.keys() and pred != target_pred and not pred.is_supertask() and not pred.is_dispatched:
                            # print "ADDING TO PREDECESSOR STACK", pred.get_first_kernel().id
                            predecessor_stack.append(pred)
                    
                    # print "LC_DEBUG: Updated Predecessor Stack ", [debug_p.get_first_kernel().id for debug_p in predecessor_stack]

                            
                    break
                target_task = None
                target_cluster = None
                pred_chain_task = False
                succ_chain_task = False
                target_pred = None


            if not task_merge:

                # print "TASK NOT MERGEABLE ",t.get_first_kernel().id

                C = Cluster(self.skeleton, "pred", t, log)
                gain_cpu = C.gain["cpu"]
                gain_gpu = C.gain["gpu"]
                sl_original  = C.SL["original"]
                sl_cpu = C.SL["cpu"]
                sl_gpu = C.SL["gpu"]
                if t.get_first_kernel().in_frontier:
                    # print "LC_DEBUG_Setting frontier task to true" ,t.get_first_kernel().id
                    C.frontier_task = True

                
                # if gain_cpu < 0 and len(available_devices["cpu"]) > 0:
                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and len(available_devices["cpu"]) > 0:
                    C.device_type = "cpu"
                    C.device_id = available_devices["cpu"].popleft()
                    if C not in potential_clusters:
                        # print lineno(), "Adding to potential cluster"
                        task_chain_map[t] = C
                        potential_clusters.append(C)
                elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and len(available_devices["gpu"]) > 0:
                    C.device_type = "gpu"
                    C.device_id = available_devices["gpu"].popleft()
                    if C not in potential_clusters:
                        # print lineno(), "Adding to potential cluster"
                        task_chain_map[t] = C
                        potential_clusters.append(C)
                else:
                    # print "LC_DEBUG_REVERT Calling Revert since predecessor cannot be assigned"
                    revert_clusters(cluster_map_predecessors, potential_clusters)
                    return False

                cluster_map_predecessors.append(t)
                # cluster_map[t] = C
                while(not t.get_first_kernel().in_frontier):

                    predecessors = []
                    # print lineno(), predecessors, "PREDECESSORS OF " ,t.get_kernel_ids()
                    for pred in self.get_task_component_parents(t):
                        # print lineno(), "PARENT",
                        
                        if pred not in task_chain_map.keys() and not pred.is_supertask() and not pred.is_dispatched:
                            # print pred.get_kernel_ids(),
                            heapq.heappush(predecessors, (-pred.get_first_kernel().rank, pred))
                    # print "."
                    target_task = None
                    predecessor_not_found = True
                    # print lineno(), "PREDECESSORS ",[task.get_first_kernel().id for prio, task in predecessors]
                    while len(predecessors) > 0 and predecessor_not_found:
                        priority, pred_task = heapq.heappop(predecessors)
                        # print lineno(), C.get_task_ids()
                        # gain_cpu, gain_gpu = C.calculate_gain(pred_task)
                        gain_cpu, gain_gpu,  sl_original, sl_cpu, sl_gpu   = C.calculate_gain_value("gain_speedup", pred_task)
                        # print lineno(), "GAINS ", gain_cpu, gain_gpu
                        #if gain_cpu > 0 and gain_gpu > 0:
                        if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == False and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == False:
                            continue
                        else:
                            dtype = ""
                            dev_id = -1
                            # if gain_cpu < 0 and gain_gpu < 0:
                            if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == True and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == True:
                                if gain_cpu > gain_gpu:
                                    if C.device_type =="cpu" or len(available_devices["cpu"]) > 0:

                                        dtype = "cpu"

                                        if C.device_type != dtype:

                                            dev_id = available_devices["cpu"].popleft()
                                            # print "Removing from PredBlock",line_no(), dev_id, dtype
                                            available_devices[C.device_type].append(C.device_id)
                                            C.device_type = dtype
                                            C.device_id = dev_id

                                        predecessor_not_found = False

                                        cluster_map_predecessors.append(pred_task)

                                else:
                                    if C.device_type =="gpu" or len(available_devices["gpu"]) > 0:
                                        dtype = "gpu"

                                        if C.device_type != dtype:
                                            dev_id = available_devices["gpu"].popleft()
                                            # print "Removing from PredBlock",line_no(),  dev_id, dtype
                                            available_devices[C.device_type].append(C.device_id)
                                            C.device_type = dtype
                                            C.device_id = dev_id

                                        predecessor_not_found = False

                                        cluster_map_predecessors.append(pred_task)

                            if predecessor_not_found:
                                # print "PREDECESSOR NOT FOUND"
                                
                                # # Predecessor not in task chain map 
                                # for pred in self.get_task_component_parents(t):
                                #     if pred not in task_chain_map.keys():
                                #         revert_clusters(cluster_map_predecessors, potential_clusters)
                                #         return False

                                # for pred in self.get_task_component_parents(t):
                                #     if pred.get_first_kernel().in_frontier:
                                #         revert_clusters(cluster_map_predecessors, potential_clusters)
                                #         return False


                                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and (C.device_type =="cpu" or len(available_devices["cpu"]) > 0):
                                    
                                    dtype = "cpu"
                                    if C.device_type != dtype:
                                        dev_id = available_devices["cpu"].popleft()
                                        # print "Removing from PredBlock", line_no(), dev_id, dtype
                                        available_devices[C.device_type].append(C.device_id)
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    predecessor_not_found = False

                                    cluster_map_predecessors.append(pred_task)
                                elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and (C.device_type == "gpu" or len(available_devices["gpu"]) > 0):
                                    dtype = "gpu"
                                    if C.device_type != dtype:
                                        dev_id = available_devices["gpu"].popleft()
                                        # print "Removing from PredBlock", line_no(), dev_id, dtype
                                        available_devices[C.device_type].append(C.device_id)
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    predecessor_not_found = False
                                    
                                    cluster_map_predecessors.append(pred_task)

                                else:
                                    # print "LC_DEBUG_REVERT Calling Revert  predecessor found cannot be assigned"
                                    revert_clusters(cluster_map_predecessors, potential_clusters)
                                    return False


                            if not predecessor_not_found:
                                # print "PREDECESSOR FOUND!"
                                C.assign_values(gain_cpu, gain_gpu, pred_task ,sl_original=sl_original, sl_cpu=sl_cpu, sl_gpu=sl_gpu)
                                task_chain_map[pred_task] = C
                                if pred_task.get_first_kernel().in_frontier:
                                    C.frontier_task = True
                                if C not in potential_clusters:
                                    potential_clusters.append(C)
                                for p in self.get_task_component_parents(t):
                                    if p != pred_task and p not in task_chain_map.keys() and not p.is_supertask() and not p.is_dispatched:
                                        # print lineno(), "Adding to stack", p.get_kernel_ids()
                                        predecessor_stack.append(p)
                                t = pred_task
                                break

                    if predecessor_not_found:

                    # Predecessor not in task chain map 
                        for pred in self.get_task_component_parents(t):
                            if pred not in task_chain_map.keys():
                                revert_clusters(cluster_map_predecessors, potential_clusters)
                                return False

                        for pred in self.get_task_component_parents(t):
                            if pred.get_first_kernel().in_frontier:
                                revert_clusters(cluster_map_predecessors, potential_clusters)
                                return False
                        break

                    else:
                        t = pred_task


        # print "POTENTIAL CLUSTERS"
        for C in potential_clusters:
            
            # C.print_information()
            cluster_map.append(C)

        return True

    def linear_cluster_contraction(self, starting_task, d, ready_queue, task_chain_map, log):

        available_devices = ready_queue
        cluster_map = []
        # task_chain_map = {}

        lookahead_depth = 0
        task = starting_task
        C = Cluster(self.skeleton, "succ", task, log)
        if task.get_first_kernel().in_frontier:
            C.frontier_task=True
        task_chain_map[task] = C
        # print "SUCC CHAIN Starts at ", task.get_kernel_ids()
        # print "With gains: ",C.gain_speedup_times["cpu"], C.gain_speedup_times["gpu"]
        while(lookahead_depth < d):
            successors = []
            task_growth = False
            for succ in self.get_task_component_children(task):
                if not succ.is_supertask() and succ not in task_chain_map.keys() and not succ.is_dispatched:
                    # print "PUSHING rank", succ.get_first_kernel().rank
                    heapq.heappush(successors, (-succ.get_first_kernel().rank, succ))
            succ = None
            while len(successors) > 0:
                succ_rank, succ = heapq.heappop(successors)
                # Complete gain function
                # print "NEXT SUCC", succ_rank, succ.get_kernel_ids()
                # gain_cpu, gain_gpu = C.calculate_gain(succ)
                gain_cpu, gain_gpu,  sl_original, sl_cpu, sl_gpu   = C.calculate_gain_value("gain_speedup", succ)
                # print gain_cpu, gain_gpu
                # if gain_cpu > 0 and gain_gpu > 0:
                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == False and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == False:
                    continue
                else:
                    dtype = ""
                    dev_id = -1
                    # if gain_cpu < 0 and gain_gpu < 0:
                    if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == True and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == True:
                        if gain_cpu > gain_gpu:
                            if C.device_type == "cpu" or len(available_devices["cpu"]) > 0:
                                if C.device_id == -1:
                                    dtype = "cpu"
                                    dev_id = available_devices["cpu"].popleft()
                                    # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                    # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        # print "Adding ", line_no(), dev_id, dtype
                                        available_devices[dtype].append(dev_id)
                                else:
                                    if C.device_type != "cpu":
                                        dtype = "cpu"
                                        dev_id = available_devices["cpu"].popleft()
                                        available_devices[C.device_type].append(C.device_id)
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True
                                            C.device_type = dtype
                                            C.device_id = dev_id
                                        else:
                                            available_devices[dtype].append(dev_id)
                                            available_devices[C.device_type].remove(C.device_id)
                                    else:
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True

                        else:
                            if C.device_type == "gpu" or len(available_devices["gpu"]) > 0:
                                if C.device_id == -1:
                                    dtype = "gpu"
                                    dev_id = available_devices["gpu"].popleft()
                                    # print "Removing ", succ.get_first_kernel().id, line_no(),  dev_id, dtype
                                    # print lineno(), "Calling predecessor assignment"
                                    # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        # print "Adding ", line_no(), dev_id, dtype
                                        available_devices[dtype].append(dev_id)
                                else:
                                    if C.device_type != "gpu":
                                        dtype = "gpu"
                                        dev_id = available_devices["gpu"].popleft()
                                        available_devices[C.device_type].append(C.device_id)
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True
                                            C.device_type = dtype
                                            C.device_id = dev_id
                                        else:
                                            available_devices[dtype].append(dev_id)
                                            available_devices[C.device_type].remove(C.device_id)
                                    else:
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True

                    if not task_growth:
                        if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and (C.device_type == "cpu" or len(available_devices["cpu"]) > 0):

                            if C.device_id == -1:
                                dtype = "cpu"
                                dev_id = available_devices["cpu"].popleft()
                                # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                # print lineno(), "Calling predecessor assignment"
                                if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                    task_growth = True
                                    C.device_type = dtype
                                    C.device_id = dev_id
                                else:
                                    # print "Adding ", line_no(), dev_id, dtype
                                    available_devices[dtype].append(dev_id)
                            else:
                                if C.device_type != "cpu":
                                    dtype = "cpu"
                                    dev_id = available_devices["cpu"].popleft()
                                    available_devices[C.device_type].append(C.device_id)
                                    # print lineno(), "Calling predecessor assignment"
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        available_devices[dtype].append(dev_id)
                                        available_devices[C.device_type].remove(C.device_id)
                                else:
                                    # print lineno(), "Calling predecessor assignment"
                                    # print available_devices
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                        elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and (C.device_type =="gpu" or len(available_devices["gpu"]) > 0):
                            if C.device_id == -1:
                                dtype = "gpu"
                                dev_id = available_devices["gpu"].popleft()
                                # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                # print lineno(), "Calling predecessor assignment"
                                if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                    task_growth = True
                                    C.device_type = dtype
                                    C.device_id = dev_id
                                else:
                                    # print "Adding ", line_no(), dev_id, dtype
                                    available_devices[dtype].append(dev_id)
                            else:
                                if C.device_type != "gpu":
                                    dtype = "gpu"
                                    dev_id = available_devices["gpu"].popleft()
                                    available_devices[C.device_type].append(C.device_id)
                                    # print lineno(), "Calling predecessor assignment"
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        available_devices[dtype].append(dev_id)
                                        available_devices[C.device_type].remove(C.device_id)
                                else:
                                    # print lineno(), "Calling predecessor assignment for", succ.get_kernel_ids()
                                    
                                    if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True

                    if task_growth:
                        C.assign_values(gain_cpu, gain_gpu, succ, sl_original=sl_original, sl_cpu=sl_cpu, sl_gpu=sl_gpu)
                        task_chain_map[succ] = C
                        # print "ADDING ", succ.get_first_kernel().id
                        
                        break

            if not task_growth:
                break

            task = succ
            lookahead_depth += 1

        # print "LC_DEBUG Cluster size of succ chain", C.cluster_size()
        if C.cluster_size() == 1:
            # print "LC_DEBUG: Cluster size is 1 and devices",
            # print available_devices
            if C.partition_class_values[0] < 5 and len(available_devices["cpu"]) > 0:
                C.device_type = "cpu"
                C.device_id = available_devices["cpu"].popleft()
                cluster_map.append(C)
            elif C.partition_class_values[0] >= 5 and len(available_devices["gpu"]) > 0:
                C.device_type = "gpu"
                C.device_id = available_devices["gpu"].popleft()
                cluster_map.append(C)
            else:
                task_chain_map.pop(C.tasks[0])
        else:
            cluster_map.append(C)
        return cluster_map

    def predecessor_assignment_wq(self, task, task_chain_map, cluster_map, available_devices, log):

        def revert_clusters_wq(cluster_map_predecessors, potential_clusters):

            for C in potential_clusters:
                # print "LC_REVERT_Removing tasks from ",
                # C.print_information()
                T = C.get_last_task()

                while T in cluster_map_predecessors:
                    # print "LC_REVERT_Removing task", T.get_first_kernel().id
                    # print "Before removing", C.SL
                    C.remove_last_task()
                    # print "After removing", C.SL
                    task_chain_map.pop(T)
                    if C.cluster_size() > 0:
                        T = C.get_last_task()
                    else:
                        break

                if C.cluster_size() == 0:
                    dtype = C.device_type
                    dev_id = C.device_id
                    # print "LC_REVERT_DEVUPDATE ", dtype,dev_id
                    available_devices[dtype].append(dev_id)


        # print "predecessor assignment called for", task.get_first_kernel().id

        if len(self.get_task_component_parents(task)) == 1:
            # print "Only one predecessor"
            return True

        cluster_map_predecessors = []
        potential_clusters = []
        predecessor_stack = []
        chain_predecessors = []

        existing_task_ids = []
        for t in task_chain_map.keys():
            existing_task_ids.append(t.get_first_kernel().id)
        # print "LC_DEBUG: Existing task ids in cluster chain maps ", existing_task_ids
        for t in self.get_task_component_parents(task):
            if t not in task_chain_map.keys() and not t.is_supertask() and not t.is_dispatched:
                predecessor_stack.append(t)
        pstack_ids = []
        for debug_p in predecessor_stack:
            pstack_ids.append(debug_p.get_kernel_ids())
        # print lineno(), "EXISTING CLUSTERS"
        # for c in cluster_map:
            # c.print_information()
        # print "LC_DEBUG: Removing Predecessor Stack ", pstack_ids


        while len(predecessor_stack) > 0:
            
            t = predecessor_stack.pop()
            # print "POPPED ", t.get_kernel_ids()
            gain_cpu = 0.0
            gain_gpu = 0.0
            sl_original = 0.0
            sl_cpu = 0.0
            sl_gpu = 0.0

            task_merge = False
            target_task = None
            target_cluster = None
            pred_chain_task = False
            succ_chain_task = False
            target_pred = None
            for pred in self.get_task_component_parents(t):
                if pred in task_chain_map.keys() and not pred.is_supertask():
                    if task_chain_map[pred].chain_type == "pred" and task_chain_map[pred].get_last_task() == pred:
                        # print "POSSIBLE PRED MERGE FOUND ",
                        target_task = t
                        target_pred = pred
                        target_cluster = task_chain_map[pred]
                        # target_cluster.print_information()
                        pred_chain_task = True
                    elif task_chain_map[pred].chain_type == "succ" and task_chain_map[pred].get_last_task() == pred:
                        # print "POSSIBLE SUCC MERGE FOUND ",
                        target_task = t
                        target_pred = pred
                        target_cluster = task_chain_map[pred]
                        # target_cluster.print_information()
                        succ_chain_task = True

                if target_task != None:
                    if pred_chain_task:
                        # gain_cpu, gain_gpu = target_cluster.calculate_gain_merge_chain(target_task)
                        gain_cpu, gain_gpu, sl_original, sl_cpu, sl_gpu  = target_cluster.calculate_gain_value("gain_speedup", target_task, merge=True)
                    if succ_chain_task:
                        # gain_cpu, gain_gpu = target_cluster.calculate_gain(target_task)
                        gain_cpu, gain_gpu , sl_original, sl_cpu, sl_gpu   = target_cluster.calculate_gain_value("gain_speedup", target_task, merge=True)

                    # if gain_cpu < 0 and target_cluster.device_type == "cpu":
                    if target_cluster.gain_cpu_admissible(gain_cpu, "gain_speedup") and target_cluster.device_type == "cpu":

                        if pred_chain_task:
                            # target_cluster.tasks.insert(0, target_task)
                            target_cluster.tasks.append(target_task)
                        if succ_chain_task:
                            target_cluster.tasks.append(target_task)
                        target_cluster.gain["cpu"] = gain_cpu
                        target_cluster.gain["gpu"] = gain_gpu

                        target_cluster.gain_history["cpu"].append(gain_cpu)
                        target_cluster.gain_history["gpu"].append(gain_gpu)
                       
                        target_cluster.SL["cpu"].append(sl_cpu)
                        target_cluster.SL["gpu"].append(sl_gpu)
                        target_cluster.SL["original"].append(sl_original)
                        target_cluster.gain_speedup_times["cpu"].append(gain_cpu)
                        target_cluster.gain_speedup_times["gpu"].append(gain_gpu)

                        task_chain_map[target_task] = target_cluster
                        cluster_map_predecessors.append(target_task)
                        if target_cluster not in potential_clusters:
                            potential_clusters.append(target_cluster)
                        # print "BOOLEAN VALUES", pred_chain_task, succ_chain_task
                        # print "GAIN CPU,", 
                        # target_cluster.print_information()
                        task_merge = True

                    elif target_cluster.gain_gpu_admissible(gain_gpu, "gain_speedup") and target_cluster.device_type == "gpu":
                        if pred_chain_task:
                            # target_cluster.tasks.insert(0, target_task)
                            target_cluster.tasks.append(target_task)
                        if succ_chain_task:
                            target_cluster.tasks.append(target_task)
                        
                        target_cluster.gain["cpu"] = gain_cpu
                        target_cluster.gain["gpu"] = gain_gpu
                        target_cluster.gain_history["cpu"].append(gain_cpu)
                        target_cluster.gain_history["gpu"].append(gain_gpu)

                        target_cluster.SL["cpu"].append(sl_cpu)
                        target_cluster.SL["gpu"].append(sl_gpu)
                        target_cluster.SL["original"].append(sl_original)
                        target_cluster.gain_speedup_times["cpu"].append(gain_cpu)
                        target_cluster.gain_speedup_times["gpu"].append(gain_gpu)

                        task_merge = True

                        task_chain_map[target_task] = target_cluster
                        cluster_map_predecessors.append(target_task)
                        if target_cluster not in potential_clusters:
                            potential_clusters.append(target_cluster)
                        # print "BOOLEAN VALUES", pred_chain_task, succ_chain_task
                        # print "GAIN GPU,", 
                        # target_cluster.print_information()
                if task_merge:
                    # print "TASK MERGE SUCCESSFUL"
                    for pred in self.get_task_component_parents(t):
                        if pred not in task_chain_map.keys() and pred != target_pred and not pred.is_supertask() and not pred.is_dispatched:
                            # print "ADDING TO PREDECESSOR STACK", pred.get_first_kernel().id
                            predecessor_stack.append(pred)
                    
                    # print "LC_DEBUG: Updated Predecessor Stack ", [debug_p.get_first_kernel().id for debug_p in predecessor_stack]

                            
                    break
                target_task = None
                target_cluster = None
                pred_chain_task = False
                succ_chain_task = False
                target_pred = None


            if not task_merge:

                # print "TASK NOT MERGEABLE ",t.get_first_kernel().id

                C = Cluster(self.skeleton, "pred", t, log)
                gain_cpu = C.gain["cpu"]
                gain_gpu = C.gain["gpu"]
                sl_original  = C.SL["original"]
                sl_cpu = C.SL["cpu"]
                sl_gpu = C.SL["gpu"]
                if t.get_first_kernel().in_frontier:
                    # print "LC_DEBUG_Setting frontier task to true" ,t.get_first_kernel().id
                    C.frontier_task = True

                
                # if gain_cpu < 0 and len(available_devices["cpu"]) > 0:
                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and len(available_devices["cpu"]) > 0:
                    C.device_type = "cpu"
                    C.device_id = available_devices["cpu"].popleft()
                    if C not in potential_clusters:
                        # print lineno(), "Adding to potential cluster"
                        task_chain_map[t] = C
                        potential_clusters.append(C)
                elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and len(available_devices["gpu"]) > 0:
                    C.device_type = "gpu"
                    C.device_id = available_devices["gpu"].popleft()
                    if C not in potential_clusters:
                        # print lineno(), "Adding to potential cluster"
                        task_chain_map[t] = C
                        potential_clusters.append(C)
                else:
                    # print "LC_DEBUG_REVERT Calling Revert since predecessor cannot be assigned"
                    revert_clusters_wq(cluster_map_predecessors, potential_clusters)
                    return False

                cluster_map_predecessors.append(t)
                # cluster_map[t] = C
                while(not t.get_first_kernel().in_frontier):

                    predecessors = []
                    # print lineno(), predecessors, "PREDECESSORS OF " ,t.get_kernel_ids()
                    for pred in self.get_task_component_parents(t):
                        # print lineno(), "PARENT",
                        
                        if pred not in task_chain_map.keys() and not pred.is_supertask() and not pred.is_dispatched:
                            # print pred.get_kernel_ids(),
                            heapq.heappush(predecessors, (-pred.get_first_kernel().rank, pred))
                    # print "."
                    target_task = None
                    predecessor_not_found = True
                    # print lineno(), "PREDECESSORS ",[task.get_first_kernel().id for prio, task in predecessors]
                    while len(predecessors) > 0 and predecessor_not_found:
                        priority, pred_task = heapq.heappop(predecessors)
                        # print lineno(), C.get_task_ids()
                        # gain_cpu, gain_gpu = C.calculate_gain(pred_task)
                        gain_cpu, gain_gpu,  sl_original, sl_cpu, sl_gpu   = C.calculate_gain_value("gain_speedup", pred_task)
                        # print lineno(), "GAINS ", gain_cpu, gain_gpu
                        #if gain_cpu > 0 and gain_gpu > 0:
                        if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == False and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == False:
                            continue
                        else:
                            dtype = ""
                            dev_id = -1
                            # if gain_cpu < 0 and gain_gpu < 0:
                            if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == True and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == True:
                                if gain_cpu > gain_gpu:
                                    if C.device_type =="cpu" or len(available_devices["cpu"]) > 0:

                                        dtype = "cpu"

                                        if C.device_type != dtype:

                                            dev_id = available_devices["cpu"].popleft()
                                            # print "Removing from PredBlock",line_no(), dev_id, dtype
                                            available_devices[C.device_type].append(C.device_id)
                                            C.device_type = dtype
                                            C.device_id = dev_id

                                        predecessor_not_found = False

                                        cluster_map_predecessors.append(pred_task)

                                else:
                                    if C.device_type =="gpu" or len(available_devices["gpu"]) > 0:
                                        dtype = "gpu"

                                        if C.device_type != dtype:
                                            dev_id = available_devices["gpu"].popleft()
                                            # print "Removing from PredBlock",line_no(),  dev_id, dtype
                                            available_devices[C.device_type].append(C.device_id)
                                            C.device_type = dtype
                                            C.device_id = dev_id

                                        predecessor_not_found = False

                                        cluster_map_predecessors.append(pred_task)

                            if predecessor_not_found:
                                # print "PREDECESSOR NOT FOUND"
                                
                                # # Predecessor not in task chain map 
                                # for pred in self.get_task_component_parents(t):
                                #     if pred not in task_chain_map.keys():
                                #         revert_clusters(cluster_map_predecessors, potential_clusters)
                                #         return False

                                # for pred in self.get_task_component_parents(t):
                                #     if pred.get_first_kernel().in_frontier:
                                #         revert_clusters(cluster_map_predecessors, potential_clusters)
                                #         return False


                                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and (C.device_type =="cpu" or len(available_devices["cpu"]) > 0):
                                    
                                    dtype = "cpu"
                                    if C.device_type != dtype:
                                        dev_id = available_devices["cpu"].popleft()
                                        # print "Removing from PredBlock", line_no(), dev_id, dtype
                                        available_devices[C.device_type].append(C.device_id)
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    predecessor_not_found = False

                                    cluster_map_predecessors.append(pred_task)
                                elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and (C.device_type == "gpu" or len(available_devices["gpu"]) > 0):
                                    dtype = "gpu"
                                    if C.device_type != dtype:
                                        dev_id = available_devices["gpu"].popleft()
                                        # print "Removing from PredBlock", line_no(), dev_id, dtype
                                        available_devices[C.device_type].append(C.device_id)
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    predecessor_not_found = False
                                    
                                    cluster_map_predecessors.append(pred_task)

                                else:
                                    # print "LC_DEBUG_REVERT Calling Revert  predecessor found cannot be assigned"
                                    revert_clusters_wq(cluster_map_predecessors, potential_clusters)
                                    return False


                            if not predecessor_not_found:
                                # print "PREDECESSOR FOUND!"
                                C.assign_values(gain_cpu, gain_gpu, pred_task ,sl_original=sl_original, sl_cpu=sl_cpu, sl_gpu=sl_gpu)
                                task_chain_map[pred_task] = C
                                if pred_task.get_first_kernel().in_frontier:
                                    C.frontier_task = True
                                if C not in potential_clusters:
                                    potential_clusters.append(C)
                                for p in self.get_task_component_parents(t):
                                    if p != pred_task and p not in task_chain_map.keys() and not p.is_supertask() and not p.is_dispatched:
                                        # print lineno(), "Adding to stack", p.get_kernel_ids()
                                        predecessor_stack.append(p)
                                t = pred_task
                                break

                    if predecessor_not_found:

                    # Predecessor not in task chain map 
                        for pred in self.get_task_component_parents(t):
                            if pred not in task_chain_map.keys():
                                revert_clusters_wq(cluster_map_predecessors, potential_clusters)
                                return False

                        for pred in self.get_task_component_parents(t):
                            if pred.get_first_kernel().in_frontier:
                                revert_clusters_wq(cluster_map_predecessors, potential_clusters)
                                return False
                        break

                    else:
                        t = pred_task


        # print "POTENTIAL CLUSTERS"
        for C in potential_clusters:
            
            # C.print_information()
            cluster_map.append(C)

        return True



    def linear_cluster_contraction_wq(self, starting_task, d, ready_queue, task_chain_map, log):

        available_devices = ready_queue
        cluster_map = []
        # task_chain_map = {}

        lookahead_depth = 0
        task = starting_task
        C = Cluster(self.skeleton, "succ", task, log)
        if task.get_first_kernel().in_frontier:
            C.frontier_task=True
        task_chain_map[task] = C
        # print "SUCC CHAIN Starts at ", task.get_kernel_ids()
        while(lookahead_depth < d):
            successors = []
            task_growth = False
            for succ in self.get_task_component_children(task):
                if not succ.is_supertask() and succ not in task_chain_map.keys() and not succ.is_dispatched:
                    # print "PUSHING rank", succ.get_first_kernel().rank
                    heapq.heappush(successors, (-succ.get_first_kernel().rank, succ))
            succ = None
            while len(successors) > 0:
                succ_rank, succ = heapq.heappop(successors)
                # Complete gain function
                # print "NEXT SUCC", succ_rank, succ.get_kernel_ids()
                # gain_cpu, gain_gpu = C.calculate_gain(succ)
                gain_cpu, gain_gpu,  sl_original, sl_cpu, sl_gpu   = C.calculate_gain_value("gain_speedup", succ)
                # if gain_cpu > 0 and gain_gpu > 0:
                if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == False and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == False:
                    continue
                else:
                    dtype = ""
                    dev_id = -1
                    # if gain_cpu < 0 and gain_gpu < 0:
                    if C.gain_cpu_admissible(gain_cpu, "gain_speedup") == True and C.gain_gpu_admissible(gain_gpu, "gain_speedup") == True:
                        if gain_cpu > gain_gpu:
                            if C.device_type == "cpu" or len(available_devices["cpu"]) > 0:
                                if C.device_id == -1:
                                    dtype = "cpu"
                                    dev_id = available_devices["cpu"].popleft()
                                    # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                    # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        # print "Adding ", line_no(), dev_id, dtype
                                        available_devices[dtype].append(dev_id)
                                else:
                                    if C.device_type != "cpu":
                                        dtype = "cpu"
                                        dev_id = available_devices["cpu"].popleft()
                                        available_devices[C.device_type].append(C.device_id)
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True
                                            C.device_type = dtype
                                            C.device_id = dev_id
                                        else:
                                            available_devices[dtype].append(dev_id)
                                            available_devices[C.device_type].remove(C.device_id)
                                    else:
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True

                        else:
                            if C.device_type == "gpu" or len(available_devices["gpu"]) > 0:
                                if C.device_id == -1:
                                    dtype = "gpu"
                                    dev_id = available_devices["gpu"].popleft()
                                    # print "Removing ", succ.get_first_kernel().id, line_no(),  dev_id, dtype
                                    # print lineno(), "Calling predecessor assignment"
                                    # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        # print "Adding ", line_no(), dev_id, dtype
                                        available_devices[dtype].append(dev_id)
                                else:
                                    if C.device_type != "gpu":
                                        dtype = "gpu"
                                        dev_id = available_devices["gpu"].popleft()
                                        available_devices[C.device_type].append(C.device_id)
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True
                                            C.device_type = dtype
                                            C.device_id = dev_id
                                        else:
                                            available_devices[dtype].append(dev_id)
                                            available_devices[C.device_type].remove(C.device_id)
                                    else:
                                        # print lineno(), "Calling predecessor assignment"
                                        # print lineno(), "Predecessor called for ", succ.get_first_kernel().id
                                        if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                            task_growth = True

                    if not task_growth:
                        if C.gain_cpu_admissible(gain_cpu, "gain_speedup") and (C.device_type == "cpu" or len(available_devices["cpu"]) > 0):

                            if C.device_id == -1:
                                dtype = "cpu"
                                dev_id = available_devices["cpu"].popleft()
                                # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                # print lineno(), "Calling predecessor assignment"
                                if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                    task_growth = True
                                    C.device_type = dtype
                                    C.device_id = dev_id
                                else:
                                    # print "Adding ", line_no(), dev_id, dtype
                                    available_devices[dtype].append(dev_id)
                            else:
                                if C.device_type != "cpu":
                                    dtype = "cpu"
                                    dev_id = available_devices["cpu"].popleft()
                                    available_devices[C.device_type].append(C.device_id)
                                    # print lineno(), "Calling predecessor assignment"
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        available_devices[dtype].append(dev_id)
                                        available_devices[C.device_type].remove(C.device_id)
                                else:
                                    # print lineno(), "Calling predecessor assignment"
                                    # print available_devices
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                        elif C.gain_gpu_admissible(gain_gpu, "gain_speedup") and (C.device_type =="gpu" or len(available_devices["gpu"]) > 0):
                            if C.device_id == -1:
                                dtype = "gpu"
                                dev_id = available_devices["gpu"].popleft()
                                # print "Removing ", succ.get_first_kernel().id, line_no(), dev_id, dtype
                                # print lineno(), "Calling predecessor assignment"
                                if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                    task_growth = True
                                    C.device_type = dtype
                                    C.device_id = dev_id
                                else:
                                    # print "Adding ", line_no(), dev_id, dtype
                                    available_devices[dtype].append(dev_id)
                            else:
                                if C.device_type != "gpu":
                                    dtype = "gpu"
                                    dev_id = available_devices["gpu"].popleft()
                                    available_devices[C.device_type].append(C.device_id)
                                    # print lineno(), "Calling predecessor assignment"
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True
                                        C.device_type = dtype
                                        C.device_id = dev_id
                                    else:
                                        available_devices[dtype].append(dev_id)
                                        available_devices[C.device_type].remove(C.device_id)
                                else:
                                    # print lineno(), "Calling predecessor assignment for", succ.get_kernel_ids()
                                    
                                    if self.predecessor_assignment_wq(succ, task_chain_map, cluster_map, available_devices, log):
                                        task_growth = True

                    if task_growth:
                        C.assign_values(gain_cpu, gain_gpu, succ, sl_original=sl_original, sl_cpu=sl_cpu, sl_gpu=sl_gpu)
                        task_chain_map[succ] = C
                        # print "ADDING ", succ.get_first_kernel().id
                        
                        break

            if not task_growth:
                break

            task = succ
            lookahead_depth += 1

        # print "LC_DEBUG Cluster size of succ chain", C.cluster_size()
        if C.cluster_size() == 1:
            # print "LC_DEBUG: Cluster size is 1 and devices",
            # print available_devices
            if C.partition_class_values[0] < 5 and len(available_devices["cpu"]) > 0:
                C.device_type = "cpu"
                C.device_id = available_devices["cpu"].popleft()
                cluster_map.append(C)
            elif C.partition_class_values[0] >= 5 and len(available_devices["gpu"]) > 0:
                C.device_type = "gpu"
                C.device_id = available_devices["gpu"].popleft()
                cluster_map.append(C)
            else:
                task_chain_map.pop(C.tasks[0])
        else:
            cluster_map.append(C)
        return cluster_map


    def local_scenario_contraction(self, task, width, depth, frontier, nCPU, mGPU, dt_max_value, percent_tolerance):
        subgraph_nodes = []
        # print "TC_GEN starting with", task.get_kernel_ids()
        if depth != -1:
            self.construct_component_peek_var(task, subgraph_nodes, width, depth)
        else:
            self.construct_component_var(task, subgraph_nodes, dt_max_value, percent_tolerance, 2)
        ex_cpu, ex_gpu = self.ex_map
        print "LSC TASK_COMPONENT_CREATED ", [k.get_kernel_ids() for k in subgraph_nodes]
        # for node in subgraph_nodes:
        #     task_name = node.get_first_kernel().name
            # print "LSC Execution Times of ", task_name, task.Class, ex_cpu[task_name], ex_gpu[task_name]
        exec_cpu = self.get_exec_cpu(subgraph_nodes)
        exec_gpu = self.get_exec_gpu(subgraph_nodes)
        exec_baseline = self.get_exec_baseline(subgraph_nodes, nCPU, mGPU)

        print [(k.get_kernel_ids(),k) for k in subgraph_nodes]

        print "LSC Times: ", exec_cpu, exec_gpu, exec_baseline
        partition_mean = np.mean([partition_class_absolute(t) for t in subgraph_nodes])
        print "LSC MEAN ", partition_mean
        min_exec = min(exec_cpu, exec_gpu)
        max_exec = max(exec_cpu, exec_gpu)

        if partition_mean < 3 and exec_cpu > exec_gpu:
            return task
        if partition_mean >7 and exec_cpu < exec_gpu:
            return task
        if partition_mean > 3 and partition_mean < 7 and max_exec / min_exec > 1.5:
            return task

        # if len(subgraph_nodes) == 1:
        #     return task

        if exec_baseline < exec_cpu and exec_baseline < exec_gpu:
            for node in subgraph_nodes:
                node.contract = False
            return task
        else:
            if exec_gpu < exec_cpu:
                if mGPU == 0:
                    return task
                subgraph_nodes[0].Class = "TEN"

                # for x in frontier.queue:
                #     print x, x.get_first_kernel().name
                for node in subgraph_nodes:
                    if node.get_first_kernel().in_frontier and node is not task:
                        # print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                        frontier.queue.remove(node)
                if len(subgraph_nodes) == 1:
                    # print "LSC Name ",task.get_first_kernel().name
                    # print "LSC EX CPU: ", ex_cpu[task.get_first_kernel().name]
                    # print "LSC EX GPU: ", ex_gpu[task.get_first_kernel().name]
                    task.projected_ex_time = ex_gpu[task.get_first_kernel().name]
                    task.get_first_kernel().projected_ex_time = ex_gpu[task.get_first_kernel().name]
                return self.merge_task_list(subgraph_nodes)
            else:

                if nCPU == 0:
                    return task
                subgraph_nodes[0].Class = "ZERO"
                for node in subgraph_nodes:
                    task_name = node.get_first_kernel().name
                    # print "LSC Execution Times of ", task_name, task.Class, ex_cpu[task_name], ex_gpu[task_name]
                    if node.get_first_kernel().in_frontier and node is not task:
                        # print "Removing ", node.get_first_kernel().id, node.get_first_kernel().name
                        frontier.queue.remove(node)
                if len(subgraph_nodes) == 1:
                    # print "LSC Name ", task.get_first_kernel().name
                    # print "LSC EX CPU: ", ex_cpu[task.get_first_kernel().name]
                    # print "LSC EX GPU: ", ex_gpu[task.get_first_kernel().name]
                    task.projected_ex_time = ex_cpu[task.get_first_kernel().name]
                    task.get_first_kernel().projected_ex_time = ex_cpu[task.get_first_kernel().name]

                return self.merge_task_list(subgraph_nodes)


    def greedy_gpu_contraction(self, task, width, depth):

        # Get Component T

        subgraph_nodes = []
        self.construct_component_gpu(task, subgraph_nodes, width, 1)

        # Construct subgraph and sort edge list
        k_set = []
        for component in subgraph_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)
        edge_list = subgraph.edges()
        sorted(edge_list, key=lambda x: subgraph[x[0]][x[1]]['weight'])
        delete_edge_count = 0
        for i in range(1, len(edge_list)):
            exec_base = 0.0
            exec_gpu = 0.0
            dt_time = 0.0
            counter = 0
            time_info = deepcopy(self.available_device_lookahead)
            for j in range(0, i):
                print i
                edge = edge_list[i]
                source = edge[0]
                target = edge[1]
                waiting_time = 0.0
                if len(time_info['gpu']) != 0:
                    waiting_time = heapq.heappop(time_info['gpu'])
                exec_base = max(exec_base, waiting_time + self.tasks[target].execution_time)
                dt_time += self.skeleton[source][target]['time']
                exec_gpu += self.tasks[target].execution_time - dt_time

            if exec_gpu > exec_base:
                delete_edge_count = i
                break

            if delete_edge_count > 0:
                for i in range(0, delete_edge_count):
                    edge = edge_list[i]
                    target = edge[1]
                    subgraph_nodes.remove(self.task_components[target])

            return self.merge_task_list(subgraph_nodes)










    def contract_component(self, task, nCPU, mGPU):
        subgraph_nodes = [task]
        for succ in self.get_task_component_children(task):
            t = succ.get_first_kernel()
            if len(self.get_task_parents(t)) <= 1:
                subgraph_nodes.append(succ)
        cpu, gpu = self.get_component_device_affinity(subgraph_nodes)
        print "Device Affinity: CPU: " + str(cpu) + " GPU:  " + str(gpu)
        percent = float(gpu) / (float(cpu) + float(gpu))
        if len(subgraph_nodes) > 1 and percent >= 0.5:
            subgraph_nodes[0].Class = "TEN"
            return self.merge_task_list(subgraph_nodes)
        elif len(subgraph_nodes) > 1 and percent < 0.5:
            subgraph_nodes[0].Class = "ZERO"
            return self.merge_task_list(subgraph_nodes)
        else:
            return task

    def greedy_contraction(self, task, nCPU, mGPU):
        subgraph_nodes = [task]
        print "DEVTYPE Parent: " + partition_class_value(task.get_first_kernel().Class)
        for succ in self.get_task_component_children(task):
            t = succ.get_first_kernel()
            # print "DEVTYPE: " + partition_class_value(t.Class)
            if len(self.get_task_parents(t)) <= 1 and partition_class_value(t.Class) == "gpu" and partition_class_value(
                    task.get_first_kernel().Class) == "gpu":
                print "DEVTYPE Succ: " + partition_class_value(t.Class)
                subgraph_nodes.append(succ)
        k_set = []
        for component in subgraph_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)
        edge_list = subgraph.edges()
        sorted(edge_list, key=lambda x: subgraph[x[0]][x[1]]['weight'])
        print "Edge List ",
        print edge_list
        delete_edge_count = 0.0
        print 'Greedy Decision ',
        print self.available_device_lookahead
        for i in range(1, len(edge_list)):
            waiting_time_lookahead = 0.0
            projected_time_lookahead = 0.0
            dt_time = 0.0
            counter = 0
            for j in range(0, i):
                print i
                edge = edge_list[i]
                source = edge[0]
                target = edge[1]
                if len(self.available_device_lookahead['gpu']) < mGPU:
                    waiting_time_lookahead = max(waiting_time_lookahead, self.tasks[target].execution_time)
                else:
                    waiting_time_lookahead = max(waiting_time_lookahead,
                                                 self.available_device_lookahead['gpu'][counter] + self.tasks[
                                                     target].execution_time)
                    counter += 1
                dt_time += self.skeleton[source][target]['time']
                projected_time_lookahead += self.tasks[target].execution_time - dt_time

            if projected_time_lookahead > waiting_time_lookahead:
                delete_edge_count = i
                break

        if delete_edge_count > 0:
            for i in range(0, delete_edge_count):
                edge = edge_list[i]
                target = edge[1]
                subgraph_nodes.remove(self.task_components[target])

        return self.merge_task_list(subgraph_nodes)


    def get_exec_gpu(self, contraction_nodes):
        k_set = []
        for component in contraction_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)

        # Calculation for CPU and GPU only times
        tdt = 0.0
        # for r, s in subgraph.edges():
        #     tdt += subgraph[r][s]['weight']
        # tdt = (2 * tdt)/BW
        task_ids = subgraph.nodes()
        global_dt_size = 0.0

        for task_id in subgraph.nodes():
            predecessors = self.skeleton.predecessors(task_id)
            successors = self.skeleton.successors(task_id)
            pred_dt_size = 0.0
            succ_dt_size = 0.0
            for pred in predecessors:
                if pred in task_ids:
                    pred_dt_size += self.skeleton[pred][task_id]['weight']

            k_source = self.tasks[task_id].Kernel_Object
            max_dt_size = 0.0


            for buf in k_source.buffer_info['output']:
                buf_size = float(buf['size']) * get_sizeof(buf['type'])
                max_dt_size += buf_size


            for succ in successors:
                if succ in task_ids:
                    succ_dt_size += self.skeleton[task_id][succ]['weight']
                    if succ_dt_size >= max_dt_size:
                        succ_dt_size = max_dt_size
                        break
            global_dt_size += pred_dt_size + succ_dt_size

        # for r, s in subgraph.edges():
        #     tdt += subgraph[r][s]['time']
        # tdt = (2 * tdt)

        ex_cpu, ex_gpu = self.ex_map


        gpu_time = 0.0

        # for k in k_set:
        #     gpu_time += ex_gpu[k.name]

        for k in k_set:
            gpu_time += k.ECO/GPU_FLOPS + k.DT/BW

        gpu_time -= global_dt_size/BW
        # print "LSC_GPU_Time_of", subgraph.nodes(), gpu_time
        return gpu_time

    def get_exec_cpu(self, contraction_nodes):
        k_set = []
        for component in contraction_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)

        # Calculation for CPU and GPU only times
        tdt = 0.0
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['time']
        tdt = 2 * tdt
        ex_cpu, ex_gpu = self.ex_map

        cpu_time = 0.0
        #
        # for k in k_set:
        #     cpu_time += ex_cpu[k.name]
        for k in k_set:
            cpu_time += k.ECO/CPU_FLOPS
        # for k in k_set:
        #     cpu_time += k.ECO/CPU_FLOPS

        # print "LSC_CPU_Time_of", subgraph.nodes(), cpu_time
        return cpu_time



    #Version assuming complete device availability at all times 

    def get_exec_baseline1(self, contraction_nodes, num_CPU, num_GPU):

        k_set = []
        for component in contraction_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)

        # Calculation for CPU and GPU only times

        tdt = 0.0
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['time']
        tdt = 2 * tdt
        ex_cpu, ex_gpu = self.ex_map

        normal_exec_time = 0.0

        # time_info = deepcopy(self.available_device_lookahead)

        # time_info = {'cpu': [], "gpu": []}
        time_info = {}
        for device in ["cpu", "gpu"]:
            for task_stats in self.currently_executing[device]:
                task, dag, device_id = task_stats
                time_value = 0.0
                if device =="cpu":
                    time_value = task.ECO/CPU_FLOPS
                else:
                    time_value= task.ECO/GPU_FLOPS + task.DT/BW
                time_info[device].append(time_value)




        # print time_info
        num_CPU, num_GPU = self.available_device_lookahead["nD"]

        levels = self.make_levels(subgraph)

        # print "LSC Baseline Calculation"
        for level in levels:

            max_cpu_time = 0.0
            max_gpu_time = 0.0
            # print "LSC Level ", level
            for i in range(0, len(level)):

                task = self.tasks[level[i]]
                # print "LSC ", time_info
                if partition_class_value(task.Class) is "cpu":
                    ready_time = 0.0
                    if len(time_info['cpu']) == num_CPU:
                        ready_time = heapq.heappop(time_info['cpu'])
                    # max_cpu_time = max(max_cpu_time, ready_time + ex_cpu[task.name])
                    max_cpu_time = max(max_cpu_time, ready_time + task.ECO / CPU_FLOPS)
                    # heapq.heappush(time_info['cpu'], ex_cpu[task.name])
                    heapq.heappush(time_info['cpu'], task.ECO / CPU_FLOPS)

                if partition_class_value(task.Class) is "gpu":
                    ready_time = 0.0
                    if len(time_info['gpu']) == num_GPU:
                        ready_time = heapq.heappop(time_info['gpu'])
                    # max_gpu_time = max(max_gpu_time, ready_time + ex_gpu[task.name])
                    max_gpu_time = max(max_gpu_time, ready_time + task.ECO / GPU_FLOPS + task.DT / BW)
                    # heapq.heappush(time_info['gpu'], ex_gpu[task.name])
                    heapq.heappush(time_info['gpu'], task.ECO / GPU_FLOPS + task.DT / BW)
                # print "LSC ", time_info
            # print "LSC CPU and GPU calculation", max_cpu_time, max_gpu_time
            normal_exec_time += max(max_cpu_time, max_gpu_time)
        # print "LSC_BASE_Time_of",subgraph.nodes(),normal_exec_time
        return normal_exec_time


    # Device availability aware Baseline Calculation

    def get_exec_baseline(self, contraction_nodes, num_CPU, num_GPU):

        # if num_CPU == 0 or num_GPU == 0:
        #     return float("inf")
        k_set = []
        for component in contraction_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)



        # Calculation for CPU and GPU only times

        tdt = 0.0
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['time']
        tdt = 2 * tdt
        ex_cpu, ex_gpu = self.ex_map

        for task in k_set:
            print "LSC: Execution Times CPU ",partition_class_absolute(task), task.name,ex_cpu[task.name], task.ECO/CPU_FLOPS
            print "LSC: Execution Times GPU",partition_class_absolute(task), task.name, ex_gpu[task.name], task.ECO / GPU_FLOPS + task.DT/BW,"(",task.ECO/GPU_FLOPS,task.DT/BW,")"


        normal_exec_time = 0.0

        # time_info = deepcopy(self.available_device_lookahead)
        # time_info = {'cpu' : [], "gpu": []}


        time_info = {'cpu': [], "gpu": []}
        # time_info = deepcopy(self.currently_executing)
        for device in ["cpu", "gpu"]:
            for task_stats in self.currently_executing[device]:
                task, dag, device_id = task_stats
                time_value = 0.0
                if device == "cpu":
                    time_value = task.ECO/CPU_FLOPS
                else:
                    time_value= task.ECO/GPU_FLOPS + task.DT/BW
                time_info[device].append(time_value)

        print time_info
        num_CPU, num_GPU = self.available_device_lookahead["nD"]
        levels = self.make_levels(subgraph)

        # print "LSC Baseline Calculation"
        for level in levels:

            max_cpu_time = 0.0
            max_gpu_time = 0.0
            # print "LSC Level ", level
            for i in range(0, len(level)):

                task = self.tasks[level[i]]
                # print "LSC ", time_info
                if partition_class_value(task.Class) is "cpu":
                    ready_time = 0.0
                    if len(time_info['cpu']) == num_CPU:
                        ready_time = heapq.heappop(time_info['cpu'])
                    # max_cpu_time = max(max_cpu_time, ready_time + ex_cpu[task.name])
                    max_cpu_time = max(max_cpu_time, ready_time + task.ECO/CPU_FLOPS)
                    # heapq.heappush(time_info['cpu'], ex_cpu[task.name])
                    heapq.heappush(time_info['cpu'], task.ECO/CPU_FLOPS)

                if partition_class_value(task.Class) is "gpu":
                    ready_time = 0.0
                    if len(time_info['gpu']) == num_GPU:
                        ready_time = heapq.heappop(time_info['gpu'])
                    # max_gpu_time = max(max_gpu_time, ready_time + ex_gpu[task.name])
                    max_gpu_time = max(max_gpu_time, ready_time + task.ECO/GPU_FLOPS + task.DT/BW)
                    # heapq.heappush(time_info['gpu'], ex_gpu[task.name])
                    heapq.heappush(time_info['gpu'], task.ECO/GPU_FLOPS + task.DT/BW)
                # print "LSC ", time_info
            # print "LSC CPU and GPU calculation", max_cpu_time, max_gpu_time
            normal_exec_time += max(max_cpu_time, max_gpu_time)
        # print "LSC_BASE_Time_of",subgraph.nodes(),normal_exec_time
        return normal_exec_time

    # Device sensitivity analysis




    def decide_contraction_execution_time(self, contraction_nodes, nCPU, mGPU):

        k_set = []
        for component in contraction_nodes:
            k_set.append(component.get_first_kernel())
        kernel_ids = map(lambda k: k.id, list(k_set))
        subgraph = self.get_skeleton_subgraph(kernel_ids)

        # Calculation for CPU and GPU only times

        tdt = 0.0
        for r, s in subgraph.edges():
            tdt += subgraph[r][s]['time']
        tdt = 2 * tdt
        ex_cpu, ex_gpu = self.ex_map

        cpu_time = 0.0
        gpu_time = 0.0
        normal_exec_time = 0.0
        for k in k_set:
            cpu_time += ex_cpu[k.name]
            gpu_time += ex_gpu[k.name]
        gpu_time -= tdt

        # Device sensitivity analysis

        levels = self.make_levels(subgraph)
        for level in levels:
            current_nCPU = nCPU + 1
            current_mGPU = mGPU + 1
            max_cpu_time = 0.0
            max_gpu_time = 0.0
            for i in range(0, len(level)):
                task = self.tasks[level[i]]
                if partition_class_value(task.Class) is "gpu":
                    if current_mGPU > 1:
                        max_gpu_time = max(max_gpu_time, task.execution_time)
                        current_mGPU -= 1
                    else:
                        max_gpu_time += task.execution_time
                if partition_class_value(task.Class) is "cpu":
                    if current_nCPU > 1:
                        max_cpu_time = max(max_cpu_time, task.execution_time)
                        current_nCPU -= 1
                    else:
                        max_cpu_time += task.execution_time
            normal_exec_time += max(max_cpu_time, max_gpu_time)

        min_execution_time = min(normal_exec_time, cpu_time, gpu_time)

        print "DECISION Normal:" + str(normal_exec_time) + " CPU: " + str(cpu_time) + " GPU:" + str(gpu_time)
        if min_execution_time is normal_exec_time:
            return "normal"
        elif min_execution_time is cpu_time:
            return "cpu"
        elif min_execution_time is gpu_time:
            return "gpu"

    def contract_component_k(self, task, k):
        subgraph_nodes = [task]
        successors = []

        for succ in self.get_task_component_children(task):
            t = succ.get_first_kernel()
            if len(self.get_task_parents(t)) <= 1:
                subgraph_nodes.append(succ)
                successors.append(succ)
        if len(successors) > 0:
            for i in range(1, k):
                new_successors = list(
                    set().union(*[set(self.get_task_component_children(succ)) for succ in successors]))
                new_successors_kernels = [s.get_first_kernel() for s in new_successors]
                for succ in new_successors:
                    L = list(set(self.get_task_component_parents(succ)))
                    L_kernels = [s.get_first_kernel() for s in L]
                    flag = True
                    for node in L_kernels:
                        if node not in new_successors_kernels:
                            flag = False
                            break
                    if flag:
                        successors.append(succ)
                        subgraph_nodes.append(succ)

        cpu, gpu = self.get_component_device_affinity(subgraph_nodes)
        print "Device Affinity: CPU: " + str(cpu) + " GPU:  " + str(gpu)
        percent = float(gpu) / (float(cpu) + float(gpu))

        if len(subgraph_nodes) > 1 and percent >= 0.5:
            subgraph_nodes[0].Class = "TEN"
            return self.merge_task_list(subgraph_nodes)
        elif len(subgraph_nodes) > 1 and percent < 0.5:
            subgraph_nodes[0].Class = "ZERO"
            return self.merge_task_list(subgraph_nodes)
        else:
            return task

    def initialize_task_component_ranks(self, rank_name):
        for task_component in self.G.nodes():
            if task_component.is_supertask():
                raise Exception("Cannot initialize rank of super task")
            else:
                task_component.rank_values[rank_name] = list(task_component.get_kernels())[0].rank_values[rank_name]
                task_component.rank_name = rank_name

    def get_component_device_affinity(self, task_components):
        cpu = 0
        gpu = 0
        for task_component in task_components:
            classes = task_component.get_kernel_classes()
            for Class in classes:
                if value_int(Class) < 5:
                    cpu += 1
                else:
                    gpu += 1
        return cpu, gpu


class DAGCreator(object):
    def create_dag(self, n, global_map, ex_map, dag_id):
        G = nx.fast_gnp_random_graph(4, 0.5)
        DAG = nx.DiGraph([(u, v) for (u, v) in G.edges() if u < v])
        task_dict = dict()
        for i in range(0, n):
            key = random.choice(global_map.keys())
            feat_dict = create_feat_dict(key, global_map)

            # extime = ex_map[key]
            extime = random.randint(1, 10)
            task_dict[i] = SimTask(key, i, dag_id, feat_dict, extime)
        TaskGraph = SimTaskDAG(task_dict, DAG, dag_id)
        return TaskGraph

    def create_dag_from_graph(self, DAG, global_map, ex_map, dag_id):
        task_dict = dict()
        n = len(DAG.nodes())
        for i in range(0, n):
            key = random.choice(global_map.keys())
            feat_dict = create_feat_dict(key, global_map)
            # extime = ex_map[key]
            extime = random.randint(1, 10)
            task_dict[i] = SimTask(key, i, dag_id, feat_dict, extime)
        TaskGraph = SimTaskDAG(task_dict, DAG, dag_id)
        return TaskGraph

    

    def create_adas_job_from_file(self, input_file, dag_id, global_map, ex_map, job_id,release_time,period=0.0):
        
        ex_cpu,ex_gpu=ex_map
        task_dict = dict()
        file_contents = open(input_file, "r").readlines()
        n, e = file_contents[0].strip("\n").split(" ")
        n=int(n)
        e=int(e)
        
        DAG = nx.DiGraph()
        for i in range(0, n):
            DAG.add_node(i)
        for i in range(1, n + 1):
            key = file_contents[i].strip("\n").split(":")[0]
            # print key
            
            extime_cpu,extime_gpu = map(float, file_contents[i].strip("\n").split(":")[1].split(","))
            ex_cpu[key]=1000*extime_cpu
            ex_gpu[key]=1000*extime_gpu
            feat_dict = create_feat_dict(key, global_map)
            if extime_cpu > extime_gpu:
                extime=extime_cpu
            else:
                extime=extime_gpu
            task_dict[i - 1] = SimTask(key, i - 1, dag_id, feat_dict, extime)
        DAG = nx.DiGraph()
        # print file_contents[n + 1:]
        for x in range(n+1,n+1+e):
            edge = file_contents[x]
            # print "EDGE", edge
            u, v = edge.strip("\n").split(" ")
            u = int(u)
            v = int(v.strip("\n"))
            DAG.add_edge(u, v, weight=0.0, time=0.0)
        fused_kernel_timings={}
        for tuple_times in file_contents[n+1+e:]:
            t,timing=tuple_times.split(":")
            t=tuple(map(int,t.split(",")))
            timing=float(timing.strip("\n"))
            fused_kernel_timings[t]=1000*timing

        name = input_file.split("/")[1]
        
        d = SimTaskDAG(task_dict, DAG, dag_id, ex_map,name=name,deadline=True)
        d.period=period
        d.job_id=job_id
        d.fused_kernel_timings=fused_kernel_timings
        d.release=release_time
        # print d.job_id
        d.calculate_wcet()
        # d.deadline=d.release+max(d.wcet,d.period)
        d.deadline=d.release+d.period
        d.adas=True
        # print d.job_id, d.release, d.wcet, d.deadline
        return d 
    



    
    def create_dag_from_file(self, input_file, dag_id, global_map, ex_map, ml=False):
        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        # print input_file
        # print input_file
        filter_list = []
        
        file_contents = open(input_file, "r").readlines()
        n = int(file_contents[0].strip("\n"))
        DAG = nx.DiGraph()
        for i in range(0, n):
            DAG.add_node(i)
        # print "TOTAL_NODES  " + str(len(DAG.nodes()))
        for i in range(1, n + 1):
            key = file_contents[i].strip("\n")
            # print key
            # extime = file_contents[i].split(" ")[1]
            filter_list.append(key)
            feat_dict = create_feat_dict(key, global_map)

            #Case for optimal

            
            if ex_cpu[key] < ex_gpu[key]:
                extime = ex_cpu[key]
                feat_dict['Class']= "ZERO"
            else:
                extime = ex_gpu[key]
                feat_dict['Class']= "TEN"
            

            
            # if (value_int(feat_dict['Class']) < 5):
            #     extime = ex_cpu[key]
            # else:
            #     extime = ex_gpu[key]
            
         
            task_dict[i - 1] = SimTask(key, i - 1, dag_id, feat_dict, extime)
        DAG = nx.DiGraph()
        # print file_contents[n + 1:]
        for edge in file_contents[n + 1:]:
            # print edge
            u, v = edge.split(" ")
            u = int(u)
            v = int(v.strip("\n"))
            DAG.add_edge(u, v, weight=0.0, time=0.0)
        name = input_file.split("/")[1]
        ml_classifier = None
        if ml:
            ml_classifier = CLTrainer(ex_map,global_map, list(set(filter_list)),name)
         

        return SimTaskDAG(task_dict, DAG, dag_id, ex_map, ml_classifier,name)

    def get_task_info(self, devtype):
        key = random.choice(global_map.keys())
        feat_dict = create_feat_dict(key, global_map)
        extime = 0.0
        while partition_class_value(feat_dict['Class']) is not devtype:
            key = random.choice(global_map.keys())
            feat_dict = create_feat_dict(key, global_map)

        if (devtype is "gpu"):
            extime = ex_gpu[key]
        else:
            extime = ex_cpu[key]
        return key, feat_dict, extime

    def create_sample_dag(self, dag_id, ex_map, filename):
        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        DAG = nx.DiGraph()
        for i in range(0, 21):
            DAG.add_node(i)
        DAG.add_edge(0, 1)
        DAG.add_edge(0, 2)
        DAG.add_edge(1, 3)
        DAG.add_edge(1, 4)
        DAG.add_edge(2, 5)
        DAG.add_edge(3, 6)
        DAG.add_edge(3, 7)
        DAG.add_edge(4, 8)
        DAG.add_edge(5, 9)
        DAG.add_edge(6, 10)
        DAG.add_edge(7, 11)
        DAG.add_edge(8, 12)
        DAG.add_edge(9, 13)
        DAG.add_edge(10, 14)
        DAG.add_edge(10, 15)
        DAG.add_edge(10, 16)
        DAG.add_edge(11, 17)
        DAG.add_edge(12, 18)
        DAG.add_edge(12, 19)
        DAG.add_edge(13, 20)
        for node in [0, 1, 2, 5, 6, 7, 9, 12, 13, 14, 15, 16, 18, 19]:
            print "Node" + str(node) + " --> gpu"
            key, feat_dict, extime = self.get_task_info("gpu")
            print " " + feat_dict['Class']
            task_dict[node] = SimTask(key, node, dag_id, feat_dict, extime)

        for node in [3, 4, 8, 10, 19, 11, 17, 20]:
            print str(node) + " --> cpu"
            key, feat_dict, extime = self.get_task_info("cpu")
            print " " + feat_dict['Class']
            task_dict[node] = SimTask(key, node, dag_id, feat_dict, extime)

        f = open(filename, "w")
        f.write(str(21))
        f.write("\n")
        for i in range(0, 21):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in DAG.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()
        return SimTaskDAG(task_dict, DAG, dag_id, ex_map)



    def select_database_dimension(self, buf_rand_pool, required_dimension):
        output = []
        for key in buf_rand_pool:
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            if required_dimension == dimension:
                output.append(key)
        return output

    def select_database_devicetype(self, buf_rand_pool, required_device_type, global_map):
        output = []
        device_type = ""
        for key in buf_rand_pool:
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            feat_dict = create_feat_dict(key, global_map)
            if value_int(feat_dict['Class']) > 5:
                device_type = "gpu"
            else:
                device_type = "cpu"

            if device_type == required_device_type:
                output.append(key)

        return output


    def randomly_select_SimTask(self, node_index, dag_id, ex_map, rand_task_pool, global_map, workitem_range, dimension):
        workitem_list = [workitem_range[0]]
        workitem = workitem_range[0]
        while(workitem < workitem_range[1]):
            workitem *=2
            workitem_list.append(workitem)

        workitem = random.choice(workitem_list)
        kernelName = random.choice(rand_task_pool[workitem][dimension])
        ex_cpu, ex_gpu = ex_map
        if kernelName is not None:
            key = kernelName+"_"+str(workitem)
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            return SimTask(key, node_index, dag_id, feat_dict, extime)
        else:
            self.randomly_select_SimTask(node_index, dag_id, rand_task_pool, global_map, workitem_range, dimension)

    def randomly_select_SimTask_key(self, node_index, dag_id, ex_map, rand_task_pool, global_map, workitem_range, dimension):
        workitem_list = [workitem_range[0]]
        workitem = workitem_range[0]
        while(workitem < workitem_range[1]):
            workitem *=2
            workitem_list.append(workitem)

        workitem = random.choice(workitem_list)
        kernelName = random.choice(rand_task_pool[workitem][dimension])
        ex_cpu, ex_gpu = ex_map
        if kernelName is not None:
            key = kernelName+"_"+str(workitem)
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            return key
            # return SimTask(key, node_index, dag_id, feat_dict, extime)
        else:
            self.randomly_select_SimTask_key(node_index, dag_id, rand_task_pool, global_map, workitem_range, dimension)





    def create_dag_randomly(self, global_map, dag_id, ex_map, n, width, regular, density, workitem_range, graph_file):

        # Create random task pool indexed by data set size and work dimension
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        ex_cpu, ex_gpu = ex_map
        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
        rand_bufsize_pool = {}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)
            print kernel_info
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
            buf_sizes = simtask.get_input_buffer_sizes()
            for buf in buf_sizes:
                if buf not in rand_bufsize_pool:
                    rand_bufsize_pool[buf] = []
                    rand_bufsize_pool[buf].append(key)
                else:
                    rand_bufsize_pool[buf].append(key)



        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        avg_tasks_per_level = exp(width * log(n))
        print avg_tasks_per_level
        level_sizes = []
        total_tasks = 0


        print "Generating Random DAG"

        # Get number of nodes per level

        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            print "Number of nodes in one level " + str(temp)
            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        index = 0
        node_index = 0
        buffer_levels = []
        task_levels = []
        graph_levels = []
        buffer_dict = {}
        tasks = []
        buffers = []
        node_mapping = {}

        dag = nx.DiGraph()
        levels = []
        level_nodes = []

        for i in range(0, level_sizes[0]):
            dimension = random.choice([1, 2])
            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                                 global_map, workitem_range, dimension)
            task_dict[node_index] = simtask
            node_mapping[index] = node_index
            G.add_node(index, shape='circle', label=simtask.name)
            task_index = index
            level_nodes.append(task_index)
            dag.add_node(node_index)
            node_index +=1
            index += 1
            input_buffers = simtask.get_input_buffer_sizes()
            output_buffers = simtask.get_output_buffer_sizes()
            print "BUFFER STATS"
            print input_buffers
            print output_buffers
            for buf_size in input_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(index, task_index)
                buffer_dict[index] = buf_size
                index+=1

            for buf_size in output_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(task_index, index)
                buffer_dict[index] = buf_size
                buffers.append(index)
                index += 1


        buffer_levels.append(buffers)
        levels.append(level_nodes)

        print "BUFFER SIZES LEVEL 0"
        print [buffer_dict[buffer_index] for buffer_index in buffers]
        print buffer_levels

        for i in range(1, len(level_sizes)):
            level_nodes = []
            bufs = buffer_levels[-1]
            input_buffer_level = []
            output_buffer_level = []

            # Number of output buffers in previous level is less than or equal to number of nodes in current level
            print "level" + str(i)
            print bufs
            buf_sizes = [buffer_dict[buffer_index] for buffer_index in bufs]
            print buf_sizes
            if len(bufs) <= level_sizes[i]:
                for buffer_index in bufs:
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()
                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1

                remaining = level_sizes[i] - len(bufs)


                while(remaining > 0):
                    remaining -= 1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)
                    dag.add_node(node_index)

                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]
                print input_buf_sizes
                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                if len(bufs) <= len(input_buffer_level):
                    edge_count = 0
                    for buffer_index in bufs:

                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if(not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    print "Edge does not exist"
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(input_buffer_level):
                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count +=1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break



                    # for buffer_index in input_buffer_level:
                    #     if G.in_degree(buffer_index) == 0:
                    #         for required_buffer_index in bufs:
                    #             if buffer_dict[buffer_index] == buffer_dict[required_buffer_index]:
                    #                 succ_task = int(G.successors(required_buf_index)[0])
                    #                 pred_task = int(G.predecessors(buffer_index)[0])
                    #                 if (not dag.has_edge(source_node, target_node)):
                    #                     dag.add_edge(source_node, target_node)
                    #                     G.add_edge(buffer_index, required_buf_index)
                    #                     edge_count +=1
                        if edge_count >level_edges:
                            break









                bufs.append(output_buffer_level)
            else:
                buf_counter = 0
                while buf_counter < level_sizes[i]:
                    buf_counter +=1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers
                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]

                print input_buf_sizes

                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                edge_count = 0
                if len(bufs) <= len(input_buffer_level):
                    for buffer_index in bufs:
                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if (not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    break
                            probe_count +=1

                            if probe_count > len(input_buffer_level):
                                break

                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(
                                        required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count += 1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break







                else:

                    counter = 0
                    for buffer_index in input_buffer_level:

                        succ_task = int(G.successors(buffer_index)[0])
                        pred_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False

                        while True:
                            required_buf_index = random.choice(bufs)
                            # print buffer_index
                            # print required_buf_index
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(buffer_index) == 0:
                                print "match found"
                                pred_task = int(G.predecessors(required_buf_index)[0])
                                print pred_task
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]

                                if (not dag.has_edge(source_node, target_node)):
                                    print "Edge does not exist"
                                    add_edge = True
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(bufs):

                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(required_buf_index, buffer_index)
            print "OUTPUT BUFFER LEVEL"
            print output_buffer_level
            buffer_levels.append(output_buffer_level)


        print levels
        print buffer_levels


        G_file = "buf_task_" + graph_file[:-5] + ".png"
        G.layout(prog='dot')
        print "Dumping buffer tasks graph"
        G.draw(G_file)
        filename = graph_file
        dag_dump_file = "complete_dump_"+filename
        dag_dump = open(dag_dump_file, 'w')
        dag_dump.write(str(len(G.nodes())) + "\n")
        for edge in G.edges():
            dag_dump.write(str(edge[0]) + " " + str(edge[1]) + "\n")
        dag_dump.write("\n")
        for buffer_index in buffer_dict:
            dag_dump.write(str(buffer_index) + " " + str(buffer_dict[buffer_index]) + "\n")
        dag_dump.write("\n")
        inv_node_map = {v: k for k, v in node_mapping.iteritems()}
        for task_index in task_dict:
            dag_dump.write(str(inv_node_map[task_index]) + " " +  str(task_dict[task_index].name) + "\n")
        # dag_dump.write(str(buffer_dict))
        # dag_dump.write(str(task_dict))
        dag_dump.close()
        f = open(filename, "w")
        f.write(str(n))
        f.write("\n")
        for i in range(0, n):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()
        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
        viz_graph = "names_"+graph_file[:-5] + ".png"
        print "Dumping Graph"
        self.dump_graph_names(simtaskdag, viz_graph)

    def create_fan_dag_updated(self, global_map, dag_id, ex_map, outdegree, cluster_depth, cluster_devratio, n, width, regular,
                       workitem_range, graph_file, dimension):

        class DisjointSet:

            def __init__(self, node_id_list=None):

                self.num_nodes = 0
                self.current_index = 0
                self.sizes = []
                self.node_index = {}
                self.num_components = 0
                self.node_ids = []
                self.parents = []
                self.node_depth = {}
                self.component_mappings = {}
                self.size_components = []
                self.component_device_ratio = {}
                self.task_dict = {}
                self.dag_id = dag_id

                if node_id_list is None:
                    node_id_list = []
                for node_id in node_id_list:
                    self.add(node_id)

            def insert(self, x, depth, task):
                if x in self.node_ids:
                    return

                self.node_depth[x] = depth
                self.task_dict[x] = task

                self.node_ids.append(x)
                self.node_index[x] = self.current_index
                self.component_mappings[self.current_index] = [x]
                self.parents.append(self.current_index)
                self.sizes.append(1)
                self.current_index += 1

                self.num_nodes += 1
                self.num_components += 1

            def find(self, x):

                if x not in self.node_index:
                    raise ValueError('{} is not an element'.format(x))

                p = self.node_index[x]
                while p != self.parents[p]:
                    q = self.parents[p]
                    self.parents[p] = self.parents[q]
                    p = q
                return p

            def union(self, x, y):

                xroot = self.find(x)
                yroot = self.find(y)

                if xroot == yroot:
                    return
                if self.sizes[xroot] < self.sizes[yroot]:
                    self.parents[xroot] = yroot
                    self.sizes[yroot] += self.sizes[xroot]
                    for node_id in self.component_mappings[xroot]:
                        self.component_mappings[yroot].append(node_id)
                    del self.component_mappings[xroot]

                else:
                    self.parents[yroot] = xroot
                    self.sizes[xroot] += self.sizes[yroot]
                    for node_id in self.component_mappings[yroot]:
                        self.component_mappings[xroot].append(node_id)
                    del self.component_mappings[yroot]
                self.num_components -= 1

            def get_component(self, x):
                if x not in self.node_ids:
                    raise ValueError('{} is not an element'.format(x))
                nodes = np.array(self.node_ids)
                vfind = np.vectorize(self.find)
                roots = vfind(nodes)
                return set(nodes[roots == self.find(x)])

            def get_component_stats(self, x):
                node_ids = self.get_component(x)
                num_cpu = 0.0
                num_gpu = 0.0
                for node in node_ids:
                    task = self.task_dict[node]
                    if partition_class(task) is "cpu":
                        num_cpu += 1
                    else:
                        num_gpu += 1

                percent_cpu = num_cpu / (num_gpu + num_cpu)
                percent_gpu = num_gpu / (num_gpu + num_cpu)
                return percent_cpu, percent_gpu

            def print_stats(self):
                print DJ.component_mappings

        import pygraphviz as pgv

        # Initializing graphs and task pools

        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag = nx.DiGraph()
        ex_cpu, ex_gpu = ex_map
        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
        rand_bufsize_pool = {}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)
            # print kernel_info
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
            buf_sizes = simtask.get_input_buffer_sizes()
            for buf in buf_sizes:
                if buf not in rand_bufsize_pool:
                    rand_bufsize_pool[buf] = []
                    rand_bufsize_pool[buf].append(key)
                else:
                    rand_bufsize_pool[buf].append(key)

            # DJ = DisjointSet()
            #
            # for id in range(0,10):
            #     key = random.choice(rand_bufsize_pool[random.choice(rand_bufsize_pool.keys())])
            #     DJ.insert(id,3,obtain_SimTask_object(key, global_map, ex_map))
            #
            # DJ.union(0,1)
            # DJ.union(1,2)
            # DJ.print_stats()
            # print DJ.get_component_stats(0)

        avg_tasks_per_level = exp(width * log(n))
        # print "Generating Random DAG"

        # Get number of nodes per level

        level_sizes = []
        total_tasks = 0
        # print "regular", regular
        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            # print "Number of nodes in one level " + str(temp)
            if len(level_sizes) > 0:
                if temp > level_sizes[-1] * outdegree:
                    temp = level_sizes[-1] * outdegree
            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        # print "DAGLevelSizes ",level_sizes

        import time
        # time.sleep(5)
        # indexes for buffers and nodes
        index = 0
        node_index = 0
        buffer_levels = []
        task_levels = []
        graph_levels = []
        buffer_dict = {}
        tasks = []
        buffers = []
        node_mapping = {}
        task_dict = dict()
        levels = []
        level_nodes = []

        # Initialize first level

        DJ = DisjointSet()
        cluster_depth_map = {}
        for i in range(0, level_sizes[0]):
            # dimension = random.choice([1, 2])
            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                   global_map, workitem_range, dimension)
            task_dict[node_index] = simtask
            node_mapping[index] = node_index
            G.add_node(index, shape='circle', label=simtask.name + "_" + partition_class(simtask))
            task_index = index
            cluster_depth_map[task_index] = cluster_depth
            level_nodes.append(task_index)
            dag.add_node(node_index)
            DJ.insert(task_index, cluster_depth, simtask)
            node_index += 1
            index += 1
            input_buffers = simtask.get_input_buffer_sizes()
            output_buffers = simtask.get_output_buffer_sizes()
            # print "BUFFER STATS"
            # print input_buffers
            # print output_buffers
            for buf_size in input_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(index, task_index)
                buffer_dict[index] = buf_size
                index += 1

            for buf_size in output_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(task_index, index)
                buffer_dict[index] = buf_size
                buffers.append(index)
                index += 1

        levels.append(level_nodes)
        for i in range(0, len(level_sizes) - 1):
            levels.append([])
        actual_n = n
        n = n - level_sizes[0]
        num_tasks = 0
        curr_level = 0
        next_level = 1
        num_next_level_tasks = 0
        curr_level_task = 0
        next_level_task = 0
        # print num_tasks
        # print "DAG nodes remaining", n

        while num_tasks < n:

            if curr_level == len(level_sizes) - 1:
                # print "Boundary case"
                num_tasks += 1
            else:

                if random.uniform(0, 1) < 0.5:
                    # print "DAG testing current level: ", curr_level, "curr_level_task: ", curr_level_task, "max_level_size: ", level_sizes[curr_level]
                    # print "FanOut Phase", num_tasks, n

                    # print curr_level_task, level_sizes[curr_level]

                    # print "Curr Level Update" ,curr_level_task,level_sizes[curr_level]
                    pending_task_creation = False
                    num_pending_tasks = 0
                    if curr_level_task == level_sizes[curr_level]:

                        if len(levels[next_level]) < level_sizes[next_level]:
                            # print "NextLevel Tasks Left to be added to next level"
                            num = 0
                            while (level_sizes[next_level] - len(levels[next_level]) > 0):



                                # key = random.choice(
                                #     self.select_database_devicetype(task_pool, random.choice(["cpu", "gpu"]),
                                #                                     global_map))
                                key = self.randomly_select_SimTask_key(node_index, dag_id, ex_map, rand_task_pool,
                                                                       global_map, workitem_range, dimension)

                                target_task = index
                                levels[next_level].append(target_task)
                                dag.add_node(node_index)
                                DJ.insert(target_task, cluster_depth, simtask)
                                cluster_depth_map[target_task] = cluster_depth
                                feat_dict = create_feat_dict(key, global_map)
                                extime = 0.0
                                if value_int(feat_dict['Class']) > 5:
                                    extime = ex_cpu[key]
                                else:
                                    extime = ex_gpu[key]
                                simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                task_dict[node_index] = simtask
                                G.add_node(index, shape='circle', label=simtask.name + "_" + partition_class(simtask))
                                task_index = index
                                node_mapping[task_index] = node_index
                                node_index += 1
                                index += 1
                                # Generate buffer to node relationships

                                input_buffers = simtask.get_input_buffer_sizes()
                                output_buffers = simtask.get_output_buffer_sizes()

                                # print "BUFFER STATS LEVEL ITERATION"
                                # print input_buffers
                                # print output_buffers

                                input_buffer_level = []
                                output_buffer_level = []
                                # Generate input buffers to node relationships for target task

                                for buf_size in input_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(index, task_index)
                                    buffer_dict[index] = buf_size
                                    input_buffer_level.append(index)
                                    index += 1

                                # Generate node to output buffer relationships for target task

                                for buf_size in output_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(task_index, index)
                                    buffer_dict[index] = buf_size
                                    output_buffer_level.append(index)

                                    index += 1
                                num += 1

                        curr_level_task = 0
                        curr_level += 1
                        next_level += 1
                        num_next_level_tasks = 0
                        # print "DAG  update  curr level ", curr_level, "curr_level_task: ", curr_level_task, "max level size: ", \
                        # level_sizes[curr_level]
                        # continue
                    # print "level stats" ,curr_level, next_level, len(level_sizes)
                    # print "curr level stats", curr_level, curr_level_task, len(levels[curr_level]), level_sizes[curr_level]
                    # print "DAG level stats: ", "curr_level: ", levels[curr_level], "next_level: ", next_level, "curr_level_task: ", curr_level_task
                    if curr_level == len(level_sizes) - 1:
                        break
                    source_task = levels[curr_level][curr_level_task]
                    curr_level_task += 1
                    num_edges = 0
                    if curr_level_task == level_sizes[curr_level] - 1 and num_next_level_tasks < level_sizes[next_level]:
                        num_edges = level_sizes[next_level] - num_next_level_tasks
                        if num_edges > outdegree:
                            pending_task_creation = True
                            num_pending_tasks = num_edges - outdegree
                            num_edges = outdegree




                    else:
                        num_edges = np.random.randint(1, max(min(outdegree, level_sizes[next_level]), 2))
                    # print "num edges" , num_edges
                    source_buffer_index = int(G.successors(source_task)[0])
                    source_buffer_size = buffer_dict[source_buffer_index]

                    if len(levels[next_level]) == 0:
                        # Generate new tasks and add outgoing edges for task in current level
                        # print "DAG 0 nodes in next level: ", next_level, "required", level_sizes[curr_level]
                        for num_iter in range(0, num_edges):
                            num_next_level_tasks += 1
                            # Determine task to be generated based on cluster component device ratio stats

                            cpu_percent, gpu_percent = DJ.get_component_stats(source_task)
                            task_pool = rand_bufsize_pool[source_buffer_size]
                            key = ""
                            if gpu_percent <= 1 - cluster_devratio:
                                key = random.choice(self.select_database_devicetype(task_pool, "gpu", global_map))
                            else:
                                key = random.choice(self.select_database_devicetype(task_pool, "cpu", global_map))

                            # Generate task object and update DJ component

                            target_task = index
                            levels[next_level].append(target_task)
                            dag.add_node(node_index)
                            dag.add_edge(node_mapping[source_task], node_index)
                            feat_dict = create_feat_dict(key, global_map)
                            extime = 0.0
                            if value_int(feat_dict['Class']) > 5:
                                extime = ex_cpu[key]
                            else:
                                extime = ex_gpu[key]
                            simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                            task_dict[node_index] = simtask
                            if cluster_depth_map[source_task] > 0:
                                DJ.insert(target_task, cluster_depth_map[source_task] - 1, simtask)
                                DJ.union(source_task, target_task)
                                cluster_depth_map[target_task] = cluster_depth_map[source_task] - 1
                            else:
                                DJ.insert(target_task, cluster_depth, simtask)
                                cluster_depth_map[target_task] = cluster_depth
                            G.add_node(index, shape='circle', label=simtask.name + "_" + partition_class(simtask))
                            task_index = index
                            node_mapping[task_index] = node_index
                            node_index += 1
                            index += 1

                            # Generate buffer to node relationships

                            input_buffers = simtask.get_input_buffer_sizes()
                            output_buffers = simtask.get_output_buffer_sizes()

                            # print "BUFFER STATS LEVEL ITERATION"
                            # print input_buffers
                            # print output_buffers

                            input_buffer_level = []
                            output_buffer_level = []
                            # Generate input buffers to node relationships for target task

                            for buf_size in input_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(index, task_index)
                                buffer_dict[index] = buf_size
                                input_buffer_level.append(index)
                                index += 1

                            # Generate node to output buffer relationships for target task

                            for buf_size in output_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(task_index, index)
                                buffer_dict[index] = buf_size
                                output_buffer_level.append(index)
                                index += 1

                            buffer_index = int(G.successors(source_task)[0])
                            for required_buffer_index in input_buffer_level:
                                if buffer_dict[required_buffer_index] == buffer_dict[buffer_index]:
                                    G.add_edge(buffer_index, required_buffer_index)
                                    break
                        num_tasks += 1
                        # print "DAG 0 nodes in next level scenario now contains", levels[next_level]

                    else:
                        move_current_task = False
                        # print "DAG next level is not empty", next_level
                        for num_iter in range(0, num_edges):

                            # Determine task to be generated/selected based on cluster component device ratio stats
                            if move_current_task:
                                # print "DAG move current task"
                                break
                            cpu_percent, gpu_percent = DJ.get_component_stats(source_task)
                            task_pool = rand_bufsize_pool[source_buffer_size]
                            if not (num_next_level_tasks < level_sizes[next_level]):
                                target_task = -1
                                is_task_selected = False
                                buffer_index = int(G.successors(source_task)[0])
                                if gpu_percent <= 1 - cluster_devratio:
                                    for t_index in levels[next_level]:
                                        task = task_dict[node_mapping[t_index]]
                                        if partition_class(task) is "gpu" and buffer_dict[
                                            buffer_index] in task.get_input_buffer_sizes():
                                            target_task = t_index
                                            found = False
                                            required_buffer_indices = G.predecessors(target_task)
                                            for required_buffer_index in required_buffer_indices:
                                                if len(G.predecessors(required_buffer_index)) == 0:
                                                    found = True
                                                    break
                                            if found:
                                                break
                                            else:
                                                target_task = -1
                                else:
                                    for t_index in levels[next_level]:
                                        task = task_dict[node_mapping[t_index]]
                                        if partition_class(task) is "cpu" and buffer_dict[
                                            buffer_index] in task.get_input_buffer_sizes():
                                            target_task = t_index
                                            found = False
                                            required_buffer_indices = G.predecessors(target_task)
                                            for required_buffer_index in required_buffer_indices:
                                                if len(G.predecessors(required_buffer_index)) == 0:
                                                    found = True
                                                    break
                                            if found:
                                                break
                                            else:
                                                target_task = -1

                                if target_task != -1:

                                    DJ.union(source_task, target_task)
                                    required_buffer_indices = G.predecessors(target_task)
                                    for required_buffer_index in required_buffer_indices:
                                        if buffer_dict[buffer_index] == buffer_dict[int(required_buffer_index)] and len(
                                                G.predecessors(required_buffer_index)) == 0:
                                            G.add_edge(buffer_index, required_buffer_index)
                                            dag.add_edge(node_mapping[source_task], node_mapping[target_task])
                                            break
                                else:
                                    move_current_task = True


                            else:

                                key = ""

                                if gpu_percent <= 1 - cluster_devratio:
                                    key = random.choice(self.select_database_devicetype(task_pool, "gpu", global_map))
                                else:
                                    key = random.choice(self.select_database_devicetype(task_pool, "cpu", global_map))

                                # Generate task object and update DJ component

                                target_task = index
                                levels[next_level].append(target_task)
                                dag.add_node(node_index)
                                dag.add_edge(node_mapping[source_task], node_index)
                                feat_dict = create_feat_dict(key, global_map)
                                extime = 0.0
                                if value_int(feat_dict['Class']) > 5:
                                    extime = ex_cpu[key]
                                else:
                                    extime = ex_gpu[key]
                                simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                task_dict[node_index] = simtask
                                if cluster_depth_map[source_task] > 0:
                                    DJ.insert(target_task, cluster_depth_map[source_task] - 1, simtask)
                                    DJ.union(source_task, target_task)
                                    cluster_depth_map[target_task] = cluster_depth_map[source_task] - 1
                                else:
                                    DJ.insert(target_task, cluster_depth, simtask)
                                    cluster_depth_map[target_task] = cluster_depth
                                G.add_node(index, shape='circle', label=simtask.name + "_" + partition_class(simtask))
                                task_index = index
                                node_mapping[task_index] = node_index
                                node_index += 1
                                index += 1

                                # Generate buffer to node relationships

                                input_buffers = simtask.get_input_buffer_sizes()
                                output_buffers = simtask.get_output_buffer_sizes()

                                # print "BUFFER STATS LEVEL ITERATION"
                                # print input_buffers
                                # print output_buffers

                                input_buffer_level = []
                                output_buffer_level = []
                                # Generate input buffers to node relationships for target task

                                for buf_size in input_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(index, task_index)
                                    buffer_dict[index] = buf_size
                                    input_buffer_level.append(index)
                                    index += 1

                                # Generate node to output buffer relationships for target task

                                for buf_size in output_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(task_index, index)
                                    buffer_dict[index] = buf_size
                                    output_buffer_level.append(index)
                                    index += 1

                                buffer_index = int(G.successors(source_task)[0])
                                for required_buffer_index in input_buffer_level:
                                    if buffer_dict[required_buffer_index] == buffer_dict[buffer_index]:
                                        G.add_edge(buffer_index, required_buffer_index)
                                        break
                                dont_make_new_task = True
                                num_next_level_tasks += 1
                        num_tasks += 1




                    if pending_task_creation:
                        pass






                else:
                    pass
                    # print "FanIn Phase"
                    # num_tasks +=1

        G_file = "buf_task_" + graph_file[:-5] + ".png"
        G.layout(prog='dot')
        # print "Dumping buffer tasks graph"
        # G.draw(G_file)
        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
        viz_graph = "names_" + graph_file[:-5] + ".png"
        # print "Dumping Graph"
        # self.dump_graph_names(simtaskdag, viz_graph)
        filename = graph_file
        f = open(filename, "w")
        f.write(str(len(dag.nodes())))
        f.write("\n")
        for i in range(0, len(dag.nodes())):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()

        return simtaskdag

    def create_fan_dag(self, global_map, dag_id, ex_map, outdegree, cluster_depth, cluster_devratio, n, width, regular, workitem_range, graph_file, dimension):

        class DisjointSet:

            def __init__(self, node_id_list=None):

                self.num_nodes = 0
                self.current_index = 0
                self.sizes = []
                self.node_index = {}
                self.num_components = 0
                self.node_ids = []
                self.parents = []
                self.node_depth = {}
                self.component_mappings = {}
                self.size_components = []
                self.component_device_ratio = {}
                self.task_dict = {}
                self.dag_id = dag_id

                if node_id_list is None:
                    node_id_list = []
                for node_id in node_id_list:
                    self.add(node_id)

            def insert(self, x, depth, task):
                if x in self.node_ids:
                    return

                self.node_depth[x] = depth
                self.task_dict[x] = task

                self.node_ids.append(x)
                self.node_index[x] = self.current_index
                self.component_mappings[self.current_index] = [x]
                self.parents.append(self.current_index)
                self.sizes.append(1)
                self.current_index += 1

                self.num_nodes += 1
                self.num_components += 1


            def find(self, x):

                if x not in self.node_index:
                    raise ValueError('{} is not an element'.format(x))

                p = self.node_index[x]
                while p != self.parents[p]:
                    q = self.parents[p]
                    self.parents[p] = self.parents[q]
                    p = q
                return p

            def union(self, x, y):

                xroot = self.find(x)
                yroot = self.find(y)

                if xroot == yroot:
                    return
                if self.sizes[xroot] < self.sizes[yroot]:
                    self.parents[xroot] = yroot
                    self.sizes[yroot] += self.sizes[xroot]
                    for node_id in self.component_mappings[xroot]:
                        self.component_mappings[yroot].append(node_id)
                    del self.component_mappings[xroot]

                else:
                    self.parents[yroot] = xroot
                    self.sizes[xroot] += self.sizes[yroot]
                    for node_id in self.component_mappings[yroot]:
                        self.component_mappings[xroot].append(node_id)
                    del self.component_mappings[yroot]
                self.num_components -= 1


            def get_component(self, x):
                if x not in self.node_ids:
                    raise ValueError('{} is not an element'.format(x))
                nodes = np.array(self.node_ids)
                vfind = np.vectorize(self.find)
                roots = vfind(nodes)
                return set(nodes[roots == self.find(x)])

            def get_component_stats(self, x):
                node_ids = self.get_component(x)
                num_cpu = 0.0
                num_gpu = 0.0
                for node in node_ids:
                    task = self.task_dict[node]
                    if partition_class(task) is "cpu":
                        num_cpu +=1
                    else:
                        num_gpu +=1

                percent_cpu = num_cpu/(num_gpu+num_cpu)
                percent_gpu = num_gpu / (num_gpu + num_cpu)
                return percent_cpu, percent_gpu

            def print_stats(self):
                print DJ.component_mappings



        import pygraphviz as pgv

        # Initializing graphs and task pools

        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag = nx.DiGraph()
        ex_cpu, ex_gpu = ex_map
        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
        rand_bufsize_pool = {}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)
            # print kernel_info
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
            buf_sizes = simtask.get_input_buffer_sizes()
            for buf in buf_sizes:
                if buf not in rand_bufsize_pool:
                    rand_bufsize_pool[buf] = []
                    rand_bufsize_pool[buf].append(key)
                else:
                    rand_bufsize_pool[buf].append(key)

            # DJ = DisjointSet()
            #
            # for id in range(0,10):
            #     key = random.choice(rand_bufsize_pool[random.choice(rand_bufsize_pool.keys())])
            #     DJ.insert(id,3,obtain_SimTask_object(key, global_map, ex_map))
            #
            # DJ.union(0,1)
            # DJ.union(1,2)
            # DJ.print_stats()
            # print DJ.get_component_stats(0)

        avg_tasks_per_level = exp(width * log(n))
        # print "Generating Random DAG"

        # Get number of nodes per level

        level_sizes = []
        total_tasks = 0
        # print "regular", regular
        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            # print "Number of nodes in one level " + str(temp)

            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        # print "DAGLevelSizes ",level_sizes

        import time
        # time.sleep(5)
        # indexes for buffers and nodes
        index = 0
        node_index = 0
        buffer_levels = []
        task_levels = []
        graph_levels = []
        buffer_dict = {}
        tasks = []
        buffers = []
        node_mapping = {}
        task_dict = dict()
        levels = []
        level_nodes = []


        # Initialize first level

        DJ = DisjointSet()
        cluster_depth_map = {}
        for i in range(0, level_sizes[0]):
            # dimension = random.choice([1, 2])
            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                   global_map, workitem_range, dimension)
            task_dict[node_index] = simtask
            node_mapping[index] = node_index
            G.add_node(index, shape='circle', label=simtask.name+"_"+partition_class(simtask))
            task_index = index
            cluster_depth_map[task_index] = cluster_depth
            level_nodes.append(task_index)
            dag.add_node(node_index)
            DJ.insert(task_index,cluster_depth,simtask)
            node_index += 1
            index += 1
            input_buffers = simtask.get_input_buffer_sizes()
            output_buffers = simtask.get_output_buffer_sizes()
            # print "BUFFER STATS"
            # print input_buffers
            # print output_buffers
            for buf_size in input_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(index, task_index)
                buffer_dict[index] = buf_size
                index += 1

            for buf_size in output_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(task_index, index)
                buffer_dict[index] = buf_size
                buffers.append(index)
                index += 1

        levels.append(level_nodes)
        for i in range(0, len(level_sizes) - 1):
            levels.append([])
        actual_n = n
        n = n - level_sizes[0]
        num_tasks = 0
        curr_level = 0
        next_level = 1
        num_next_level_tasks = 0
        curr_level_task = 0
        next_level_task = 0
        # print num_tasks
        # print "DAG nodes remaining", n

        while num_tasks < n:

            if curr_level == len(level_sizes) - 1:
                # print "Boundary case"
                num_tasks +=1
            else:

                if random.uniform(0, 1) < 0.5:
                    # print "DAG testing current level: ", curr_level, "curr_level_task: ", curr_level_task, "max_level_size: ", level_sizes[curr_level]
                    # print "FanOut Phase", num_tasks, n

                    # print curr_level_task, level_sizes[curr_level]



                    # print "Curr Level Update" ,curr_level_task,level_sizes[curr_level]

                    if curr_level_task == level_sizes[curr_level]:

                        if len(levels[next_level]) < level_sizes[next_level]:
                            # print "NextLevel Tasks Left to be added to next level"
                            num = 0
                            while (level_sizes[next_level] - len(levels[next_level]) > 0):


                                key = random.choice(self.select_database_devicetype(task_pool, random.choice(["cpu","gpu"]), global_map))

                                target_task = index
                                levels[next_level].append(target_task)
                                dag.add_node(node_index)
                                DJ.insert(target_task, cluster_depth, simtask)
                                cluster_depth_map[target_task] = cluster_depth
                                feat_dict = create_feat_dict(key, global_map)
                                extime = 0.0
                                if value_int(feat_dict['Class']) > 5:
                                    extime = ex_cpu[key]
                                else:
                                    extime = ex_gpu[key]
                                simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                task_dict[node_index] = simtask
                                G.add_node(index, shape='circle', label=simtask.name + "_" + partition_class(simtask))
                                task_index = index
                                node_mapping[task_index] = node_index
                                node_index += 1
                                index += 1
                                # Generate buffer to node relationships

                                input_buffers = simtask.get_input_buffer_sizes()
                                output_buffers = simtask.get_output_buffer_sizes()

                                # print "BUFFER STATS LEVEL ITERATION"
                                # print input_buffers
                                # print output_buffers

                                input_buffer_level = []
                                output_buffer_level = []
                                # Generate input buffers to node relationships for target task

                                for buf_size in input_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(index, task_index)
                                    buffer_dict[index] = buf_size
                                    input_buffer_level.append(index)
                                    index += 1

                                # Generate node to output buffer relationships for target task

                                for buf_size in output_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(task_index, index)
                                    buffer_dict[index] = buf_size
                                    output_buffer_level.append(index)

                                    index += 1
                                num += 1

                        curr_level_task = 0
                        curr_level +=1
                        next_level +=1
                        num_next_level_tasks = 0
                        # print "DAG  update  curr level ", curr_level, "curr_level_task: ", curr_level_task, "max level size: ", \
                        level_sizes[curr_level]
                        # continue
                    # print "level stats" ,curr_level, next_level, len(level_sizes)
                    # print "curr level stats", curr_level, curr_level_task, len(levels[curr_level]), level_sizes[curr_level]
                    # print "DAG level stats: ", "curr_level: ", levels[curr_level], "next_level: ", next_level, "curr_level_task: ", curr_level_task
                    if curr_level == len(level_sizes) - 1:
                        break
                    source_task = levels[curr_level][curr_level_task]
                    curr_level_task += 1
                    num_edges = 0
                    if curr_level_task == level_sizes[curr_level] - 1 and num_next_level_tasks < level_sizes[next_level]:
                        num_edges = level_sizes[next_level] - num_next_level_tasks

                    else:
                        num_edges = np.random.randint(1, max(min(outdegree, level_sizes[next_level]),2))
                    # print "num edges" , num_edges
                    source_buffer_index = int(G.successors(source_task)[0])
                    source_buffer_size = buffer_dict[source_buffer_index]

                    if len(levels[next_level]) == 0:
                        # Generate new tasks and add outgoing edges for task in current level
                        # print "DAG 0 nodes in next level: ", next_level, "required", level_sizes[curr_level]
                        for num_iter in range(0, num_edges):
                            num_next_level_tasks += 1
                            # Determine task to be generated based on cluster component device ratio stats

                            cpu_percent, gpu_percent = DJ.get_component_stats(source_task)
                            task_pool = rand_bufsize_pool[source_buffer_size]
                            key = ""
                            if gpu_percent <= 1 - cluster_devratio:
                                key = random.choice(self.select_database_devicetype(task_pool, "gpu", global_map))
                            else:
                                key = random.choice(self.select_database_devicetype(task_pool, "cpu", global_map))


                            # Generate task object and update DJ component

                            target_task = index
                            levels[next_level].append(target_task)
                            dag.add_node(node_index)
                            dag.add_edge(node_mapping[source_task], node_index)
                            feat_dict = create_feat_dict(key, global_map)
                            extime = 0.0
                            if value_int(feat_dict['Class']) > 5:
                                extime = ex_cpu[key]
                            else:
                                extime = ex_gpu[key]
                            simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                            task_dict[node_index] = simtask
                            if cluster_depth_map[source_task] > 0:
                                DJ.insert(target_task, cluster_depth_map[source_task]-1,simtask)
                                DJ.union(source_task, target_task)
                                cluster_depth_map[target_task] = cluster_depth_map[source_task]-1
                            else:
                                DJ.insert(target_task, cluster_depth, simtask)
                                cluster_depth_map[target_task] = cluster_depth
                            G.add_node(index, shape='circle', label=simtask.name+"_"+partition_class(simtask))
                            task_index = index
                            node_mapping[task_index] = node_index
                            node_index += 1
                            index += 1

                            # Generate buffer to node relationships

                            input_buffers = simtask.get_input_buffer_sizes()
                            output_buffers = simtask.get_output_buffer_sizes()

                            # print "BUFFER STATS LEVEL ITERATION"
                            # print input_buffers
                            # print output_buffers

                            input_buffer_level = []
                            output_buffer_level = []
                            # Generate input buffers to node relationships for target task

                            for buf_size in input_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(index, task_index)
                                buffer_dict[index] = buf_size
                                input_buffer_level.append(index)
                                index += 1

                            # Generate node to output buffer relationships for target task

                            for buf_size in output_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(task_index, index)
                                buffer_dict[index] = buf_size
                                output_buffer_level.append(index)
                                index += 1

                            buffer_index = int(G.successors(source_task)[0])
                            for required_buffer_index in input_buffer_level:
                                if buffer_dict[required_buffer_index] == buffer_dict[buffer_index]:
                                    G.add_edge(buffer_index, required_buffer_index)
                                    break
                        num_tasks += 1
                        # print "DAG 0 nodes in next level scenario now contains", levels[next_level]

                    else:
                        move_current_task = False
                        # print "DAG next level is not empty", next_level
                        for num_iter in range(0, num_edges):

                            # Determine task to be generated/selected based on cluster component device ratio stats
                            if move_current_task:
                                # print "DAG move current task"
                                break
                            cpu_percent, gpu_percent = DJ.get_component_stats(source_task)
                            task_pool = rand_bufsize_pool[source_buffer_size]
                            if not(num_next_level_tasks < level_sizes[next_level]):
                                target_task = -1
                                is_task_selected = False
                                buffer_index = int(G.successors(source_task)[0])
                                if gpu_percent <= 1 - cluster_devratio:
                                    for t_index in levels[next_level]:
                                        task = task_dict[node_mapping[t_index]]
                                        if partition_class(task) is "gpu" and buffer_dict[buffer_index] in task.get_input_buffer_sizes():
                                            target_task = t_index
                                            found = False
                                            required_buffer_indices = G.predecessors(target_task)
                                            for required_buffer_index in required_buffer_indices:
                                                if len(G.predecessors(required_buffer_index)) == 0:
                                                    found = True
                                                    break
                                            if found:
                                                break
                                            else:
                                                target_task = -1
                                else:
                                    for t_index in levels[next_level]:
                                        task = task_dict[node_mapping[t_index]]
                                        if partition_class(task) is "cpu" and buffer_dict[buffer_index] in task.get_input_buffer_sizes() :
                                            target_task = t_index
                                            found = False
                                            required_buffer_indices = G.predecessors(target_task)
                                            for required_buffer_index in required_buffer_indices:
                                                if len(G.predecessors(required_buffer_index)) == 0:
                                                    found = True
                                                    break
                                            if found:
                                                break
                                            else:
                                                target_task = -1

                                if target_task != -1:


                                    DJ.union(source_task, target_task)
                                    required_buffer_indices = G.predecessors(target_task)
                                    for required_buffer_index in required_buffer_indices:
                                        if buffer_dict[buffer_index] == buffer_dict[int(required_buffer_index)] and len(G.predecessors(required_buffer_index)) == 0:
                                            G.add_edge(buffer_index, required_buffer_index)
                                            dag.add_edge(node_mapping[source_task], node_mapping[target_task])
                                            break
                                else:
                                    move_current_task = True


                            else:


                                key = ""

                                if gpu_percent <= 1 - cluster_devratio:
                                    key = random.choice(self.select_database_devicetype(task_pool, "gpu", global_map))
                                else:
                                    key = random.choice(self.select_database_devicetype(task_pool, "cpu", global_map))


                                # Generate task object and update DJ component

                                target_task = index
                                levels[next_level].append(target_task)
                                dag.add_node(node_index)
                                dag.add_edge(node_mapping[source_task], node_index)
                                feat_dict = create_feat_dict(key, global_map)
                                extime = 0.0
                                if value_int(feat_dict['Class']) > 5:
                                    extime = ex_cpu[key]
                                else:
                                    extime = ex_gpu[key]
                                simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                task_dict[node_index] = simtask
                                if cluster_depth_map[source_task] > 0:
                                    DJ.insert(target_task, cluster_depth_map[source_task]-1,simtask)
                                    DJ.union(source_task, target_task)
                                    cluster_depth_map[target_task] = cluster_depth_map[source_task]-1
                                else:
                                    DJ.insert(target_task, cluster_depth, simtask)
                                    cluster_depth_map[target_task] = cluster_depth
                                G.add_node(index, shape='circle', label=simtask.name+"_"+partition_class(simtask))
                                task_index = index
                                node_mapping[task_index] = node_index
                                node_index += 1
                                index += 1

                                # Generate buffer to node relationships

                                input_buffers = simtask.get_input_buffer_sizes()
                                output_buffers = simtask.get_output_buffer_sizes()

                                # print "BUFFER STATS LEVEL ITERATION"
                                # print input_buffers
                                # print output_buffers

                                input_buffer_level = []
                                output_buffer_level = []
                                # Generate input buffers to node relationships for target task

                                for buf_size in input_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(index, task_index)
                                    buffer_dict[index] = buf_size
                                    input_buffer_level.append(index)
                                    index += 1

                                # Generate node to output buffer relationships for target task

                                for buf_size in output_buffers:
                                    G.add_node(index, shape='square', label=str(buf_size))
                                    G.add_edge(task_index, index)
                                    buffer_dict[index] = buf_size
                                    output_buffer_level.append(index)
                                    index += 1

                                buffer_index = int(G.successors(source_task)[0])
                                for required_buffer_index in input_buffer_level:
                                    if buffer_dict[required_buffer_index] == buffer_dict[buffer_index]:
                                        G.add_edge(buffer_index, required_buffer_index)
                                        break
                                dont_make_new_task = True
                                num_next_level_tasks += 1
                        num_tasks += 1











                else:
                    pass
                    # print "FanIn Phase"
                    # num_tasks +=1

        G_file = "buf_task_" + graph_file[:-5] + ".png"
        G.layout(prog='dot')
        # print "Dumping buffer tasks graph"
        # G.draw(G_file)
        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
        viz_graph = "names_" + graph_file[:-5] + ".png"
        # print "Dumping Graph"
        # self.dump_graph_names(simtaskdag, viz_graph)
        filename = graph_file
        f = open(filename, "w")
        f.write(str(len(dag.nodes())))
        f.write("\n")
        for i in range(0, len(dag.nodes())):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()

        return simtaskdag














    def create_param_dag_randomly(self, global_map, dag_id, ex_map, n, width, regular, density, workitem_range, graph_file, dimension):

        # Create random task pool indexed by data set size and work dimension
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        ex_cpu, ex_gpu = ex_map
        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
        rand_bufsize_pool = {}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)
            print kernel_info
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
            else:
                extime = ex_gpu[key]
            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
            buf_sizes = simtask.get_input_buffer_sizes()
            for buf in buf_sizes:
                if buf not in rand_bufsize_pool:
                    rand_bufsize_pool[buf] = []
                    rand_bufsize_pool[buf].append(key)
                else:
                    rand_bufsize_pool[buf].append(key)



        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        avg_tasks_per_level = exp(width * log(n))
        print avg_tasks_per_level
        level_sizes = []
        total_tasks = 0


        print "Generating Random DAG"

        # Get number of nodes per level

        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            print "Number of nodes in one level " + str(temp)
            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        index = 0
        node_index = 0
        buffer_levels = []
        task_levels = []
        graph_levels = []
        buffer_dict = {}
        tasks = []
        buffers = []
        node_mapping = {}

        dag = nx.DiGraph()
        levels = []
        level_nodes = []

        for i in range(0, level_sizes[0]):
            # dimension = random.choice([1, 2])
            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                                 global_map, workitem_range, dimension)
            task_dict[node_index] = simtask
            node_mapping[index] = node_index
            G.add_node(index, shape='circle', label=simtask.name)
            task_index = index
            level_nodes.append(task_index)
            dag.add_node(node_index)
            node_index +=1
            index += 1
            input_buffers = simtask.get_input_buffer_sizes()
            output_buffers = simtask.get_output_buffer_sizes()
            print "BUFFER STATS"
            print input_buffers
            print output_buffers
            for buf_size in input_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(index, task_index)
                buffer_dict[index] = buf_size
                index+=1

            for buf_size in output_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(task_index, index)
                buffer_dict[index] = buf_size
                buffers.append(index)
                index += 1


        buffer_levels.append(buffers)
        levels.append(level_nodes)

        print "BUFFER SIZES LEVEL 0"
        print [buffer_dict[buffer_index] for buffer_index in buffers]
        print buffer_levels

        for i in range(1, len(level_sizes)):
            level_nodes = []
            bufs = buffer_levels[-1]
            input_buffer_level = []
            output_buffer_level = []

            # Number of output buffers in previous level is less than or equal to number of nodes in current level
            print "level" + str(i)
            print bufs
            buf_sizes = [buffer_dict[buffer_index] for buffer_index in bufs]
            print buf_sizes
            if len(bufs) <= level_sizes[i]:
                for buffer_index in bufs:
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool

                    def create_dag_randomly(self, global_map, dag_id, ex_map, n, width, regular, density,
                                            workitem_range, graph_file):

                        # Create random task pool indexed by data set size and work dimension
                        import pygraphviz as pgv
                        G = pgv.AGraph(strict=False, directed=True)
                        G.node_attr['style'] = 'filled'
                        ex_cpu, ex_gpu = ex_map
                        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
                        rand_bufsize_pool = {}
                        for k in rand_task_pool.keys():
                            rand_task_pool[k] = {1: [], 2: []}
                        for key in global_map.keys():
                            kernel_info = key.split("_")
                            kernelName = kernel_info[0]
                            worksize = kernel_info[1].strip("\n")
                            if kernelName == 'uncoalesced' or kernelName == 'shared':
                                kernelName = kernelName + "_copy"
                                worksize = kernel_info[2].strip("\n")
                            if kernelName == 'transpose':
                                kernelName = kernelName + "_naive"
                                worksize = kernel_info[2].strip("\n")
                            dataset = int(worksize)
                            dimension = obtain_kernel_dimension(key)
                            rand_task_pool[dataset][dimension].append(kernelName)
                            print kernel_info
                            feat_dict = create_feat_dict(key, global_map)
                            extime = 0.0
                            if value_int(feat_dict['Class']) > 5:
                                extime = ex_cpu[key]
                            else:
                                extime = ex_gpu[key]
                            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
                            buf_sizes = simtask.get_input_buffer_sizes()
                            for buf in buf_sizes:
                                if buf not in rand_bufsize_pool:
                                    rand_bufsize_pool[buf] = []
                                    rand_bufsize_pool[buf].append(key)
                                else:
                                    rand_bufsize_pool[buf].append(key)

                        ex_cpu, ex_gpu = ex_map
                        task_dict = dict()
                        avg_tasks_per_level = exp(width * log(n))
                        print avg_tasks_per_level
                        level_sizes = []
                        total_tasks = 0

                        print "Generating Random DAG"

                        # Get number of nodes per level

                        while (True):
                            temp = get_random_integer_around(avg_tasks_per_level, regular)
                            print "Number of nodes in one level " + str(temp)
                            if total_tasks + temp > n:
                                temp = n - total_tasks
                            level_sizes.append(temp)
                            total_tasks += temp
                            if (total_tasks >= n):
                                break
                        index = 0
                        node_index = 0
                        buffer_levels = []
                        task_levels = []
                        graph_levels = []
                        buffer_dict = {}
                        tasks = []
                        buffers = []
                        node_mapping = {}

                        dag = nx.DiGraph()
                        levels = []
                        level_nodes = []

                        for i in range(0, level_sizes[0]):
                            # dimension = random.choice([1, 2])
                            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                                   global_map, workitem_range, dimension)
                            task_dict[node_index] = simtask
                            node_mapping[index] = node_index
                            G.add_node(index, shape='circle', label=simtask.name)
                            task_index = index
                            level_nodes.append(task_index)
                            dag.add_node(node_index)
                            node_index += 1
                            index += 1
                            input_buffers = simtask.get_input_buffer_sizes()
                            output_buffers = simtask.get_output_buffer_sizes()
                            print "BUFFER STATS"
                            print input_buffers
                            print output_buffers
                            for buf_size in input_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(index, task_index)
                                buffer_dict[index] = buf_size
                                index += 1

                            for buf_size in output_buffers:
                                G.add_node(index, shape='square', label=str(buf_size))
                                G.add_edge(task_index, index)
                                buffer_dict[index] = buf_size
                                buffers.append(index)
                                index += 1

                        buffer_levels.append(buffers)
                        levels.append(level_nodes)

                        print "BUFFER SIZES LEVEL 0"
                        print [buffer_dict[buffer_index] for buffer_index in buffers]
                        print buffer_levels

                        for i in range(1, len(level_sizes)):
                            level_nodes = []
                            bufs = buffer_levels[-1]
                            input_buffer_level = []
                            output_buffer_level = []

                            # Number of output buffers in previous level is less than or equal to number of nodes in current level
                            print "level" + str(i)
                            print bufs
                            buf_sizes = [buffer_dict[buffer_index] for buffer_index in bufs]
                            print buf_sizes
                            if len(bufs) <= level_sizes[i]:
                                for buffer_index in bufs:
                                    buf = buffer_dict[buffer_index]
                                    task_pool = rand_bufsize_pool[buf]
                                    print task_pool
                                    key = random.choice(self.select_database_dimension(task_pool,dimension))
                                    level_nodes.append(index)

                                    dag.add_node(node_index)

                                    feat_dict = create_feat_dict(key, global_map)
                                    extime = 0.0
                                    if value_int(feat_dict['Class']) > 5:
                                        extime = ex_cpu[key]
                                    else:
                                        extime = ex_gpu[key]
                                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                    task_dict[node_index] = simtask
                                    G.add_node(index, shape='circle', label=simtask.name)
                                    task_index = index
                                    node_mapping[task_index] = node_index
                                    node_index += 1
                                    index += 1
                                    input_buffers = simtask.get_input_buffer_sizes()
                                    output_buffers = simtask.get_output_buffer_sizes()
                                    print "BUFFER STATS LEVEL ITERATION"
                                    print input_buffers
                                    print output_buffers

                                    # Generate input buffers to node relationships for current level

                                    for buf_size in input_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(index, task_index)
                                        buffer_dict[index] = buf_size
                                        input_buffer_level.append(index)
                                        index += 1

                                    # Generate node to output buffer relationships for current level

                                    for buf_size in output_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(task_index, index)
                                        buffer_dict[index] = buf_size
                                        output_buffer_level.append(index)
                                        index += 1

                                remaining = level_sizes[i] - len(bufs)

                                while (remaining > 0):
                                    remaining -= 1
                                    buffer_index = random.choice(bufs)
                                    buf = buffer_dict[buffer_index]
                                    task_pool = rand_bufsize_pool[buf]
                                    print task_pool
                                    key = random.choice(task_pool)
                                    level_nodes.append(index)
                                    dag.add_node(node_index)

                                    feat_dict = create_feat_dict(key, global_map)
                                    extime = 0.0
                                    if value_int(feat_dict['Class']) > 5:
                                        extime = ex_cpu[key]
                                    else:
                                        extime = ex_gpu[key]
                                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                    task_dict[node_index] = simtask
                                    G.add_node(index, shape='circle', label=simtask.name)
                                    task_index = index
                                    node_mapping[task_index] = node_index
                                    node_index += 1
                                    index += 1
                                    input_buffers = simtask.get_input_buffer_sizes()
                                    output_buffers = simtask.get_output_buffer_sizes()

                                    print "BUFFER STATS LEVEL ITERATION"
                                    print input_buffers
                                    print output_buffers

                                    # Generate input buffers to node relationships for current level

                                    for buf_size in input_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(index, task_index)
                                        buffer_dict[index] = buf_size
                                        input_buffer_level.append(index)
                                        index += 1

                                    # Generate node to output buffer relationships for current level

                                    for buf_size in output_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(task_index, index)
                                        buffer_dict[index] = buf_size
                                        output_buffer_level.append(index)
                                        index += 1
                                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                                print [buffer_dict[buffer_index] for buffer_index in bufs]
                                print "INPUT BUF SIZES LEVEL ITERATION"
                                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]
                                print input_buf_sizes
                                level_edges = 0
                                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                                    level_edges = len(input_buffer_level)
                                else:
                                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)),
                                                                    len(input_buffer_level))
                                if len(bufs) <= len(input_buffer_level):
                                    edge_count = 0
                                    for buffer_index in bufs:

                                        pred_task = int(G.predecessors(buffer_index)[0])
                                        succ_task = -1
                                        source_node = -1
                                        target_node = -1
                                        required_buf_index = -1
                                        buffer_size = buffer_dict[buffer_index]
                                        probe_count = 0
                                        add_edge = False
                                        while True:
                                            required_buf_index = random.choice(input_buffer_level)
                                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(
                                                    required_buf_index) == 0:
                                                succ_task = int(G.successors(required_buf_index)[0])
                                                source_node = node_mapping[pred_task]
                                                target_node = node_mapping[succ_task]
                                                if (not dag.has_edge(source_node, target_node)):
                                                    add_edge = True
                                                    print "Edge does not exist"
                                                    break
                                                else:
                                                    print "Edge exists"
                                            probe_count += 1
                                            if probe_count > len(input_buffer_level):
                                                break
                                        if add_edge:
                                            dag.add_edge(source_node, target_node)
                                            G.add_edge(buffer_index, required_buf_index)
                                            edge_count += 1
                                        if edge_count > level_edges:
                                            break

                                    if edge_count < level_edges:
                                        for buffer_index in bufs:
                                            for required_buf_index in input_buffer_level:
                                                if buffer_dict[buffer_index] == buffer_dict[
                                                    required_buf_index] and G.in_degree(required_buf_index) == 0:
                                                    succ_task = int(G.successors(required_buf_index)[0])
                                                    pred_task = int(G.predecessors(buffer_index)[0])
                                                    if (not dag.has_edge(source_node, target_node)):
                                                        dag.add_edge(source_node, target_node)
                                                        G.add_edge(buffer_index, required_buf_index)
                                                        edge_count += 1
                                                if edge_count > level_edges:
                                                    break
                                            if edge_count > level_edges:
                                                break

                                        # for buffer_index in input_buffer_level:
                                        #     if G.in_degree(buffer_index) == 0:
                                        #         for required_buffer_index in bufs:
                                        #             if buffer_dict[buffer_index] == buffer_dict[required_buffer_index]:
                                        #                 succ_task = int(G.successors(required_buf_index)[0])
                                        #                 pred_task = int(G.predecessors(buffer_index)[0])
                                        #                 if (not dag.has_edge(source_node, target_node)):
                                        #                     dag.add_edge(source_node, target_node)
                                        #                     G.add_edge(buffer_index, required_buf_index)
                                        #                     edge_count +=1
                                        if edge_count > level_edges:
                                            break

                                bufs.append(output_buffer_level)
                            else:
                                buf_counter = 0
                                while buf_counter < level_sizes[i]:
                                    buf_counter += 1
                                    buffer_index = random.choice(bufs)
                                    buf = buffer_dict[buffer_index]
                                    task_pool = rand_bufsize_pool[buf]
                                    print task_pool
                                    key = random.choice(self.select_database_dimension(task_pool, dimension))
                                    level_nodes.append(index)

                                    dag.add_node(node_index)

                                    feat_dict = create_feat_dict(key, global_map)
                                    extime = 0.0
                                    if value_int(feat_dict['Class']) > 5:
                                        extime = ex_cpu[key]
                                    else:
                                        extime = ex_gpu[key]
                                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                                    task_dict[node_index] = simtask
                                    G.add_node(index, shape='circle', label=simtask.name)
                                    task_index = index
                                    node_mapping[task_index] = node_index
                                    node_index += 1
                                    index += 1
                                    input_buffers = simtask.get_input_buffer_sizes()
                                    output_buffers = simtask.get_output_buffer_sizes()

                                    print "BUFFER STATS LEVEL ITERATION"
                                    print input_buffers
                                    print output_buffers
                                    # Generate input buffers to node relationships for current level

                                    for buf_size in input_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(index, task_index)
                                        buffer_dict[index] = buf_size
                                        input_buffer_level.append(index)
                                        index += 1

                                    # Generate node to output buffer relationships for current level

                                    for buf_size in output_buffers:
                                        G.add_node(index, shape='square', label=str(buf_size))
                                        G.add_edge(task_index, index)
                                        buffer_dict[index] = buf_size
                                        output_buffer_level.append(index)
                                        index += 1
                                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                                print [buffer_dict[buffer_index] for buffer_index in bufs]
                                print "INPUT BUF SIZES LEVEL ITERATION"
                                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]

                                print input_buf_sizes

                                level_edges = 0
                                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                                    level_edges = len(input_buffer_level)
                                else:
                                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)),
                                                                    len(input_buffer_level))
                                edge_count = 0
                                if len(bufs) <= len(input_buffer_level):
                                    for buffer_index in bufs:
                                        pred_task = int(G.predecessors(buffer_index)[0])
                                        succ_task = -1
                                        source_node = -1
                                        target_node = -1
                                        required_buf_index = -1
                                        buffer_size = buffer_dict[buffer_index]
                                        probe_count = 0
                                        add_edge = False
                                        while True:
                                            required_buf_index = random.choice(input_buffer_level)
                                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(
                                                    required_buf_index) == 0:
                                                succ_task = int(G.successors(required_buf_index)[0])
                                                source_node = node_mapping[pred_task]
                                                target_node = node_mapping[succ_task]
                                                if (not dag.has_edge(source_node, target_node)):
                                                    add_edge = True
                                                    break
                                            probe_count += 1

                                            if probe_count > len(input_buffer_level):
                                                break

                                        if add_edge:
                                            dag.add_edge(source_node, target_node)
                                            G.add_edge(buffer_index, required_buf_index)
                                            edge_count += 1
                                        if edge_count > level_edges:
                                            break

                                    if edge_count < level_edges:
                                        for buffer_index in bufs:
                                            for required_buf_index in input_buffer_level:
                                                if buffer_dict[buffer_index] == buffer_dict[
                                                    required_buf_index] and G.in_degree(
                                                        required_buf_index) == 0:
                                                    succ_task = int(G.successors(required_buf_index)[0])
                                                    pred_task = int(G.predecessors(buffer_index)[0])
                                                    if (not dag.has_edge(source_node, target_node)):
                                                        dag.add_edge(source_node, target_node)
                                                        G.add_edge(buffer_index, required_buf_index)
                                                        edge_count += 1
                                                if edge_count > level_edges:
                                                    break
                                            if edge_count > level_edges:
                                                break







                                else:

                                    counter = 0
                                    for buffer_index in input_buffer_level:

                                        succ_task = int(G.successors(buffer_index)[0])
                                        pred_task = -1
                                        source_node = -1
                                        target_node = -1
                                        required_buf_index = -1
                                        buffer_size = buffer_dict[buffer_index]
                                        probe_count = 0
                                        add_edge = False

                                        while True:
                                            required_buf_index = random.choice(bufs)
                                            # print buffer_index
                                            # print required_buf_index
                                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(
                                                    buffer_index) == 0:
                                                print "match found"
                                                pred_task = int(G.predecessors(required_buf_index)[0])
                                                print pred_task
                                                source_node = node_mapping[pred_task]
                                                target_node = node_mapping[succ_task]

                                                if (not dag.has_edge(source_node, target_node)):
                                                    print "Edge does not exist"
                                                    add_edge = True
                                                    break
                                                else:
                                                    print "Edge exists"
                                            probe_count += 1
                                            if probe_count > len(bufs):
                                                break
                                        if add_edge:
                                            dag.add_edge(source_node, target_node)
                                            G.add_edge(required_buf_index, buffer_index)
                            print "OUTPUT BUFFER LEVEL"
                            print output_buffer_level
                            buffer_levels.append(output_buffer_level)

                        print levels
                        print buffer_levels

                        G_file = "buf_task_" + graph_file[:-5] + ".png"
                        G.layout(prog='dot')
                        print "Dumping buffer tasks graph"
                        G.draw(G_file)
                        filename = graph_file
                        dag_dump_file = "complete_dump_" + filename
                        dag_dump = open(dag_dump_file, 'w')
                        dag_dump.write(str(len(G.nodes())) + "\n")
                        for edge in G.edges():
                            dag_dump.write(str(edge[0]) + " " + str(edge[1]) + "\n")
                        dag_dump.write("\n")
                        for buffer_index in buffer_dict:
                            dag_dump.write(str(buffer_index) + " " + str(buffer_dict[buffer_index]) + "\n")
                        dag_dump.write("\n")
                        inv_node_map = {v: k for k, v in node_mapping.iteritems()}
                        for task_index in task_dict:
                            dag_dump.write(str(inv_node_map[task_index]) + " " + str(task_dict[task_index].name) + "\n")
                        # dag_dump.write(str(buffer_dict))
                        # dag_dump.write(str(task_dict))
                        dag_dump.close()
                        f = open(filename, "w")
                        f.write(str(n))
                        f.write("\n")
                        for i in range(0, n):
                            f.write(task_dict[i].name)
                            f.write("\n")
                        for e in dag.edges():
                            u, v = e
                            edge = str(u) + " " + str(v)
                            f.write(edge)
                            f.write("\n")
                        f.close()
                        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
                        viz_graph = "names_" + graph_file[:-5] + ".png"
                        print "Dumping Graph"
                        self.dump_graph_names(simtaskdag, viz_graph)

                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()
                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1

                remaining = level_sizes[i] - len(bufs)


                while(remaining > 0):
                    remaining -= 1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(self.select_database_dimension(task_pool, dimension))
                    level_nodes.append(index)
                    dag.add_node(node_index)

                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]
                print input_buf_sizes
                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                if len(bufs) <= len(input_buffer_level):
                    edge_count = 0
                    for buffer_index in bufs:

                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if(not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    print "Edge does not exist"
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(input_buffer_level):
                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count +=1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break



                    # for buffer_index in input_buffer_level:
                    #     if G.in_degree(buffer_index) == 0:
                    #         for required_buffer_index in bufs:
                    #             if buffer_dict[buffer_index] == buffer_dict[required_buffer_index]:
                    #                 succ_task = int(G.successors(required_buf_index)[0])
                    #                 pred_task = int(G.predecessors(buffer_index)[0])
                    #                 if (not dag.has_edge(source_node, target_node)):
                    #                     dag.add_edge(source_node, target_node)
                    #                     G.add_edge(buffer_index, required_buf_index)
                    #                     edge_count +=1
                        if edge_count >level_edges:
                            break









                bufs.append(output_buffer_level)
            else:
                buf_counter = 0
                while buf_counter < level_sizes[i]:
                    buf_counter +=1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(self.select_database_dimension(task_pool, dimension))
                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers
                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]

                print input_buf_sizes

                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                edge_count = 0
                if len(bufs) <= len(input_buffer_level):
                    for buffer_index in bufs:
                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if (not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    break
                            probe_count +=1

                            if probe_count > len(input_buffer_level):
                                break

                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(
                                        required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count += 1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break







                else:

                    counter = 0
                    for buffer_index in input_buffer_level:

                        succ_task = int(G.successors(buffer_index)[0])
                        pred_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False

                        while True:
                            required_buf_index = random.choice(bufs)
                            # print buffer_index
                            # print required_buf_index
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(buffer_index) == 0:
                                print "match found"
                                pred_task = int(G.predecessors(required_buf_index)[0])
                                print pred_task
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]

                                if (not dag.has_edge(source_node, target_node)):
                                    print "Edge does not exist"
                                    add_edge = True
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(bufs):

                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(required_buf_index, buffer_index)
            print "OUTPUT BUFFER LEVEL"
            print output_buffer_level
            buffer_levels.append(output_buffer_level)


        print levels
        print buffer_levels


        G_file = "buf_task_" + graph_file[:-5] + ".png"
        G.layout(prog='dot')
        print "Dumping buffer tasks graph"
        G.draw(G_file)
        filename = graph_file
        dag_dump_file = "complete_dump_"+filename
        dag_dump = open(dag_dump_file, 'w')
        dag_dump.write(str(len(G.nodes())) + "\n")
        for edge in G.edges():
            dag_dump.write(str(edge[0]) + " " + str(edge[1]) + "\n")
        dag_dump.write("\n")
        for buffer_index in buffer_dict:
            dag_dump.write(str(buffer_index) + " " + str(buffer_dict[buffer_index]) + "\n")
        dag_dump.write("\n")
        inv_node_map = {v: k for k, v in node_mapping.iteritems()}
        for task_index in task_dict:
            dag_dump.write(str(inv_node_map[task_index]) + " " +  str(task_dict[task_index].name) + "\n")
        # dag_dump.write(str(buffer_dict))
        # dag_dump.write(str(task_dict))
        dag_dump.close()
        f = open(filename, "w")
        f.write(str(n))
        f.write("\n")
        for i in range(0, n):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()
        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
        viz_graph = "names_"+graph_file[:-5] + ".png"
        print "Dumping Graph"
        self.dump_graph_names(simtaskdag, viz_graph)


    def create_clustered_dag_randomly(self, global_map, dag_id, ex_map, n, width, regular, density, workitem_range, graph_file, percent_GPU):

        # Create random task pool indexed by data set size and work dimension
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        ex_cpu, ex_gpu = ex_map
        rand_task_pool = {k: {} for k in [128 * 2 ** (r - 1) for r in range(1, 8)]}
        rand_bufsize_pool = {}
        rand_dev_bufsize_pool = {'cpu': {}, 'gpu': {}}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)
            print kernel_info
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            device_class = ""
            if value_int(feat_dict['Class']) > 5:
                extime = ex_cpu[key]
                device_class = "cpu"
            else:
                extime = ex_gpu[key]
                device_class = "gpu"
            simtask = SimTask(key, 0, dag_id, feat_dict, extime)
            buf_sizes = simtask.get_input_buffer_sizes()
            for buf in buf_sizes:
                if buf not in rand_dev_bufsize_pool:
                    rand_bufsize_pool[buf] = []
                    rand_bufsize_pool[buf].append(key)
                else:
                    rand_bufsize_pool[buf].append(key)



        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        avg_tasks_per_level = exp(width * log(n))
        print avg_tasks_per_level
        level_sizes = []
        total_tasks = 0


        print "Generating Random DAG"

        # Get number of nodes per level

        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            print "Number of nodes in one level " + str(temp)
            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        index = 0
        node_index = 0
        buffer_levels = []
        task_levels = []
        graph_levels = []
        buffer_dict = {}
        tasks = []
        buffers = []
        node_mapping = {}

        dag = nx.DiGraph()
        levels = []
        level_nodes = []

        for i in range(0, level_sizes[0]):
            dimension = random.choice([1, 2])
            simtask = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool,
                                                                 global_map, workitem_range, dimension)
            task_dict[node_index] = simtask
            node_mapping[index] = node_index
            G.add_node(index, shape='circle', label=simtask.name)
            task_index = index
            level_nodes.append(task_index)
            dag.add_node(node_index)
            node_index +=1
            index += 1
            input_buffers = simtask.get_input_buffer_sizes()
            output_buffers = simtask.get_output_buffer_sizes()
            print "BUFFER STATS"
            print input_buffers
            print output_buffers
            for buf_size in input_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(index, task_index)
                buffer_dict[index] = buf_size
                index+=1

            for buf_size in output_buffers:
                G.add_node(index, shape='square', label=str(buf_size))
                G.add_edge(task_index, index)
                buffer_dict[index] = buf_size
                buffers.append(index)
                index += 1


        buffer_levels.append(buffers)
        levels.append(level_nodes)

        print "BUFFER SIZES LEVEL 0"
        print [buffer_dict[buffer_index] for buffer_index in buffers]
        print buffer_levels

        for i in range(1, len(level_sizes)):
            level_nodes = []
            bufs = buffer_levels[-1]
            input_buffer_level = []
            output_buffer_level = []

            # Number of output buffers in previous level is less than or equal to number of nodes in current level
            print "level" + str(i)
            print bufs
            buf_sizes = [buffer_dict[buffer_index] for buffer_index in bufs]
            print buf_sizes
            if len(bufs) <= level_sizes[i]:
                for buffer_index in bufs:
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()
                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1

                remaining = level_sizes[i] - len(bufs)


                while(remaining > 0):
                    remaining -= 1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)
                    dag.add_node(node_index)

                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers

                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]
                print input_buf_sizes
                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                if len(bufs) <= len(input_buffer_level):
                    edge_count = 0
                    for buffer_index in bufs:

                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if(not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    print "Edge does not exist"
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(input_buffer_level):
                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count +=1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break



                    # for buffer_index in input_buffer_level:
                    #     if G.in_degree(buffer_index) == 0:
                    #         for required_buffer_index in bufs:
                    #             if buffer_dict[buffer_index] == buffer_dict[required_buffer_index]:
                    #                 succ_task = int(G.successors(required_buf_index)[0])
                    #                 pred_task = int(G.predecessors(buffer_index)[0])
                    #                 if (not dag.has_edge(source_node, target_node)):
                    #                     dag.add_edge(source_node, target_node)
                    #                     G.add_edge(buffer_index, required_buf_index)
                    #                     edge_count +=1
                        if edge_count >level_edges:
                            break









                bufs.append(output_buffer_level)
            else:
                buf_counter = 0
                while buf_counter < level_sizes[i]:
                    buf_counter +=1
                    buffer_index = random.choice(bufs)
                    buf = buffer_dict[buffer_index]
                    task_pool = rand_bufsize_pool[buf]
                    print task_pool
                    key = random.choice(task_pool)
                    level_nodes.append(index)

                    dag.add_node(node_index)


                    feat_dict = create_feat_dict(key, global_map)
                    extime = 0.0
                    if value_int(feat_dict['Class']) > 5:
                        extime = ex_cpu[key]
                    else:
                        extime = ex_gpu[key]
                    simtask = SimTask(key, node_index, dag_id, feat_dict, extime)
                    task_dict[node_index] = simtask
                    G.add_node(index, shape='circle', label=simtask.name)
                    task_index = index
                    node_mapping[task_index] = node_index
                    node_index += 1
                    index += 1
                    input_buffers = simtask.get_input_buffer_sizes()
                    output_buffers = simtask.get_output_buffer_sizes()

                    print "BUFFER STATS LEVEL ITERATION"
                    print input_buffers
                    print output_buffers
                    # Generate input buffers to node relationships for current level

                    for buf_size in input_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(index, task_index)
                        buffer_dict[index] = buf_size
                        input_buffer_level.append(index)
                        index += 1

                    # Generate node to output buffer relationships for current level

                    for buf_size in output_buffers:
                        G.add_node(index, shape='square', label=str(buf_size))
                        G.add_edge(task_index, index)
                        buffer_dict[index] = buf_size
                        output_buffer_level.append(index)
                        index += 1
                print "BUFFER SIZES PREVIOUS LEVEL ITERATION"
                print [buffer_dict[buffer_index] for buffer_index in bufs]
                print "INPUT BUF SIZES LEVEL ITERATION"
                input_buf_sizes = [buffer_dict[buffer_index] for buffer_index in input_buffer_level]

                print input_buf_sizes

                level_edges = 0
                if 1 + int(density * len(input_buffer_level)) >= len(input_buffer_level):
                    level_edges = len(input_buffer_level)
                else:
                    level_edges = np.random.randint(1 + int(density * len(input_buffer_level)), len(input_buffer_level))
                edge_count = 0
                if len(bufs) <= len(input_buffer_level):
                    for buffer_index in bufs:
                        pred_task = int(G.predecessors(buffer_index)[0])
                        succ_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False
                        while True:
                            required_buf_index = random.choice(input_buffer_level)
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(required_buf_index) == 0:
                                succ_task = int(G.successors(required_buf_index)[0])
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]
                                if (not dag.has_edge(source_node, target_node)):
                                    add_edge = True
                                    break
                            probe_count +=1

                            if probe_count > len(input_buffer_level):
                                break

                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(buffer_index, required_buf_index)
                            edge_count += 1
                        if edge_count > level_edges:
                            break

                    if edge_count < level_edges:
                        for buffer_index in bufs:
                            for required_buf_index in input_buffer_level:
                                if buffer_dict[buffer_index] == buffer_dict[required_buf_index] and G.in_degree(
                                        required_buf_index) == 0:
                                    succ_task = int(G.successors(required_buf_index)[0])
                                    pred_task = int(G.predecessors(buffer_index)[0])
                                    if (not dag.has_edge(source_node, target_node)):
                                        dag.add_edge(source_node, target_node)
                                        G.add_edge(buffer_index, required_buf_index)
                                        edge_count += 1
                                if edge_count > level_edges:
                                    break
                            if edge_count > level_edges:
                                break







                else:

                    counter = 0
                    for buffer_index in input_buffer_level:

                        succ_task = int(G.successors(buffer_index)[0])
                        pred_task = -1
                        source_node = -1
                        target_node = -1
                        required_buf_index = -1
                        buffer_size = buffer_dict[buffer_index]
                        probe_count = 0
                        add_edge = False

                        while True:
                            required_buf_index = random.choice(bufs)
                            # print buffer_index
                            # print required_buf_index
                            if buffer_dict[required_buf_index] == buffer_size and G.in_degree(buffer_index) == 0:
                                print "match found"
                                pred_task = int(G.predecessors(required_buf_index)[0])
                                print pred_task
                                source_node = node_mapping[pred_task]
                                target_node = node_mapping[succ_task]

                                if (not dag.has_edge(source_node, target_node)):
                                    print "Edge does not exist"
                                    add_edge = True
                                    break
                                else:
                                    print "Edge exists"
                            probe_count += 1
                            if probe_count > len(bufs):

                                break
                        if add_edge:
                            dag.add_edge(source_node, target_node)
                            G.add_edge(required_buf_index, buffer_index)
            print "OUTPUT BUFFER LEVEL"
            print output_buffer_level
            buffer_levels.append(output_buffer_level)


        print levels
        print buffer_levels


        G_file = "buf_task_" + graph_file[:-5] + ".png"
        G.layout(prog='dot')
        print "Dumping buffer tasks graph"
        G.draw(G_file)
        filename = graph_file
        dag_dump_file = "complete_dump_"+filename
        dag_dump = open(dag_dump_file, 'w')
        dag_dump.write(str(len(G.nodes())) + "\n")
        for edge in G.edges():
            dag_dump.write(str(edge[0]) + " " + str(edge[1]) + "\n")
        dag_dump.write("\n")
        for buffer_index in buffer_dict:
            dag_dump.write(str(buffer_index) + " " + str(buffer_dict[buffer_index]) + "\n")
        dag_dump.write("\n")
        inv_node_map = {v: k for k, v in node_mapping.iteritems()}
        for task_index in task_dict:
            dag_dump.write(str(inv_node_map[task_index]) + " " +  str(task_dict[task_index].name) + "\n")
        # dag_dump.write(str(buffer_dict))
        # dag_dump.write(str(task_dict))
        dag_dump.close()
        f = open(filename, "w")
        f.write(str(n))
        f.write("\n")
        for i in range(0, n):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()
        simtaskdag = SimTaskDAG(task_dict, dag, dag_id, ex_map)
        viz_graph = "names_"+graph_file[:-5] + ".png"
        print "Dumping Graph"
        self.dump_graph_names(simtaskdag, viz_graph)

    def create_OpenCLDAG_randomly(self, global_map, dag_id, ex_map, n, width, regular, density, workitem_range, graph_file):
        """
        Takes a set of parameters pertaining to the nature of the graph, a dictionary of execution time estimates and
        randomly generates a DAG of OpenCL Kernels

        Args:
            global_map ():
            dag_id ():
            ex_map ():
            n ():
            width ():
            regular ():
            density ():
            workitem_range ():

        """

        # Create task pool indexed according to workitem size and workdimension

        rand_task_pool ={k : {} for k in [128 * 2 ** (r-1) for r in range(1,8)]}
        for k in rand_task_pool.keys():
            rand_task_pool[k] = {1: [], 2: []}
        for key in global_map.keys():
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            dataset = int(worksize)
            dimension = obtain_kernel_dimension(key)
            rand_task_pool[dataset][dimension].append(kernelName)


        ex_cpu, ex_gpu = ex_map
        task_dict = dict()
        avg_tasks_per_level = exp(width * log(n))
        print avg_tasks_per_level
        level_sizes = []
        total_tasks = 0
        # Determine contents of levels and populates with node ids for DAG
        print "Generating Random DAG"
        while (True):
            temp = get_random_integer_around(avg_tasks_per_level, regular)
            print "Number of nodes in one level " + str(temp)
            if total_tasks + temp > n:
                temp = n - total_tasks
            level_sizes.append(temp)
            total_tasks += temp
            if (total_tasks >= n):
                break
        print level_sizes
        node_index = 0
        dag = nx.DiGraph()
        levels = []
        for level in level_sizes:
            level_nodes = []
            for i in range(0, level):
                dag.add_node(node_index)
                level_nodes.append(node_index)
                node_index += 1
            levels.append(level_nodes)
        print dag.nodes()
        print levels

        # Constrained SimTask node generation for the DAG

        # Initialize level 1
        for i in range(0, len(levels[0])):
            node_index = levels[0][i]
            dimension = random.choice([1, 2])
            task_dict[node_index] = self.randomly_select_SimTask(node_index, dag_id, ex_map, rand_task_pool, global_map, workitem_range, dimension)
            print "Level 0"
            print node_index
            print task_dict[node_index].name
            print task_dict[node_index].get_dimension()

        # First Pass Update levels
        for i in range(1, len(levels)):
            for j in range(0, len(levels[i])):
                task_node = levels[i][j]
                num_parents = min(1 + int(density * len(levels[i - 1])), len(levels[i - 1]))
                existing_parents = []
                for k in range(0, num_parents):
                    parent_index = np.random.randint(0, len(levels[i - 1]))
                    if parent_index not in existing_parents:
                        parent_node = levels[i - 1][parent_index]
                        print parent_node
                        dataset = task_dict[parent_node].get_dataset()
                        dimension = task_dict[parent_node].get_dimension()
                        if task_node not in task_dict.keys():
                            task_dict[task_node] = self.randomly_select_SimTask(task_node, dag_id, ex_map, rand_task_pool, global_map, [dataset, dataset], dimension)

                        if task_dict[task_node].get_dataset() == task_dict[parent_node].get_dataset():
                            if dag.in_degree(task_node) < task_dict[task_node].get_num_input_buffers():
                                dag.add_edge(parent_node, task_node, weight=0.0, time=0.0)


                        existing_parents.append(parent_index)

        # Second pass Update levels for disconnected nodes

        for i in range(0, len(levels)-1):
            for j in range(0, len(levels[i])):
                task_node = levels[i][j]
                if dag.out_degree(task_node) == 0:
                    for level_index in range(i+1, len(levels)-1):
                        for child_index in range(0, len(levels[level_index])):
                            child_node = levels[level_index][child_index]
                            if task_dict[child_node].get_num_input_buffers() > dag.in_degree(child_node):
                                flag = 0
                                dag.add_edge(task_node, child_node)
                                break





        # for i in range(0, len(levels)):
        #     for j in range(0, len(levels[i])):
        #         key = random.choice(global_map.keys())
        #         feat_dict = create_feat_dict(key, global_map)
        #         extime = 0.0
        #         if value_int(feat_dict['Class']) > 5:
        #             extime = ex_cpu[key]
        #         else:
        #             extime = ex_gpu[key]
        #         node_index = levels[i][j]
        #         task_dict[node_index] = SimTask(key, node_index, dag_id, feat_dict, extime)
        #         kernel_object = task_dict[node_index].Kernel_Object
        #         print j,
        #         print kernel_object.name
        #         print kernel_object.work_dimension
        #         print kernel_object.global_work_size
        #         print "Input Buffers: " + str(len(kernel_object.buffer_info['input']))
        #         print "Output Buffers: " + str(len(kernel_object.buffer_info['output']))


        # # Determine edges across levels and nodes based on density parameter and workdimension constraint
        # for i in range(1, len(levels)):
        #     for j in range(0, len(levels[i])):
        #         task_node = levels[i][j]
        #         num_parents = min(1 + int(density * len(levels[i - 1])), len(levels[i - 1]))
        #         existing_parents = []
        #         for k in range(0, num_parents):
        #             parent_index = np.random.randint(0, len(levels[i - 1]))
        #             if parent_index not in existing_parents:
        #                 parent_node = levels[i - 1][parent_index]
        #                 dag.add_edge(parent_node, task_node, weight=0.0, time=0.0)
        #                 existing_parents.append(parent_index)
        # print dag.edges()

        # Dump graph in file

        filename = "dag_" + str(n) + "_" + str(width) + "_" + str(regular) + "_" + str(density) + ".graph"
        f = open(filename, "w")
        f.write(str(n))
        f.write("\n")
        for i in range(0, n):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()

        filename = graph_file
        f = open(filename, "w")
        f.write(str(n))
        f.write("\n")
        for i in range(0, n):
            f.write(task_dict[i].name)
            f.write("\n")
        for e in dag.edges():
            u, v = e
            edge = str(u) + " " + str(v)
            f.write(edge)
            f.write("\n")
        f.close()
        return SimTaskDAG(task_dict, dag, dag_id, ex_map)

    def visualize_graph(self, dag):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class(dag.tasks[node])
            G.add_node(node, label=Class)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v)
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_ids(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():

            G.add_node(node, label=str(node))
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v)


        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)


    def dump_graph_class(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class_absolute(dag.tasks[node])
            G.add_node(node, label=Class)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v)
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_DT_ECO(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class(dag.tasks[node])
            G.add_node(node, label=dag.tasks[node].ECO)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v, label=S[u][v]['weight'])
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['style'] = 'filled'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_start_finish_times(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            name = dag.tasks[node].name
            rank = dag.tasks[node].rank
            
            device_type = dag.tasks[node].device_type
            device_id = str(dag.tasks[node].device_id)
            start_time = (dag.tasks[node].start_time)
            finish_time = (dag.tasks[node].finish_time)
            execution_time = finish_time - start_time
            label_string =  str(node) + " " + device_type + " " + str(start_time)
            # label_string = name + "\n" + "RANK: " + str(rank) +"\n" + str(start_time) + " " + str(
                # finish_time) + "\n" + device_type + device_id + ", " + str(execution_time)
            G.add_node(node, label=label_string)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v, S[u][v]['time'])
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_heatmap_exec(self, dag1_s, dag2_s, dag_filename):
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag1 = dag1_s.skeleton
        dag1 = dag2_s.skeleton
        T = dag2_s.G
        for node in dag1.nodes():
            name = dag1_s.tasks[node].name
            start_time1 = (dag1_s.tasks[node].start_time)
            finish_time1 = (dag1_s.tasks[node].finish_time)
            execution_time1 = finish_time1 - start_time1
            list_time_string = str(start_time1) + " " + str(finish_time1)
            start_time2 = (dag2_s.tasks[node].start_time)
            finish_time2 = (dag2_s.tasks[node].finish_time)
            execution_time2 = finish_time2 - start_time2
            contract_time_string = str(start_time2) + " " + str(finish_time2)
            speedup = execution_time1/execution_time2

            label_string = name + "\n" +str(speedup) + "\n" + list_time_string + "\n" + contract_time_string + "\n"
            time_diff = finish_time1 - finish_time2
            color = ''
            if time_diff < 0:
                color = 'red'
            else:
                color = 'green'

            # if speedup <=1 :
            #     color = 'red'
            # elif speedup <= 1.25:
            #     color = 'blue'
            # elif speedup <= 1.5:
            #     color = 'turquoise'
            # elif speedup <= 1.75:
            #     color = 'green'
            # else:
            #     color = 'darkgreen'
            G.add_node(node, label=label_string, fillcolor=color)

        for edge in dag1.edges():
            u, v = edge
            G.add_edge(u, v, dag1[u][v]['time'])



        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(dag_filename)

    def dump_graph_heatmap_info(self, dag1_s, dag2_s, dag_filename):
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag1 = dag1_s.skeleton
        dag1 = dag2_s.skeleton
        T = dag2_s.G
        for node in dag1.nodes():
            name = dag1_s.tasks[node].name
            start_time1 = (dag1_s.tasks[node].start_time)
            finish_time1 = (dag1_s.tasks[node].finish_time)
            execution_time1 = finish_time1 - start_time1
            list_time_string = str(start_time1) + " " + str(finish_time1)
            start_time2 = (dag2_s.tasks[node].start_time)
            finish_time2 = (dag2_s.tasks[node].finish_time)
            execution_time2 = finish_time2 - start_time2
            contract_time_string = str(start_time2) + " " + str(finish_time2)
            speedup = execution_time1/execution_time2
            rank = dag1_s.tasks[node].rank
            step1 = dag1_s.tasks[node].dispatch_step
            step2 = dag2_s.tasks[node].dispatch_step
            ex1 = finish_time1 - start_time1
            ex2 = finish_time2 - start_time2
            fsp = finish_time2/finish_time1
            label_string = name + "\nEXSP: " + str(speedup) + "\n" + "L: " + str(ex1) + " C: " + str(ex2) + "\nFSP: " + str(fsp) + "\n"+ "\nL: " + list_time_string + "\nC: " + contract_time_string + "\nRANK " + str(rank) + "\n" + "L: " + str(step1) + " C: " + str(step2)

            time_diff = finish_time1 - finish_time2

            color = ''
            if time_diff <= 0:
                color = 'red'
            else:
                color = 'green'

            # if speedup <=1 :
            #     color = 'red'
            # elif speedup <= 1.25:
            #     color = 'blue'
            # elif speedup <= 1.5:
            #     color = 'turquoise'
            # elif speedup <= 1.75:
            #     color = 'green'
            # else:
            #     color = 'darkgreen'
            G.add_node(node, label=label_string, penwidth=5.75, style='dashed', color=color, fillcolor='grey')

        for edge in dag1.edges():
            u, v = edge
            G.add_edge(u, v, dag1[u][v]['time'])


        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['style'] = 'filled'
                    n.attr['fillcolor'] = color




        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(dag_filename)












    def dump_graph_heatmap_holistic_info(self, dag1_s, dag2_s, dag_filename):
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag1 = dag1_s.skeleton
        dag1 = dag2_s.skeleton
        T = dag2_s.G
        for node in dag1.nodes():
            name = dag1_s.tasks[node].name
            device_type1 = dag1_s.tasks[node].device_type
            device_id1 = str(dag1_s.tasks[node].device_id)
            device_type2 = dag2_s.tasks[node].device_type
            device_id2 = str(dag2_s.tasks[node].device_id)
            start_time1 = (dag1_s.tasks[node].start_time)
            finish_time1 = (dag1_s.tasks[node].finish_time)
            execution_time1 = finish_time1 - start_time1
            list_time_string = str(start_time1) + " " + str(finish_time1)
            start_time2 = (dag2_s.tasks[node].start_time)
            finish_time2 = (dag2_s.tasks[node].finish_time)
            execution_time2 = finish_time2 - start_time2
            contract_time_string = str(start_time2) + " " + str(finish_time2)
            speedup = execution_time1/execution_time2
            rank = dag1_s.tasks[node].rank
            step1 = dag1_s.tasks[node].dispatch_step
            step2 = dag2_s.tasks[node].dispatch_step
            ex1 = finish_time1 - start_time1
            ex2 = finish_time2 - start_time2
            fsp = finish_time2/finish_time1
            # label_string = name + "\nEXSP: " + str(speedup) + "\n" + "L: " + str(ex1) + " C: " + str(ex2) + "\nFSP: " + str(fsp) + "\n"+ "\nL: " + list_time_string + "\nC: " + contract_time_string + "\nRANK " + str(rank) + "\n" + "L: " + str(step1) + " C: " + str(step2)
            label_string = name + "1.0\n" + device_type1 + device_id1 + " --> " + device_type2 + device_id2
            time_diff = finish_time1 - finish_time2

            color = ''
            if time_diff <= 0:
                color = 'red'
            else:
                color = 'green'

            # if speedup <=1 :
            #     color = 'red'
            # elif speedup <= 1.25:
            #     color = 'blue'
            # elif speedup <= 1.5:
            #     color = 'turquoise'
            # elif speedup <= 1.75:
            #     color = 'green'
            # else:
            #     color = 'darkgreen'
            G.add_node(node, label=label_string, penwidth=5.75, style='dashed', color=color, fillcolor='grey')

        for edge in dag1.edges():
            u, v = edge
            G.add_edge(u, v, dag1[u][v]['time'])


        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                holistic_speedup = 1.0
                ex1 = 0.0
                ex2 = 0.0
                for k in kid:
                    ex1_t= float(dag1_s.tasks[k].finish_time) - float(dag1_s.tasks[k].start_time)
                    ex2_t = float(dag2_s.tasks[k].finish_time) - float(dag2_s.tasks[k].start_time)
                    ex1 += ex1_t
                    ex2 += ex2_t
                holistic_speedup = ex1/ex2

                nodes = node.get_kernel_ids()
                for k in nodes:
                    name = dag2_s.tasks[k].name
                    n = G.get_node(k)
                    device_type1 = dag1_s.tasks[k].device_type
                    device_id1 = str(dag1_s.tasks[k].device_id)
                    device_type2 = dag2_s.tasks[k].device_type
                    device_id2 = str(dag2_s.tasks[k ].device_id)
                    n.attr['shape'] = 'square'
                    n.attr['style'] = 'filled'
                    n.attr['fillcolor'] = color
                    n.attr['label'] = name + " " + str(holistic_speedup) + "\n" + device_type1 + device_id1 + " --> " + device_type2 + device_id2





        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(dag_filename)

    def dump_graph_heatmap(self, dag1_s, dag2_s, dag_filename):
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        dag1 = dag1_s.skeleton
        dag1 = dag2_s.skeleton

        for node in dag1.nodes():
            name = dag1_s.tasks[node].name
            device_type1 = dag1_s.tasks[node].device_type
            device_id1 = str(dag1_s.tasks[node].device_id)
            device_type2 = dag2_s.tasks[node].device_type
            device_id2 = str(dag2_s.tasks[node].device_id)
            start_time1 = (dag1_s.tasks[node].start_time)
            finish_time1 = (dag1_s.tasks[node].finish_time)
            execution_time1 = finish_time1 - start_time1
            list_time_string = str(start_time1) + " " + str(finish_time1)
            start_time2 = (dag2_s.tasks[node].start_time)
            finish_time2 = (dag2_s.tasks[node].finish_time)
            execution_time2 = finish_time2 - start_time2
            contract_time_string = str(start_time2) + " " + str(finish_time2)
            speedup = execution_time1/execution_time2

            # label_string = name + "\n" +str(speedup) + "\n" + list_time_string + "\n" + contract_time_string + "\n"
            label_string = name + "\n" + str(speedup) + "\n" + device_type1 + device_id1 + "-->" + device_type2+device_id2 + "\n" + str(execution_time1) + "-->" + str(execution_time2)
            color = ''
            if speedup <=1 :
                color = 'red'
            elif speedup <= 1.5:
                color = 'blue'
            elif speedup <= 3:
                color = 'turquoise'
            elif speedup <= 6:
                color = 'green'
            else:
                color = 'darkgreen'
            G.add_node(node, label=label_string, penwidth=5.75, color=color, fillcolor='grey')

        for edge in dag1.edges():
            u, v = edge
            G.add_edge(u, v, dag1[u][v]['time'])

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(dag_filename)

    def dump_graph_component_classes(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class(dag.tasks[node])
            G.add_node(node, label=str(dag.tasks[node].device_type) + str(dag.tasks[node].device_id) + " " + str(dag.tasks[node].execution_time))
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v, label=S[u][v]['time'])
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = 'red'
                if partition_class_value(node.get_first_kernel().Class) == "gpu":
                    color = 'green'
                else:
                    color = 'blue'
                # color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_execution_times(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class(dag.tasks[node])
            G.add_node(node, label=dag.tasks[node].execution_time)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v, label=S[u][v]['time'])
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)

    def dump_graph_names(self, dag, file_name):
        '''
        G = dag.skeleton
        plot = figure(title="Graph Plotting", x_range=(-1.1, 1.1), y_range=(-1.1, 1.1),
                      tools="", toolbar_location=None)
        graph = from_networkx(G, nx.spring_layout, scale=2, center=(0, 0))
        plot.renderers.append(graph)
        output_file("visualize_graph.html")
        show(plot)
        '''
        import pygraphviz as pgv
        G = pgv.AGraph(strict=False, directed=True)
        G.node_attr['style'] = 'filled'
        S = dag.skeleton
        T = dag.G
        for node in S.nodes():
            Class = partition_class(dag.tasks[node])
            G.add_node(node, label=dag.tasks[node].name)
        for edge in S.edges():
            u, v = edge
            G.add_edge(u, v)
        for node in T:
            if node.is_supertask():
                r = lambda: random.randint(0, 255)
                # print('#%02X%02X%02X' % (r(),r(),r()))
                color = "#" + str(format(r(), '02x')) + str(format(r(), '02x')) + str(format(r(), '02x'))
                kid = node.get_kernel_ids()
                for k in kid:
                    n = G.get_node(k)
                    n.attr['shape'] = 'square'
                    n.attr['fillcolor'] = color

        G.layout(prog='dot')
        # file_name = "dag_" + str(dag.dag_id) + "_contracted.png"
        G.draw(file_name)





class Ranker(object):
    """
    Class to compute different rank functions for a list of Simulation DAGs
    """

    def __init__(self, dags, ex_map):
        self.dags = dags
        self.ex_map = ex_map


    def oct(self, dag, nCPU=6, mGPU=6):
        rev_top_list = list(nx.topological_sort(dag.skeleton))
        rev_top_list.reverse()
        proc = []
        octable = {}
        for i in range(0, nCPU):
            p = "cpu"+str(i)
            proc.append(p)
            octable[p]={}
        for i in range(0, mGPU):
            p = "gpu"+str(i)
            proc.append(p)
            octable[p]={}
        for p in proc:
            for t in range(0, len(dag.tasks)):
                octable[p][t] = 0.0

        for p in proc:
            for t in rev_top_list:
                if dag.skeleton.out_degree(t) == 0:
                    octable[p][t] = 0.0
                else:
                    tmax = 0.0
                    for succ in dag.skeleton.successors(t):
                        tmin = float("inf")
                        for succ_proc in proc:
                            if proc == succ_proc:
                                if "cpu" in succ_proc:
                                    tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / CPU_FLOPS)
                                else:
                                    tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / GPU_FLOPS)
                            else:
                                if "cpu" in succ_proc:
                                    if "cpu" in proc:
                                        tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / CPU_FLOPS)
                                    else:
                                        tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / CPU_FLOPS + dag.tasks[t].DT / BW)
                                else:
                                    if "cpu" in proc:
                                        tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / GPU_FLOPS + dag.tasks[succ].DT /BW)
                                    else:
                                        tmin = min(tmin, octable[succ_proc][succ] + dag.tasks[succ].ECO / GPU_FLOPS + dag.tasks[t].DT/BW + dag.tasks[succ].DT/BW)

                        tmax = max(tmax, tmin)
                    octable[p][t] = tmax



            for task in dag.skeleton.nodes():
                trank = 0.0
                for p in proc:
                    trank += octable[p][t]

                dag.tasks[task].rank = trank/len(proc)










    def tlevel(self,dag):
        top_list = list(nx.topological_sort(dag.skeleton))
        for node in top_list:
            if dag.skeleton.in_degree(node) == 0:
                task = dag.tasks[node]
                if value_int(task.Class) < 5:
                    task.rank = task.ECO / CPU_FLOPS
                else:
                    task.rank = task.ECO / GPU_FLOPS + task.DT / BW

        for node in top_list:
            max = 0.0
            task = dag.tasks[node]
            if value_int(task.Class) < 5:
                task.rank = task.ECO / CPU_FLOPS
            else:
                task.rank = task.ECO / GPU_FLOPS + task.DT / BW
            max = task.rank
            for parent in dag.get_task_parents(task):
                if parent.rank + task.rank > max:
                    max = parent.rank + task.rank
            task.rank = max


    def tlevel_exec(self,dag):
        ex_cpu, ex_gpu = self.ex_map
        top_list = list(nx.topological_sort(dag.skeleton))
        for node in top_list:
            if dag.skeleton.in_degree(node) == 0:
                task = dag.tasks[node]
                if value_int(task.Class) < 5:
                    task.rank = ex_cpu[task.name]
                else:
                    task.rank = ex_gpu[task.name]

        for node in top_list:
            max = 0.0
            task = dag.tasks[node]
            if value_int(task.Class) < 5:
                task.rank = ex_cpu[task.name]
            else:
                task.rank = ex_gpu[task.name]
            max = task.rank
            for parent in dag.get_task_parents(task):
                if parent.rank + task.rank > max:
                    max = parent.rank + task.rank
            task.rank = max

    def blevel(self,dag):
        rev_top_list = list(nx.topological_sort(dag.skeleton))
        rev_top_list.reverse()
        for node in rev_top_list:
            if dag.skeleton.out_degree(node) == 0:
                task = dag.tasks[node]
                # print " ECO0 of ", node, task.ECO
                if value_int(task.Class) < 5:
                    task.rank = task.ECO / CPU_FLOPS
                else:
                    task.rank = task.ECO / GPU_FLOPS + task.DT / BW
                # print "RANKCompute", node, task.rank

        for node in rev_top_list:
            max = 0.0
            task = dag.tasks[node]
            # print "ECOn of ",node, task.ECO
            if value_int(task.Class) < 5:
                task.rank = task.ECO / CPU_FLOPS
            else:
                task.rank = task.ECO / GPU_FLOPS + task.DT / BW
            max = task.rank
            for child in dag.get_task_children(task):
                if child.rank + task.rank > max:
                    max = child.rank + task.rank
            task.rank = max
            # print "RANKCompute", node, task.rank
    
    def local_deadline(self,dag):
        
        def wcet(task,dag):
            exec_cpu,exec_gpu=dag.ex_map
            if exec_cpu[task.name]>ex_gpu[task.name]:
                return exec_cpu[task.name]
            else: 
                return exec_gpu[task.name]

        rev_top_list = list(nx.topological_sort(dag.skeleton))
        rev_top_list.reverse()
        ex_cpu,ex_gpu=dag.ex_map
        for node in rev_top_list:
            if dag.skeleton.out_degree(node) == 0:
                task = dag.tasks[node]
                task.rank=wcet(task,dag)
        
                
        for node in rev_top_list:
            
            max = 0.0
            if dag.skeleton.out_degree(node) != 0:
                task = dag.tasks[node]
                
                # print "ECOn of ",node, task.ECO
                task.rank=wcet(task,dag)
                max = task.rank
                for child in dag.get_task_children(task):
                    if child.rank + task.rank > max:
                        max = child.rank + task.rank
                task.rank = max
            # print "RANKCompute", node, task.rank

        # Calculate local deadlines

        for node in rev_top_list:
            task=dag.tasks[node]
            if dag.skeleton.out_degree(node) == 0:
                task.rank=dag.deadline
            else:
                task.rank=dag.deadline-task.rank+wcet(task,dag)
        

    
    def blevel_exec(self,dag):
        ex_cpu, ex_gpu = self.ex_map
        rev_top_list = list(nx.topological_sort(dag.skeleton))
        rev_top_list.reverse()
        for node in rev_top_list:
            if dag.skeleton.out_degree(node) == 0:
                task = dag.tasks[node]
                print " ECO0 of ", node, task.ECO
                if value_int(task.Class) < 5:
                    task.rank = ex_cpu[task.name]
                else:
                    task.rank = ex_gpu[task.name]
                print "RANKCompute", node, task.rank

        for node in rev_top_list:
            max = 0.0
            task = dag.tasks[node]
            print "ECOn of ",node, task.ECO
            if value_int(task.Class) < 5:
                task.rank = ex_cpu[task.name]
            else:
                task.rank = ex_gpu[task.name]
            max = task.rank
            for child in dag.get_task_children(task):
                if child.rank + task.rank > max:
                    max = child.rank + task.rank
            task.rank = max
            print "RANKCompute", node, task.rank

    def compute_upward_rank_ECO_DT(self, dag, node):
        max_node = 0
        temp = 0
        rank = 0
        task = dag.tasks[node]
        # If node is dummy node set rank to 0 else set rank to ECO_DT measure
        # print nx.number_of_nodes(dag.skeleton)
        if node is (nx.number_of_nodes(dag.skeleton) - 1):
            rank = 0
        else:
            if value_int(task.Class) < 5:
                rank = task.ECO / CPU_FLOPS
            else:
                rank = task.ECO / GPU_FLOPS + task.DT / BW

                # If node has no successors return rank value
        if (len(dag.skeleton.successors(node)) == 0):
            task.rank = rank
            dag.ranks[node] = rank
            return rank

            # Compute recursively maximum among successors
        else:
            for succ in dag.skeleton.successors(node):
                temp = self.compute_upward_rank_ECO_DT(dag, succ)
                if (max_node < rank + temp):
                    max_node = rank + temp
            rank = max_node
            task.rank = rank
            dag.ranks[node] = rank
            return rank

    def compute_downward_rank_ECO_DT(self, dag, node):
        max_node = 0
        temp = 0
        rank = 0
        task = dag.tasks[node]
        # If node is dummy node set rank to 0 else set rank to ECO_DT measure
        # print nx.number_of_nodes(dag.skeleton)
        if node is dag.num_nodes + 1:
            rank = 0
        else:
            if value_int(task.Class) < 5:
                rank = task.ECO / CPU_FLOPS
            else:
                rank = task.ECO / GPU_FLOPS + task.DT / BW

                # If node has no predecessors return rank value
        if (len(dag.skeleton.predecessors(node)) == 0):
            task.rank = rank
            dag.ranks[node] = rank
            return rank

            # Compute recursively maximum among predecessors
        else:
            for pred in dag.skeleton.predecessors(node):
                temp = self.compute_downward_rank_ECO_DT(dag, pred)
                if (max_node < rank + temp):
                    max_node = rank + temp
            rank = max_node
            task.rank = rank
            dag.ranks[node] = rank
            return rank

    def compute_upward_rank_EXTime(self, dag, node):
        ex_cpu, ex_gpu = self.ex_map
        max_node = 0
        temp = 0
        rank = 0
        task = dag.tasks[node]
        # If node is dummy node set rank to 0 else set rank to ExTime measure
        if node is dag.num_nodes:
            rank = 0
        else:
            if value_int(task.Class) < 5:
                rank = ex_cpu[task.name]
            else:
                rank = ex_gpu[task.name]

                # If node has no successors return rank value
        if (len(dag.skeleton.successors(node)) == 0):
            task.rank = rank
            dag.ranks[node] = rank
            return rank

            # Compute recursively maximum among successors
        else:
            for succ in dag.skeleton.successors(node):
                temp = self.compute_upward_rank_EXTime(dag, succ)
                if (max_node < rank + temp):
                    max_node = rank + temp
            rank = max_node
            task.rank = rank
            dag.ranks[node] = rank
            return rank

    def compute_downward_rank_EXTime(self, dag, node):
        ex_cpu, ex_gpu = self.ex_map
        max_node = 0
        temp = 0
        rank = 0
        task = dag.tasks[node]
        # If node is dummy node set rank to 0 else set rank to ExTime measure
        if node is dag.num_nodes + 1:
            rank = 0
        else:
            if value_int(task.Class) < 5:
                rank = ex_cpu[task.name]
            else:
                rank = ex_gpu[task.name]

                # If node has no predecessors return rank value
        if (len(dag.skeleton.predecessors(node)) == 0):
            task.rank = rank
            dag.ranks[node] = rank
            return rank

            # Compute recursively maximum among predecessors
        else:
            for pred in dag.skeleton.predecessors(node):
                temp = self.compute_downward_rank_EXTime(dag, pred)
                if (max_node < rank + temp):
                    max_node = rank + temp
            rank = max_node
            task.rank = rank
            dag.ranks[node] = rank
            return rank

    def compute_percentage_remaining_ECO(self, dag):
        for node in dag.skeleton.nodes():
            ECO_Sum = 0.0
            for succ in dag.skeleton.descendants(node):
                ECO_Sum = ECO_Sum + dag.tasks[succ].ECO
            dag.tasks[node].rank = ECO_Sum

    def compute_percentage_remaining_DT(self, dag):
        for node in dag.skeleton.nodes():
            DT_Sum = 0.0
            for succ in dag.skeleton.descendants(node):
                DT_Sum = DT_Sum + dag.tasks[succ].DT
            dag.tasks[node].rank = DT_Sum

    def compute_DT_next_level(self, dag):
        for node in dag.skeleton.nodes():
            DT_Sum = 0.0
            for succ in dag.skeleton.successors(node):
                DT_Sum += dag.skeleton[node][succ]['weight']
            dag.tasks[node].rank = DT_Sum

    def compute_rank_dags(self, compute_rank, nCPU, mGPU):
        # print "Computing ranks of dags"
        if compute_rank is "upward_rank_ECO_DT":
            for dag in self.dags:
                print dag.dag_id
                print nx.is_directed_acyclic_graph(dag.skeleton)
                dag.add_dummy_node_source()
                self.compute_upward_rank_ECO_DT(dag, dag.num_nodes)
                dag.remove_dummy_node_source()
                dag.update_rank_values(compute_rank)
        if compute_rank is "downward_rank_ECO_DT":
            for dag in self.dags:
                dag.add_dummy_node_exit()
                self.compute_downward_rank_ECO_DT(dag, dag.num_nodes + 1)
                dag.remove_dummy_node_exit()
                dag.update_rank_values(compute_rank)
        if compute_rank is "upward_rank_EXTime":
            for dag in self.dags:
                dag.add_dummy_node_source()
                self.compute_upward_rank_EXTime(dag, dag.num_nodes)
                dag.remove_dummy_node_source()
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "downward_rank_EXTime":
            for dag in self.dags:
                dag.add_dummy_node_exit()
                self.compute_downward_rank_ECO_DT(dag, dag.num_nodes + 1)
                dag.remove_dummy_node_exit()
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "percentage_ECO_remaining":
            for dag in self.dags:
                self.compute_percentage_remaining_ECO(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "percentage_DT_remaining":
            for dag in self.dags:
                self.compute_percentage_remaining_DT(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "DT_next_level":
            for dag in self.dags:
                self.compute_DT_next_level(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "blevel":
            for dag in self.dags:
                self.blevel(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "tlevel":
            for dag in self.dags:
                self.tlevel(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "blevel_exec":
            for dag in self.dags:
                self.blevel_exec(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "tlevel_exec":
            for dag in self.dags:
                self.tlevel_exec(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        if compute_rank is "oct":
            for dag in self.dags:
                self.oct(dag, nCPU, mGPU)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)
        
        if compute_rank is "local_deadline":
            for dag in self.dags:
                self.local_deadline(dag)
                dag.update_rank_values(compute_rank)
                dag.initialize_task_component_ranks(compute_rank)



class ScheduleEngine(object):
    """
    Class for backbone of scheduling algorithms
    """

    def __init__(self, nCPU, mGPU, rank_name, ex_map,online=False, global_map=None,adas=False):
        self.avg_makespan=0.0
        self.frontier = Q.PriorityQueue()
        self.nCPU = nCPU
        self.mGPU = mGPU
        self.num_CPU_devices = nCPU
        self.num_GPU_devices = mGPU
        self.dispatch_step = 0
        self.device_history = {"gpu": [], "cpu": []}
        self.ready_queue = {"gpu": collections.deque(), "cpu": collections.deque()}
        self.dispatch_queue = {"gpu":collections.deque(), "cpu": collections.deque()} 
        
        self.available_device_lookahead = {"gpu": [], "cpu": [], "nD": [nCPU, mGPU]}
        self.time_stamp = 0.0
        self.prev_time_stamp = 0.0
        self.estimate_time_stamp = 0.0
        self.estimate_prev_time_stamp = 0.0
        self.currently_executing = {"gpu": [], "cpu": []}
        self.waiting_tasks = {"gpu": {}, "cpu": {}}
        self.makespan = 0.0
        self.rank_name = rank_name
        self.ex_map = ex_map
        self.online=False
        self.adas=False
        self.arriving_dags = []
        self.dag_arrival_time_stamps = []
        self.dag_arrival_timestamp_map = {}
        if online:
            self.online=True
            # self.arrival_time_stamps = self.random_arrival_times()
            # self.create_random_dags_online()
        for i in range(0, nCPU):
            self.waiting_tasks['cpu'][i] = None
        for i in range(0, mGPU):
            self.waiting_tasks['gpu'][i] = None
        if adas:
            self.adas=True
        self.dags = []
        self.log = {}
        self.global_map = None
        if global_map:
            self.global_map = global_map
    



    # Operations for online scheduling

    def random_arrival_times(self,lambda_rate=1.5, time_horizon=10):
        intervals = [random.expovariate(lambda_rate) for i in range(1000)]
        timestamps = [0.0]
        timestamp = 0.0
        for t in intervals:
            timestamp += t*1000000
            timestamps.append(timestamp)
        return timestamps[:time_horizon]
    
    def random_arrival_times_limit(self,lambda_rate=1.5, time_limit=10):
        intervals = [random.expovariate(lambda_rate) for i in range(1000)]
        timestamps = [0.0]
        timestamp = 0.0
        time_limit *=1000000
        for t in intervals:
            timestamp += t*1000000
            if timestamp>time_limit:
                break
            timestamps.append(timestamp)
        self.arrival_time_stamps = timestamps

    def create_mm_dags_online(self,filename,weights):
        n=100
        work_item_array = []
        work_items=[1024,2048,4096,8192]
        index_elements = []
       
        for i in range(n):
            index_elements.append(i)
            work_item_array.append(0)
       
        for w,W in zip(weights,work_items):
            num_samples = int(w*100)
            current_list = random.sample(index_elements,num_samples)
            for i in current_list:
                work_item_array[i]=W
            index_elements = list(set(index_elements)-set(current_list))
       
        # print work_item_array
       
        file_name_1024 = "graphs_1024.txt"
        file_name_2048 = "graphs_2048.txt"
        file_name_4096 = "graphs_4096.txt"
        file_name_8192 = "graphs_8192.txt"
       
        dagpool_1024=[]
        dagpool_2048=[]
        dagpool_4096=[]
        dagpool_8192=[]
        
        for graph in open(file_name_1024).readlines():
            dagpool_1024.append(graph.split(":")[0])
        for graph in open(file_name_2048).readlines():
            dagpool_2048.append(graph.split(":")[0])
        for graph in open(file_name_4096).readlines():
            dagpool_4096.append(graph.split(":")[0])
        for graph in open(file_name_8192).readlines():
            dagpool_8192.append(graph.split(":")[0])
        
        dag_names = []
        
        for W in work_item_array:
            if W ==1024:
                dag_name = random.choice(dagpool_1024)
            elif W == 2048:
                dag_name = random.choice(dagpool_2048)
            elif W == 4096:
                dag_name = random.choice(dagpool_4096)
            elif W == 8192:
                dag_name = random.choice(dagpool_8192)

            dag_names.append(dag_name)
        
        # print len(dag_names)
        # print len(self.arrival_time_stamps)
        d = ','.join(dag_names) + "\n"
        ts =','.join(map(str,self.arrival_time_stamps)) + "\n"
        
        configuration_file = open(filename,'w')
        configuration_file.write(d)
        configuration_file.write(ts)
        configuration_file.close()



    def select_rates(self, dags):
        for dag in dags:
            dag.rate=random.choice(dag.rates)
            dag.period=1/dag.rate
            dag.period=int(dag.period)-int(dag.period)%5 

    def generate_periodic_schedule(self, dags, periods=None):
        import fractions
        from math import ceil
        def _lcm(a,b): return abs(a * b) / fractions.gcd(a,b) if a and b else 0
        def lcm(a):
            return reduce(_lcm, a)
        rates = []
        if periods is None:
            self.select_rates(dags)
            rates = [d.rate for d in dags]
            periods = [d.period for d in dags]
        else:
            for p,d in zip(periods,dags):
                d.period=p
                d.rate=1/float(d.period)
                rates.append(d.rate)
        # print rates
        wcets=[d.wcet for d in dags]
        print wcets
        print periods
        hyperperiod=lcm(periods)
        print hyperperiod
        DC=DAGCreator()
        arrival_instances = []
        dag_arrivals = []
        
        for d in dags:
            i=0.0
            s=0.0
            # print int(d.period/hyperperiod)
            for i in xrange(0,int(ceil(hyperperiod/d.period))):
                release=d.phase+d.period*i
                task_arrival_pair = (d,release)
                arrival_instances.append(task_arrival_pair)
                # dag_arrivals.append((dag,release))
 

        arrival_instances = sorted(arrival_instances,key=lambda x: x[1])
        dag_arrivals = sorted(dag_arrivals,key=lambda x: x[1])
        
        # print arrival_instances
        dag_id = 0
        for d, release in arrival_instances:
            
            job_id = d.dag_id
            input_file = "ADAS_Graphs/"+d.name
            dag=DC.create_adas_job_from_file(input_file,dag_id,self.global_map,self.ex_map,job_id,release,d.period)
            # print dag.job_id, dag.release,dag.deadline,dag.wcet,dag.period
            self.dag_arrival_time_stamps.append(release)
            self.dag_arrival_timestamp_map[dag]=release
            self.arriving_dags.append(dag)
            dag_id +=1
        r = Ranker(self.arriving_dags, self.ex_map)
        r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)
        # for d in self.arriving_dags[0:10]:
        #     d.print_rank_info()

        
                            
        dag = self.arriving_dags.pop(0)
        self.dags.append(dag)
        self.dag_arrival_time_stamps.pop(0)
        
        return rates, periods



    
    def create_random_dags_online(self, rank, filename=None,work_item=None):
        # file_name = "NewStats/dump_graphs_new_"+str(rank)+".stats"
        file_name = ""
        if work_item:
            file_name = "graphs_"+str(work_item)+".txt"
        else:
            file_name = "graphs_8192.txt"
        graphs = open(file_name, 'r').readlines()
        dagpool = []
        for graph in graphs:
            dagpool.append(graph.split(":")[0])
                
        DC = DAGCreator()
        starting_dag_id = len(self.dags)
        dag_names = []
        for i in range(len(self.arrival_time_stamps)):
            # print "Creating dag ", starting_dag_id + i
            dag_name = random.choice(dagpool)
            dag_names.append(dag_name)
            dag = DC.create_dag_from_file(dag_name, starting_dag_id + i, self.global_map, self.ex_map)
            # DC.dump_graph_ids(dag, "list_dag1_ids.png")
            self.dag_arrival_timestamp_map[dag]=self.arrival_time_stamps[i]
            # dagname_ts_map[dag_name]=self.arrival_time_stamps[i]
            self.arriving_dags.append(dag)
            r = Ranker(self.arriving_dags, self.ex_map)
            r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)

        if filename:
            d = ','.join(dag_names) + "\n"
            ts =','.join(map(str,self.arrival_time_stamps)) + "\n"
            configuration_file = open(filename,'w')
            configuration_file.write(d)
            configuration_file.write(ts)
            configuration_file.close()

        dag = self.arriving_dags.pop(0)
        self.dags.append(dag)
        self.arrival_time_stamps.pop(0)
        '''
        for task_component in dag.free_task_components:
            task_component.get_first_kernel().in_frontier = True
            print "OnlineAddFrontier ",task_component.get_kernel_ids()
            self.frontier.put(task_component)
        '''

    def release_dags(self, timestamp):
        dags = []
        
        if len(self.dag_arrival_time_stamps) <=0:
            return
        t = self.dag_arrival_time_stamps[0] # WAS THERE IN ORIGINAL FRAMEWORK
        # t = self.dag_arrival_time_stamps.pop(0) # WAS NOT THERE IN ORIGINAL FRAMEWORK
        
        #if t > timestamp:  WAS THERE IN ORIGINAL FRAMEWORK
        if t < timestamp:
            dag = self.arriving_dags.pop(0)
            self.dags.append(dag)
            dag.starting_time = self.dag_arrival_time_stamps.pop(0)
            for task_component in dag.free_task_components:
                task_component.get_first_kernel().in_frontier = True
                self.frontier.put(task_component)
            self.time_stamp = t
        # else:               # WAS NOT THERE IN ORIGINAL FRAMEWORK
            # self.dag_arrival_time_stamps.insert(0,t) # WAS NOT THERE IN ORIGINAL FRAMEWORK
        
        while t <= timestamp and len(self.dag_arrival_time_stamps) > 0:
            # print t,timestamp
            dag = self.arriving_dags.pop(0)
            self.dags.append(dag)
            dag.starting_time = self.dag_arrival_time_stamps.pop(0)
            if len(self.dag_arrival_time_stamps) > 0:
                t = self.dag_arrival_time_stamps[0]
            for task_component in dag.free_task_components:
                task_component.get_first_kernel().in_frontier = True
                self.frontier.put(task_component)
        # print "OnlineRelease"    

    
    
    
    def online_setup(self, rate, horizon, cluster_rank, filename=None, work_item=None):
        self.random_arrival_times_limit(rate,horizon)
        self.create_random_dags_online(cluster_rank,filename,work_item=None)
    
    def generate_mm_configurations(self,rate,limit,weights,filename):
        self.arrival_time_stamps = self.random_arrival_times(rate,limit)
        self.create_mm_dags_online(filename,weights)
        

        


    def calculate_avg_makespan(self):
        for dag in self.dags:
            # print dag.job_id,dag.starting_time,dag.finishing_time,dag.deadline
            self.avg_makespan += dag.finishing_time-dag.starting_time
        self.avg_makespan = self.avg_makespan/len(self.dags)
    
    def calculate_avg_lateness(self):
        lateness = 0.0
        for dag in self.dags:
            # print "\n", dag.job_id,"RELEASE", dag.release, "START TIME", dag.starting_time, "FINISHING TIME ", dag.finishing_time,"DEADLINE", dag.deadline, "WCET", dag.wcet
            # dag.print_task_info()
            lateness += dag.finishing_time-dag.deadline
        lateness = lateness/len(self.dags)
        print "LATENESS", lateness
        return lateness
    
    def calculate_percent_lateness(self):
        lateness = 0.0
        makespan = 0.0
        percent_lateness = 0.0
        
        for dag in self.dags:
            # print "\n", dag.job_id,"RELEASE", dag.release, "START TIME", dag.starting_time, "FINISHING TIME ", dag.finishing_time,"DEADLINE", dag.deadline, "WCET", dag.wcet
            # dag.print_task_info()
            lateness = dag.finishing_time-dag.deadline
            makespan =dag.finishing_time-dag.starting_time
            percent_lateness += lateness/makespan
        return percent_lateness/len(self.dags)

    

    def calculate_avg_tardiness(self):
        tardiness = 0.0
        counter = 0
        for dag in self.dags:
            # print "\n", dag.job_id,"RELEASE", dag.release, "START TIME", dag.starting_time, "FINISHING TIME ", dag.finishing_time,"DEADLINE", dag.deadline, "WCET", dag.wcet
            # dag.print_task_info()
            t = dag.finishing_time-dag.deadline
            if t > 0:
                tardiness += dag.finishing_time-dag.deadline
                counter +=1
        if counter > 0:
            tardiness = tardiness/counter
        print "TARDINESS", tardiness
        return tardiness,counter
        
    def dump_online_makespans(self,name):
        filename = open(name,'w')
        for dag in self.dags:
            makespan = dag.finishing_time - dag.starting_time
            line = dag.name+ ":" + str(makespan)+"\n"
            # print line
            filename.write(line)
        filename.close()

    
    def online_setup_configuration(self, online_configuration_file):
        online_configuration = open(online_configuration_file,'r').readlines()
        arriving_dags = online_configuration[0].strip("\n").split(",")
        arriving_timestamps = map(float, online_configuration[1].strip("\n").split(","))
        self.arrival_time_stamps = arriving_timestamps
        DC = DAGCreator()
        starting_dag_id = len(self.dags)
        for i in range(len(self.arrival_time_stamps)):
            dag_name = arriving_dags[i]
            dag = DC.create_dag_from_file(dag_name, starting_dag_id + i, self.global_map, self.ex_map)
            dag.modify_partition_classes("GraphsNewAccuracy/accuracy_"+dag_name[10:][:-6]+".stats")
            # print "modified"
            self.dag_arrival_timestamp_map[dag]=self.arrival_time_stamps[i]
            self.arriving_dags.append(dag)
            r = Ranker(self.arriving_dags, self.ex_map)
            r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)
        dag = self.arriving_dags.pop(0)
        self.dags.append(dag)
        dag.starting_time = self.arrival_time_stamps.pop(0)

        

   
   

        



        
        

    # Operations for Task Components

    def dispatch_task_component(self, cpu, gpu, task_component, dag):
        # Update start time of task with current time stamp of system
        print "time Stamp: " + str(self.time_stamp)
        task_component.start_time = self.time_stamp
        # Update currently executing list with task, dag and device id dispatched
        if (cpu == -1 and gpu != -1):
            self.currently_executing['gpu'].append((task_component, dag, gpu))
            self.mGPU -= 1
        if (cpu != -1 and gpu == -1):
            self.currently_executing['cpu'].append((task_component, dag, cpu))
            self.nCPU -= 1

    def update_finished_task_component(self, executing_task_component, dev_type):
        task_component, dag, device_id = executing_task_component
        task_component.finish_time = self.time_stamp
        task_component.is_finished = True
        print "for " + str(list(task_component.get_kernel_names())) + " of DAG " + str(task_component.dag_id)

        dag.finished_tasks += list(task_component.get_kernels())
        # dev_history = (device_id, task.name, task.dag_id, task.start_time, task.finish_time, task.execution_time)
        dev_history = (device_id, task_component.get_kernel_names(), task_component.dag_id, task_component.start_time,
                       task_component.finish_time, task_component.finish_time - task_component.start_time)
        # Add back device id to ready queue and update device counters

        if dev_type == "gpu":
            self.ready_queue['gpu'].append(device_id)
            self.mGPU += 1
            self.device_history['gpu'].append(dev_history)
            print "Updating GPU Device History " + str(device_id)
        else:
            self.ready_queue['cpu'].append(device_id)
            self.nCPU += 1
            self.device_history['cpu'].append(dev_history)
            print "Updating CPU Device History" + str(device_id)
        # Add successors of finished task to frontier
        print "Adding successors of task component " + str(list(task_component.get_kernel_names())) + " of DAG " + str(
            task_component.dag_id)

        gid = nx.relabel_nodes(dag.G, lambda s: s.id, copy=True)
        for i in gid.successors(task_component.id):
            succ_task = dag.task_component_id_map[i]
            # print succ_task.name
            if dag.are_task_component_parents_finished(succ_task):
                self.frontier.put(succ_task)

    def update_execution_pool_of_task_components(self):
        min_execution_time_CPU = 1e10
        min_execution_time_GPU = 1e10
        cpu_index = 0
        gpu_index = 0
        self.prev_time_stamp = self.time_stamp
        # Obtain minimum execution time for cpu devices
        for i in range(0, len(self.currently_executing['cpu'])):
            task_component, dag, device_id = self.currently_executing['cpu'][i]
            print "PROJECTED CPU ",
            task_component.projected_ex_time
            if (min_execution_time_CPU > task_component.start_time + task_component.projected_ex_time):
                min_execution_time_CPU = task_component.start_time + task_component.projected_ex_time
                cpu_index = i
        # Obtain minimum execution time for gpu devices
        for i in range(0, len(self.currently_executing['gpu'])):
            task_component, dag, device_id = self.currently_executing['gpu'][i]
            print "PROJECTED GPU ",
            task_component.projected_ex_time
            if (min_execution_time_GPU > task_component.start_time + task_component.projected_ex_time):
                min_execution_time_GPU = task_component.start_time + task_component.projected_ex_time
                gpu_index = i

        print "CPU min time: " + str(min_execution_time_CPU)
        print "GPU min time " + str(min_execution_time_GPU)

        if (min_execution_time_GPU < min_execution_time_CPU):
            executing_task_component = self.currently_executing['gpu'][gpu_index]
            task_component, dag, device_id = executing_task_component
            self.currently_executing['gpu'].pop(gpu_index)
            # Update system time stamp and finishing time of task
            execution_time = task_component.projected_ex_time
            print "Updating time stamp " + str(self.time_stamp) + " to ",
            self.time_stamp = self.time_stamp + execution_time
            print(self.time_stamp),
            self.update_finished_task_component(executing_task_component, "gpu")
        else:
            executing_task_component = self.currently_executing['cpu'][cpu_index]
            task_component, dag, device_id = executing_task_component
            self.currently_executing['cpu'].pop(cpu_index)
            # Update system time stamp and finishing time of task
            execution_time = task_component.projected_ex_time
            print "Updating time stamp " + str(self.time_stamp) + " to ",
            self.time_stamp = self.time_stamp + execution_time
            print(self.time_stamp),
            self.update_finished_task_component(executing_task_component, "cpu")

        # Diminish projected execution time as per elapsed time of scheduling engine

        print "Time stamp: " + str(self.time_stamp)
        print "Previous Time Stamp: " + str(self.prev_time_stamp)
        elapsed = self.time_stamp - self.prev_time_stamp
        print "Elapsed: " + str(elapsed)
        for i in range(0, len(self.currently_executing['cpu'])):
            task_component, dag, device_id = self.currently_executing['cpu'][i]
            task_component.projected_ex_time = task_component.projected_ex_time - elapsed

        # Diminish projected execution time as per elapsed time of scheduling engine
        for i in range(0, len(self.currently_executing['gpu'])):
            task_component, dag, device_id = self.currently_executing['gpu'][i]
            task_component.projected_ex_time = task_component.projected_ex_time - elapsed

    # Operations for Task Sets

    #
    # def dispatch_task_set(self, cpu, gpu, task_component, dag):    
    #     # Update start time of task with current time stamp of system
    #     print "time Stamp: " + str(self.time_stamp)
    #     task_component.start_time = self.time_stamp
    #     # Update currently executing list with task, dag and device id dispatched
    #
    #     task_component.sorted_list = list(task_component.get_kernels_sorted(dag))
    #     task = task_component.sorted_list[0]
    #     print "Dispatching " + str(task.id) + " of " + str(dag.dag_id)
    #     task.start_time = self.time_stamp
    #     del task_component.sorted_list[0]
    #     dag.reduce_execution_time(task, task_component)
    #     if (cpu == -1 and gpu != -1):
    #         self.currently_executing['gpu'].append((task, dag, gpu))
    #         self.mGPU -= 1
    #         heapq.heappush(self.available_device_lookahead['gpu'], task.projected_ex_time)
    #     if (cpu != -1 and gpu == -1):
    #         self.currently_executing['cpu'].append((task, dag, cpu))
    #         self.nCPU -= 1
    #         heapq.heappush(self.available_device_lookahead['cpu'], task.projected_ex_time)

    def dispatch_task_set(self, cpu, gpu, task_component, dag):
        # print "time Stamp: " + str(self.time_stamp)
        task_component.is_dispatched = True
        task_component.start_time = self.time_stamp
        task_list = task_component.get_free_kernels(dag)
        total_task_list_ids = task_component.get_kernel_ids()
        task_component.number_of_tasks = len(total_task_list_ids)
        # print "TASK_COMPONENT size",
        # print task_component.number_of_tasks,
        # print total_task_list_ids
        for t in task_list:
            task_component.local_frontier.append(t)

        task = task_component.local_frontier.popleft()
        if self.adas:
            # print "REDUCING TIME"
            dag.reduce_adas_execution_time(task, task_component)
        else:
            dag.reduce_execution_time(task, task_component)
        # print "\n DISPATCHING " , total_task_list_ids , "of DAG " + str(dag.dag_id) , "to CPU", cpu, "GPU", gpu
        task.start_time = self.time_stamp

        task.in_frontier = False
        task.dispatch_step = self.dispatch_step
        self.dispatch_step += 1
        # print "TASK_DISPATCHED_GLOBAL ", task.id, task_component.number_of_tasks
        if (cpu == -1 and gpu != -1):
            self.currently_executing['gpu'].append((task, dag, gpu))
            if not task_component.to_be_scheduled:
                self.mGPU -= 1
            heapq.heappush(self.available_device_lookahead['gpu'], task.projected_ex_time)
            # heapq.heappush(self.available_device_lookahead['gpu'], task.estimated_ex_time)
        if (cpu != -1 and gpu == -1):
            self.currently_executing['cpu'].append((task, dag, cpu))
            if not task_component.to_be_scheduled:
                self.nCPU -= 1
            heapq.heappush(self.available_device_lookahead['cpu'], task.projected_ex_time)
            # heapq.heappush(self.available_device_lookahead['cpu'], task.estimated_ex_time)
        if task_component.to_be_scheduled == True:
            task_component.to_be_scheduled = False
        return self.time_stamp


    def suspend(self, dev_type, device_id, task_component):
        # print "SUSPEND"
        self.waiting_tasks[dev_type][device_id] = task_component


    def resume(self, task_component, task):
        # print "RESUME"
        dev_type = ""
        device_id = ""
        for device in ['cpu', 'gpu']:
            for dev_id in range(0, len(self.waiting_tasks[device])):
                if self.waiting_tasks[device][dev_id] == task_component:
                    dev_type = device
                    device_id = dev_id
                    self.waiting_tasks[device][dev_id] = None
        task_component.local_frontier.append(task)
        next_task = task_component.local_frontier.popleft()
        dag = self.dags[task_component.dag_id]
        if self.adas:
            dag.reduce_adas_execution_time(next_task, task_component)
        else:
            dag.reduce_execution_time(next_task, task_component)
        next_task.start_time = self.time_stamp
        # print "SCHEDULE: next task ",
        # print str(next_task.id) + " of " + str(dag.dag_id),
        # print " of task component id",
        # print task_component.id,
        # print " ",
        # print next_task.start_time,
        # print " ",
        # print next_task.projected_ex_time
        if dev_type == "gpu":
            self.currently_executing['gpu'].append((next_task, dag, device_id))
            # heapq.heappush(self.available_device_lookahead['gpu'], next_task.projected_ex_time)
            heapq.heappush(self.available_device_lookahead['gpu'], next_task.estimated_ex_time)
        else:
            self.currently_executing['cpu'].append((next_task, dag, device_id))
            heapq.heappush(self.available_device_lookahead['cpu'], next_task.projected_ex_time)
            heapq.heappush(self.available_device_lookahead['cpu'], next_task.estimated_ex_time)



        # dev_type = ""
        # device_id = ""
        # for device in ['cpu', 'gpu']:
        #     for dev_id in range(0, len(self.waiting_tasks[device])):
        #         if self.waiting_tasks[device][dev_id] == task_component:
        #             dev_type = device
        #             device_id = dev_id
        #             self.waiting_tasks[device][dev_id] = None
        #
        # dag = self.dags[task_component.dag_id]
        #
        # print "RESUME " + str(dev_type) + str(device_id)
        # if dev_type == "gpu":
        #     if len(task_component.sorted_list) == 0:
        #         print "Task Component Finished"
        #         self.ready_queue['gpu'].append(device_id)
        #         self.mGPU += 1
        #     else:
        #         next_task = task_component.sorted_list[0]
        #         if not dag.are_parents_finished(next_task):
        #             print "SUSPENDING GPU " + str(device_id)
        #             self.suspend(dev_type, device_id, task_component)
        #             return
        #         del task_component.sorted_list[0]
        #         dag.reduce_execution_time(next_task, task_component)
        #         next_task.start_time = self.time_stamp
        #         print "next task ",
        #         print str(next_task.id) + " of " + str(dag.dag_id),
        #         print " of task component id",
        #         print task_component.id,
        #         print " ",
        #         print next_task.start_time,
        #         print " ",
        #         print next_task.projected_ex_time
        #         self.currently_executing['gpu'].append((next_task, dag, device_id))
        #         heapq.heappush(self.available_device_lookahead['gpu'], next_task.projected_ex_time)
        #
        # else:
        #     if len(task_component.sorted_list) == 0:
        #         print "Task Component Finished"
        #         self.ready_queue['cpu'].append(device_id)
        #         self.nCPU += 1
        #     else:
        #         next_task = task_component.sorted_list[0]
        #         if not dag.are_parents_finished(next_task):
        #             print "SUSPENDING CPU " + str(device_id)
        #             self.suspend(dev_type, device_id, task_component)
        #             return
        #         del task_component.sorted_list[0]
        #         dag.reduce_execution_time(next_task, task_component)
        #         next_task.start_time = self.time_stamp
        #         print "next task ",
        #         print str(next_task.id) + " of " + str(dag.dag_id),
        #         print " of task component id",
        #         print task_component.id,
        #         print " ",
        #         print next_task.start_time,
        #         print " ",
        #         print next_task.projected_ex_time
        #         self.currently_executing['cpu'].append((next_task, dag, device_id))
        #         heapq.heappush(self.available_device_lookahead['cpu'], next_task.projected_ex_time)

    def get_waiting_tasks(self):
        gpu_tasks = self.waiting_tasks['gpu']
        cpu_tasks = self.waiting_tasks['cpu']

        for device in gpu_tasks:
            if gpu_tasks[device] is not None:
                pass
                # print "SCHEDULE: Suspended Status GPU",device, gpu_tasks[device].get_kernel_ids(),gpu_tasks[device].dag_id

        for device in cpu_tasks:
            if cpu_tasks[device] is not None:
                pass
                # print "SCHEDULE: Suspended Status CPU",device, cpu_tasks[device].get_kernel_ids(),cpu_tasks[device].dag_id

    def update_finished_task_set(self, executing_task, dev_type):

        # Getting task, task component, dag and device ids

        task, dag, device_id = executing_task
        # print "Current Task Component ID ",

        task_component = dag.task_components[task.id]
        # print task_component.id
        
        # Bookkeeping status of task parameters and frontier and finished status
        if not self.adas:
            task.finish_time = self.time_stamp
        else:
            task.finish_time = task.start_time + task.execution_time
        task.is_finished = True
        # print "TASK_FINISHED ", task.id
        # print "SCHEDULE_Updating finished tasks for task", task.id,"with local deadline", task.rank, "of", task_component.get_kernel_ids(), "of DAG", dag.dag_id, "with deadline", dag.deadline, "START AND FINISH", task.start_time, task.finish_time
        heapq.heappop(self.available_device_lookahead[dev_type])
        dag.finished_tasks.append(task)

        # Checking whether task component has finished

        task_component.number_of_tasks -= 1
        if task_component.number_of_tasks == 0:
            # print "SCHEDULE: TASK COMPONENT finished",
            # print task_component
            if dev_type == "gpu":
                # print "FINISH GPU: Task Component Finished"
                self.ready_queue['gpu'].append(device_id)
                self.mGPU += 1
            else:
                # print "FINISH CPU: Task Component Finished"
                self.ready_queue['cpu'].append(device_id)
                self.nCPU += 1

            task_component.is_finished=True

        # Updating device history

        dev_history = (device_id, task.name, task.dag_id, task.start_time, task.finish_time, task.finish_time - task.start_time)

        if dev_type == "gpu":
            self.device_history['gpu'].append(dev_history)
            # print "Updating GPU Device History " + str(device_id)
        else:
            self.device_history['cpu'].append(dev_history)
            # print "Updating CPU Device History " + str(device_id)

        # Successor augmentation if all of its parents are finished
        # 1) Successor is not a supertask and does not belong to current component ---> Add to frontier
        # 2) Successor belongs to current compoent --> Add to local frontier
        # 3) Successor belongs to a different task component ---> If component is suspended, resume
        # 4) Successor beleongs to a different task component and is supposed to be scheduled ---> dispatch directly to specified device

        # print "SCHEDULE: Investigating successors of task" + str(task.id) + " of dag" + str(dag.dag_id)

        for i in dag.skeleton.successors(task.id):
            succ_task = dag.tasks[i]

            # All parents of successor task has finished
            # print "TASK_DISPATCHED_INVESTIGATE ", succ_task.id, task.id
            if dag.are_parents_finished(succ_task):
                # print "."
                # print "SCHEDULE_TASK_DISPATCHED_PARENTS_FINISHED (Successor, Current Task)", succ_task.id, task.id
                succ_task_component = dag.task_components[succ_task.id]
                # print "SUSPENDED_TASKS", self.waiting_tasks
                self.get_waiting_tasks()
                # Case 2

                if succ_task_component == task_component:
                    # print "."
                    # print "SCHEDULE: Update local frontier TASK_DISPATCHED_LOCAL_FRONTIER (Successor, Current Task)", succ_task.id, task.id,
                    task_component.local_frontier.append(succ_task)

                # Case 4
                elif succ_task_component.to_be_scheduled:
                    cpu = -1
                    gpu = -1
                    if succ_task_component.future_device_type == "cpu":
                        cpu = succ_task_component.future_device_id
                    else:
                        gpu = succ_task_component.future_device_id
                    self.dispatch_task_set(cpu, gpu, succ_task_component, self.dags[succ_task_component.dag_id])


                # Case 3

                elif succ_task_component in self.waiting_tasks['gpu'].values() or succ_task_component in self.waiting_tasks['cpu'].values():
                    # print "RESUME SUSPENDED TASK"
                    self.resume(succ_task_component, succ_task)

                # Case 1

                elif not succ_task_component.is_supertask():
                    # print "."
                    # print "TASK_DISPATCHED_GLOBAL_FRONTIER ", succ_task.id
                    # print succ_task.name
                    dag.task_components[succ_task.id].get_first_kernel().in_frontier = True
                    # print "UPDATE-FRONTIER", dag.task_components[succ_task.id].get_kernel_names()
                    self.frontier.put(dag.task_components[succ_task.id])
        # print "."
        # print "REMAINING ",
        # dag.get_remaining_tasks()
        # Remove task from local frontier and add to currently executing

        if task_component.is_supertask():
            # print "SCHEDULE_SUSPEND_LOCAL_FRONTIER_SIZE of ", task_component.get_kernel_ids(), [task.id for task in task_component.local_frontier]
            if len(task_component.local_frontier) > 0:
                next_task = task_component.local_frontier.popleft()

                next_task.dispatch_step = self.dispatch_step
                self.dispatch_step += 1
                if self.adas:
                    dag.reduce_adas_execution_time(next_task, task_component)
                else:
                    dag.reduce_execution_time(next_task, task_component)
                
                next_task.start_time = self.time_stamp
                # task_component.number_of_tasks -= 1
                # print "SCHEDULE: next task ",
                # print str(next_task.id) + " of " + str(dag.dag_id),
                # print " of task component id",
                # print task_component.id,
                # print " ",
                # print next_task.start_time,
                # print " ",
                # print next_task.projected_ex_time
                # print "TASK_DISPATCHED_LOCAL ", next_task.id
                if dev_type == "gpu":
                    self.currently_executing['gpu'].append((next_task, dag, device_id))
                    # heapq.heappush(self.available_device_lookahead['gpu'], next_task.projected_ex_time)
                    heapq.heappush(self.available_device_lookahead['gpu'], next_task.estimated_ex_time)
                else:
                    self.currently_executing['cpu'].append((next_task, dag, device_id))
                    # heapq.heappush(self.available_device_lookahead['cpu'], next_task.projected_ex_time)
                    heapq.heappush(self.available_device_lookahead['cpu'], next_task.estimated_ex_time)
            elif task_component.number_of_tasks > 0:
                # print "SCHEDULE_SUSPEND_TOTAL_SIZE", task_component.number_of_tasks
                # print "SCHEDULE_SUSPEND_TASK", task_component.get_kernel_ids(), task_component, task_component.dag_id
                self.suspend(dev_type, device_id, task_component)









    # def update_finished_task_set(self, executing_task, dev_type):
    #     task, dag, device_id = executing_task
    #     task.finish_time = self.time_stamp
    #     task.is_finished = True
    #     task.in_frontier = False
    #     heapq.heappop(self.available_device_lookahead[dev_type])
    #     dag.finished_tasks.append(task)
    #     # print "for " + str(list(task_component.get_kernel_names())) + " of DAG " + str(task_component.dag_id)
    #     dev_history = (
    #     device_id, task.name, task.dag_id, task.start_time, task.finish_time, task.finish_time - task.start_time)
    #     # dev_history = (device_id, task_component.get_kernel_names(), task_component.dag_id, task_component.start_time, task_component.finish_time, task_component.finish_time - task_component.start_time)
    #     # Add back device id to ready queue and update device counters
    #     # If task is in some super task add next task in sorted list to execution pool
    #
    #     print "Possible Execution Pool Addition for " + str(task.id) + " of " + str(dag.dag_id)
    #     task_component = dag.task_components[task.id]
    #     if task_component.is_supertask():
    #         print "Component Size : ",
    #         print len(list(task_component.get_kernel_names())),
    #         print "Number of tasks left ",
    #         print len(task_component.sorted_list)
    #         if dev_type == "gpu":
    #             if len(task_component.sorted_list) == 0:
    #                 print "Task Component Finished"
    #                 self.ready_queue['gpu'].append(device_id)
    #                 self.mGPU += 1
    #             else:
    #                 next_task = task_component.sorted_list[0]
    #                 if not dag.are_parents_finished(next_task):
    #                     self.suspend(dev_type, device_id, task_component)
    #                     return
    #                 del task_component.sorted_list[0]
    #                 dag.reduce_execution_time(next_task, task_component)
    #                 next_task.start_time = self.time_stamp
    #                 print "next task ",
    #                 print str(next_task.id) + " of " + str(dag.dag_id),
    #                 print " of task component id",
    #                 print task_component.id,
    #                 print " ",
    #                 print next_task.start_time,
    #                 print " ",
    #                 print next_task.projected_ex_time
    #                 self.currently_executing['gpu'].append((next_task, dag, device_id))
    #                 heapq.heappush(self.available_device_lookahead['gpu'], next_task.projected_ex_time)
    #             self.device_history['gpu'].append(dev_history)
    #             print "Updating GPU Device History " + str(device_id)
    #         else:
    #             if len(task_component.sorted_list) == 0:
    #                 print "Task Component Finished"
    #                 self.ready_queue['cpu'].append(device_id)
    #                 self.nCPU += 1
    #             else:
    #                 next_task = task_component.sorted_list[0]
    #                 if not dag.are_parents_finished(next_task):
    #                     self.suspend(dev_type, device_id, task_component)
    #                     return
    #                 del task_component.sorted_list[0]
    #                 dag.reduce_execution_time(next_task, task_component)
    #                 next_task.start_time = self.time_stamp
    #                 print "next task ",
    #                 print str(next_task.id) + " of " + str(dag.dag_id),
    #                 print " of task component id",
    #                 print task_component.id,
    #                 print " ",
    #                 print next_task.start_time,
    #                 print " ",
    #                 print next_task.projected_ex_time
    #                 self.currently_executing['cpu'].append((next_task, dag, device_id))
    #                 heapq.heappush(self.available_device_lookahead['cpu'], next_task.projected_ex_time)
    #             self.device_history['cpu'].append(dev_history)
    #             print "Updating CPU Device History" + str(device_id)
    #     else:
    #         if dev_type == "gpu":
    #             self.ready_queue['gpu'].append(device_id)
    #             self.mGPU += 1
    #             self.device_history['gpu'].append(dev_history)
    #             print "Updating GPU Device History " + str(device_id)
    #         else:
    #             self.ready_queue['cpu'].append(device_id)
    #             self.nCPU += 1
    #             self.device_history['cpu'].append(dev_history)
    #             print "Updating CPU Device History" + str(device_id)
    #             # Add successors of task to frontier if that task is not in the super task
    #
    #     print "Investigating successors of task not in the task component to frontier " + str(
    #         task.id) + " of dag" + str(dag.dag_id)
    #     print "Current Task Component ID ",
    #     print task_component.id
    #     for i in dag.skeleton.successors(task.id):
    #         succ_task = dag.tasks[i]
    #         if dag.are_parents_finished(succ_task):
    #             succ_task_component = dag.task_components[succ_task.id]
    #             if not succ_task_component.is_supertask():
    #                 print succ_task.name
    #                 dag.task_components[succ_task.id].get_first_kernel().in_frontier = True
    #                 print "UPDATE-FRONTIER", dag.task_components[succ_task.id].get_kernel_names()
    #                 self.frontier.put(dag.task_components[succ_task.id])
    #             elif succ_task_component in self.waiting_tasks.values():
    #                 self.resume(succ_task_component)
    #


        # for i in dag.skeleton.successors(task.id):
        #     succ_task = dag.tasks[i]
        #     print "\n Successor Component ID ",
        #     print (dag.task_components[succ_task.id]).id
        #     if (dag.task_components[succ_task.id]).id is not task_component.id:
        #         print succ_task.name
        #         if dag.are_parents_finished(succ_task):
        #             print "\n Pushing to frontier task component of",
        #             print succ_task.name
        #             dag.task_components[succ_task.id].get_first_kernel().in_frontier = True
        #             print "UPDATE-FRONTIER", dag.task_components[succ_task.id].get_kernel_names()
        #             self.frontier.put(dag.task_components[succ_task.id])
        #
        #
        #             print "Frontier Size updated to " + str(len(self.frontier.queue))

    def update_execution_pool_of_task_sets(self):
        min_execution_time_CPU = 1e10
        min_execution_time_GPU = 1e10
        flag = False
        if (len(self.currently_executing['cpu']) > 0) or (len(self.currently_executing['gpu']) > 0):
            flag = True
        if flag is False:
            print "EMPTY ",
            print self.ready_queue,
            # print self.available_device_lookahead,
            # print self.waiting_tasks,
            # print self.nCPU,self.mGPU


        if (len(self.currently_executing['cpu']) > 0):
            first_task_CPU, dag_CPU, dev_CPU = self.currently_executing['cpu'][0]
            min_execution_time_CPU = first_task_CPU.projected_ex_time
            flag = flag and True
        if (len(self.currently_executing['gpu']) > 0):
            first_task_GPU, dag_GPU, dev_GPU = self.currently_executing['gpu'][0]
            min_execution_time_GPU = first_task_GPU.projected_ex_time
        cpu_index = 0
        gpu_index = 0
        self.prev_time_stamp = self.time_stamp
        self.estimate_prev_time_stamp = self.estimate_time_stamp
        # Obtain minimum execution time for cpu devices
        for i in range(0, len(self.currently_executing['cpu'])):
            task, dag, device_id = self.currently_executing['cpu'][i]
            # if device_id not in self.waiting_tasks['cpu']:
            # print "PROJECTED CPU execution pool update check for task ",
            # print task.name,
            # print " ",
            # print task.start_time,
            # print " + ",
            # print task.projected_ex_time,
            # print " = ",
            # print task.start_time + task.projected_ex_time
            if (min_execution_time_CPU > task.projected_ex_time):
                min_execution_time_CPU = task.projected_ex_time
                cpu_index = i
                # print "PROJECTED task with minimum finishing time on CPU: ",
                # print task.name
        # Obtain minimum execution time for gpu devices
        for i in range(0, len(self.currently_executing['gpu'])):
            task, dag, device_id = self.currently_executing['gpu'][i]
            # if device_id not in self.waiting_tasks['gpu']:

            # print "PROJECTED GPU CPU execution pool update check for task ",
            # print task.name,
            # print " ",
            # print task.start_time,
            # print " + ",
            # print task.projected_ex_time,
            # print " = ",
            # print task.start_time + task.projected_ex_time
            if (
                min_execution_time_GPU > task.projected_ex_time):  # removed task.start_time from equation, since the decision of finishing a task is dependent on current execution time
                min_execution_time_GPU = task.projected_ex_time
                gpu_index = i
                # print "PROJECTED task with minimum finishing time on GPU: ",
                # print task.name

        # print "PROJECTED CPU min time: " + str(min_execution_time_CPU)
        # print "PROJECTED GPU min time " + str(min_execution_time_GPU)

        # print len(self.currently_executing['cpu'])
        # print len(self.currently_executing['gpu'])
        cpu_task = SimTask(), None, 0
        gpu_task = SimTask(), None, 0

        if (min_execution_time_GPU < min_execution_time_CPU):
            executing_task = self.currently_executing['gpu'][gpu_index]
            task, dag, device_id = executing_task
            task.device_id = device_id
            task.device_type = "GPU"
            self.currently_executing['gpu'].pop(gpu_index)
            # Update system time stamp and finishing time of task
            execution_time = task.projected_ex_time
            # print "projected_ex_time ",
            # print task.projected_ex_time
            # print "PROJECTED Updating time stamp of " + str(task.name) + " " + str(self.time_stamp) + " to ",
            # print "GPU_OLD_TIME_STAMP: ", self.time_stamp, task.id
            self.time_stamp = self.time_stamp + execution_time
            # print "GPU_NEW_TIME_STAMP: ", self.time_stamp, task.id
            # self.time_stamp = min_execution_time_GPU
            # print(self.time_stamp)
            gpu_task = task, dag, device_id
            # self.update_finished_task_set(executing_task, "gpu")
        else:
            executing_task = self.currently_executing['cpu'][cpu_index]
            task, dag, device_id = executing_task
            task.device_id = device_id
            task.device_type = "CPU"
            self.currently_executing['cpu'].pop(cpu_index)
            # Update system time stamp and finishing time of task
            execution_time = task.projected_ex_time
            # print "projected_ex_time ",
            # print task.projected_ex_time
            # print "PROJECTED Updating time stamp " + str(task.name) + " " + str(self.time_stamp) + " to ",
            # print "CPU_OLD_TIME_STAMP: ",self.time_stamp, task.id
            self.time_stamp = self.time_stamp + execution_time
            # print "CPU_NEW_TIME_STAMP: ",self.time_stamp, task.id
            self.estimate_time_stamp += task.estimated_ex_time
            # self.time_stamp = min_execution_time_CPU
            # print(self.time_stamp)
            cpu_task = task, dag, device_id
            # self.update_finished_task_set(executing_task, "cpu")

        # Diminish projected execution time as per elapsed time of scheduling engine

        # print "PROJECTED Time stamp: " + str(self.time_stamp)
        # print "PROJECTED Previous Time Stamp: " + str(self.prev_time_stamp)
        elapsed = self.time_stamp - self.prev_time_stamp

        estimated_elapsed = self.estimate_time_stamp - self.estimate_prev_time_stamp
        # print "ESTIMATED_ELAPSED due to finishing of",task.name,elapsed, estimated_elapsed
        # print "Elapsed: " + str(elapsed)
        for i in range(0, len(self.currently_executing['cpu'])):
            task, dag, device_id = self.currently_executing['cpu'][i]
            # print "PROJECTED Diminished time for CPU device ",
            # print i,
            # print "and task: ",
            # print task.name,
            # print " ",
            # print " ",
            # print task.projected_ex_time,
            # print " - ",
            # print elapsed,
            # print " = ",
            task.projected_ex_time = task.projected_ex_time - elapsed
            task.estimated_ex_time -= estimated_elapsed
            # print task.projected_ex_time
        # Diminish projected execution time as per elapsed time of scheduling engine
        for i in range(0, len(self.currently_executing['gpu'])):
            task, dag, device_id = self.currently_executing['gpu'][i]
            # print "PROJECTED Diminished time for GPU device ",
            # print i,
            # print "and task: ",
            # print task.name,
            # print " ",
            # print task.projected_ex_time,
            # print " - ",
            # print elapsed,
            # print " = ",
            task.projected_ex_time = task.projected_ex_time - elapsed
            task.estimated_ex_time = task.estimated_ex_time - estimated_elapsed
            # print task.projected_ex_time
            # print "for elapsed time ",
            # print elapsed

        # Diminished execution time of device waiting times

        # print "AVAILABLE_DEVICE_LOOKAHEAD_BEFORE ",self.available_device_lookahead
        for i in range(0, len(self.available_device_lookahead['gpu'])):
            # self.available_device_lookahead['gpu'][i] -= elapsed
            self.available_device_lookahead['gpu'][i] -= estimated_elapsed

        for i in range(0, len(self.available_device_lookahead['cpu'])):
            self.available_device_lookahead['cpu'][i] -= estimated_elapsed

        # print "AVAILABLE_DEVICE_LOOKAHEAD_AFTER ", self.available_device_lookahead

        # Update finishing of task set only after update of current execution pool execution times for each task

        
        
        ex_cpu, ex_gpu = self.ex_map

        if (min_execution_time_GPU < min_execution_time_CPU):
            gpu_t, x, y = gpu_task
            # print "UPDATE_FINISHED_TASK_SET", gpu_t.id ,"GPU" ,y
            self.update_finished_task_set(gpu_task, "gpu")
            # print lineno(), "UPDATING ", gpu_t.name, gpu_t.feature_vector,ex_gpu[gpu_t.name]
            if gpu_t.feature_vector not in self.log:
                
                self.log[gpu_t.feature_vector] = {}
                self.log[gpu_t.feature_vector]["cpu"] = - 1.0
                self.log[gpu_t.feature_vector]["gpu"] = ex_gpu[gpu_t.name]
            else:
                self.log[gpu_t.feature_vector]["gpu"] = ex_gpu[gpu_t.name]


        else:
            cpu_t, x, y = cpu_task
            # print "UPDATE_FINISHED_TASK_SET", cpu_t.id ,"CPU" ,y
            self.update_finished_task_set(cpu_task, "cpu")
            # print lineno(), "UPDATING ", cpu_t.name, cpu_t.feature_vector,ex_cpu[cpu_t.name]
            if cpu_t.feature_vector not in self.log:
                
                self.log[cpu_t.feature_vector] = {}
                self.log[cpu_t.feature_vector]["gpu"] = - 1.0
                self.log[cpu_t.feature_vector]["cpu"] = ex_cpu[cpu_t.name]
            else:
                self.log[cpu_t.feature_vector]["cpu"] = ex_cpu[cpu_t.name]
        self.makespan = self.time_stamp
        
        if self.online:
            self.release_dags(self.time_stamp)

        return cpu_task,gpu_task

    def schedule_linear_clusters(self, dags, depth):
        # print "Schedule Engine for Task Sets Initiated"
        if not self.online:
            self.dags = dags
        # if self.online:
        #     self.create_random_dags_online(0)
        # Compute ranks of tasks for all DAGs

        # print "Computing Ranks"

            r = Ranker(dags, self.ex_map)

            r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)
       
        # for dag in dags:
        #    for node in dag.skeleton.nodes():
        #        print "RANK ", node, dag.tasks[node].name, dag.tasks[node].rank

        # Set up device queues
       
        
        # print "Rechecking task component ids"
        dt_max_value = 0.0
        for dag in dags:
            if dt_max_value < dag.get_max_dt():
                dt_max_value = dag.get_max_dt()
            dag.assign_device_lookahead(self.available_device_lookahead)
            dag.assign_currently_executing(self.currently_executing)

            # for task_component in dag.G.nodes():
            #     print str(task_component.dag_id) + "-->" + str(task_component.id)

        # print "Setting up device queues"

        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        # print "Initializing Frontier of Task Components"
    
        for dag in self.dags:
            for task_component in dag.free_task_components:
                task_component.get_first_kernel().in_frontier = True
                self.frontier.put(task_component)

        #Scheduling Iteration Behaviour
        dispatch_counter = 0
        while not all_processed(self.dags):
            # print "LC_DEBUG_Dispatching CURRENT FRONTIER TC_GEN",
            # print len([tc.get_kernel_ids() for tc in self.frontier.queue])
            # print len(self.dags)
            # print "Iteration Stats: nCPU " + str(self.nCPU),
            # print " mGPU " + str(self.mGPU),
            # print " Frontier Size " + str(len(self.frontier.queue))
            # print "Currently Executing ",
            # for dkey in self.currently_executing.keys():
            #     for tkey in self.currently_executing[dkey]:
            #             task, dag, device_id =  tkey
            #             print dkey, device_id, task.id,
            # print "."
            # print "Ready Queue", self.ready_queue,
            
            # print "SUSPENDED ,",
            # # print "Suspended", self.waiting_tasks
            # for dkey in self.waiting_tasks.keys():
            #     for tkey in self.waiting_tasks[dkey]:
            #         if self.waiting_tasks[dkey][tkey] != None:
            #             print dkey, tkey, self.waiting_tasks[dkey][tkey].get_kernel_ids(),
            # print "."


            if (self.nCPU == 0 and self.mGPU == 0) or self.frontier.empty():
                self.update_execution_pool_of_task_sets()


            else:
                if not self.frontier.empty():

                    cluster_map = []
                    task_chain_map = {}
                    frontiers_removed = []
                    blacklist_frontier = []
                    # print "LC_DEBUG_Frontier_Beginning", [f.get_first_kernel().id for f in self.frontier.queue]
                    # print "LC_DEBUG: Before Cluster Merging", self.ready_queue
                    while (len(self.ready_queue["cpu"]) > 0 or len(self.ready_queue["gpu"]) > 0) and len(self.frontier.queue) > 0:

                        task_index_in_queue = 0
                        task_object = None
                        max = 0
                        for i in range(0, len(self.frontier.queue)):
                            task_rank = self.frontier.queue[i].get_first_kernel().rank
                            if task_rank > max:
                                task_object = self.frontier.queue[i]
                                max = task_rank
                                task_index_in_queue = i

                        starting_task = self.frontier.queue[task_index_in_queue]
                        self.frontier.queue.remove(starting_task)
                        frontiers_removed.append(starting_task)
                        # print "LC_DEBUG: Removing frontier task", dispatch_counter, starting_task.get_kernel_ids()
                        dag = self.dags[starting_task.dag_id]
                        # print "LC_DEBUG: Invoking lc contraction"
                        clusters_generated = dag.linear_cluster_contraction(starting_task, depth, self.ready_queue, task_chain_map, self.log)


                        if len(clusters_generated) == 0:
                            blacklist_frontier.append(starting_task)
                        # print "Clusters for ",starting_task.get_first_kernel().id, starting_task.get_first_kernel().Class
                        for cluster in clusters_generated:
                            # cluster.print_information()
                            if cluster.frontier_task:
                                f = None
                                if cluster.chain_type == "succ":
                                    f = cluster.tasks[0]
                                else:
                                    # f = cluster.tasks[-1]
                                    f = cluster.tasks[0]
                                if f in self.frontier.queue and f != starting_task:
                                    # print "LC_DEBUG_Removing additional frontier task", f.get_first_kernel().id
                                    self.frontier.queue.remove(f)
                                    frontiers_removed.append(f)
                        
                        # cluster_map.extend(clusters_generated)
                        for c in clusters_generated:
                            if c not in cluster_map:
                                cluster_map.append(c)

                    # print "LC_DEBUG: Overall clusters created and dispatched"
                    dispatch_counter += 1
                    # print lineno(), "LC_DEBUG_Dispatching, future DISPATCH COUNTER ", dispatch_counter
                    if len(cluster_map) == 0:

                        for f in frontiers_removed:
                            self.frontier.queue.append(f)
                        # print "LC_DEBUG_Frontier_ClusterMapNotGenerated", [f.get_first_kernel().id for f in self.frontier.queue]
                        # print self.frontier.queue
                        # print "LC_DEBUG After Cluster Merging", self.ready_queue
                        # print lineno(), "CALLING UPDATE"
                        self.update_execution_pool_of_task_sets()
                        #restore frontier


                    else:
                        # print "LC_DEBUG After Cluster Merging", self.ready_queue
                        # print "LC_DEBUG_Frontier_ClusterMapGenerated", [f.get_first_kernel().id for f in self.frontier.queue], [c.get_task_ids() for c in cluster_map]
                        for cluster in cluster_map:
                            # print "LC_DEBUG_Possible_Dispatch: ",
                            # cluster.print_information()
                            dag = self.dags[cluster.dag_id]
                            cpu = -1
                            gpu = -1
                            if cluster.device_type == "cpu":
                                cpu = cluster.device_id
                            else:
                                gpu = cluster.device_id
                            if cluster.frontier_task: 
                                # print "LC_DEBUG_Dispatching Task ",
                                # cluster.print_information()
                                if cluster.chain_type == "succ":
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"


                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)

                                    self.dispatch_task_set(cpu, gpu, dispatching_task_set, dag)

                                else:
                                    # cluster.tasks.reverse()
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    self.dispatch_task_set(cpu, gpu, dispatching_task_set, dag)
                            else:
                                # print "LC_DEBUG_future: To be dispatched in the future",
                                # cluster.print_information()
                                if cluster.chain_type == "succ":
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    dispatching_task_set.to_be_scheduled = True
                                    dispatching_task_set.future_device_type = cluster.device_type
                                    dispatching_task_set.future_device_id = cluster.device_id
                                
                                
                                else:
                                    # cluster.tasks.reverse()
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    dispatching_task_set.to_be_scheduled = True
                                    dispatching_task_set.future_device_type = cluster.device_type
                                    dispatching_task_set.future_device_id = cluster.device_id

                                if dispatching_task_set.future_device_type == "cpu":
                                    self.nCPU -= 1
                                else:
                                    self.mGPU -= 1

                    for b in blacklist_frontier:
                        if b not in self.frontier.queue and not b.is_supertask() and not b.is_dispatched:
                            self.frontier.queue.append(b)
        # self.makespan = self.time_stamp


    def schedule_linear_clusters_with_queues(self, dags, depth):
        # print "Schedule Engine for Task Sets Initiated"
        if not self.online:
            self.dags = dags
        # if self.online:
        #     self.create_random_dags_online(0)
        # Compute ranks of tasks for all DAGs

        # print "Computing Ranks"

            r = Ranker(dags, self.ex_map)

            r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)
       
        # for dag in dags:
        #    for node in dag.skeleton.nodes():
        #        print "RANK ", node, dag.tasks[node].name, dag.tasks[node].rank

        # Set up device queues
       
        
        # print "Rechecking task component ids"
        dt_max_value = 0.0
        for dag in dags:
            if dt_max_value < dag.get_max_dt():
                dt_max_value = dag.get_max_dt()
            dag.assign_device_lookahead(self.available_device_lookahead)
            dag.assign_currently_executing(self.currently_executing)

            # for task_component in dag.G.nodes():
            #     print str(task_component.dag_id) + "-->" + str(task_component.id)

        # print "Setting up device queues"

        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        # print "Initializing Frontier of Task Components"
    
        for dag in self.dags:
            for task_component in dag.free_task_components:
                task_component.get_first_kernel().in_frontier = True
                self.frontier.put(task_component)

        #Scheduling Iteration Behaviour
        dispatch_counter = 0
        while not all_processed(self.dags):
            print "LC_DEBUG_Dispatching CURRENT FRONTIER TC_GEN",
            print len([tc.get_kernel_ids() for tc in self.frontier.queue])
            print len(self.dags)
            # print "Iteration Stats: nCPU " + str(self.nCPU),
            # print " mGPU " + str(self.mGPU),
            # print " Frontier Size " + str(len(self.frontier.queue))
            # print "Currently Executing ",
            # for dkey in self.currently_executing.keys():
            #     for tkey in self.currently_executing[dkey]:
            #             task, dag, device_id =  tkey
            #             print dkey, device_id, task.id,
            # print "."
            # print "Ready Queue", self.ready_queue,
            
            # print "SUSPENDED ,",
            # # print "Suspended", self.waiting_tasks
            # for dkey in self.waiting_tasks.keys():
            #     for tkey in self.waiting_tasks[dkey]:
            #         if self.waiting_tasks[dkey][tkey] != None:
            #             print dkey, tkey, self.waiting_tasks[dkey][tkey].get_kernel_ids(),
            # print "."


            if self.nCPU == 0 and self.mGPU == 0 or self.frontier.empty():

                self.update_execution_pool_of_task_sets()


            else:
                if not self.frontier.empty():

                    cluster_map = []
                    task_chain_map = {}
                    frontiers_removed = []
                    blacklist_frontier = []
                    # print "LC_DEBUG_Frontier_Beginning", [f.get_first_kernel().id for f in self.frontier.queue]
                    # print "LC_DEBUG: Before Cluster Merging", self.ready_queue
                    while (len(self.ready_queue["cpu"]) > 0 or len(self.ready_queue["gpu"]) > 0) and len(self.frontier.queue) > 0:

                        task_index_in_queue = 0
                        task_object = None
                        max = 0
                        for i in range(0, len(self.frontier.queue)):
                            task_rank = self.frontier.queue[i].get_first_kernel().rank
                            if task_rank > max:
                                task_object = self.frontier.queue[i]
                                max = task_rank
                                task_index_in_queue = i

                        starting_task = self.frontier.queue[task_index_in_queue]
                        self.frontier.queue.remove(starting_task)
                        frontiers_removed.append(starting_task)
                        # print "LC_DEBUG: Removing frontier task", dispatch_counter, starting_task.get_kernel_ids()
                        dag = self.dags[starting_task.dag_id]
                        # print "LC_DEBUG: Invoking lc contraction"
                        clusters_generated = dag.linear_cluster_contraction(starting_task, depth, self.ready_queue, task_chain_map, self.log)


                        if len(clusters_generated) == 0:
                            blacklist_frontier.append(starting_task)
                        # print "Clusters for ",starting_task.get_first_kernel().id, starting_task.get_first_kernel().Class
                        for cluster in clusters_generated:
                            # cluster.print_information()
                            if cluster.frontier_task:
                                f = None
                                if cluster.chain_type == "succ":
                                    f = cluster.tasks[0]
                                else:
                                    # f = cluster.tasks[-1]
                                    f = cluster.tasks[0]
                                if f in self.frontier.queue and f != starting_task:
                                    # print "LC_DEBUG_Removing additional frontier task", f.get_first_kernel().id
                                    self.frontier.queue.remove(f)
                                    frontiers_removed.append(f)
                        
                        # cluster_map.extend(clusters_generated)
                        for c in clusters_generated:
                            if c not in cluster_map:
                                cluster_map.append(c)

                    # print "LC_DEBUG: Overall clusters created and dispatched"
                    dispatch_counter += 1
                    # print lineno(), "LC_DEBUG_Dispatching, future DISPATCH COUNTER ", dispatch_counter
                    if len(cluster_map) == 0:

                        for f in frontiers_removed:
                            self.frontier.queue.append(f)
                        # print "LC_DEBUG_Frontier_ClusterMapNotGenerated", [f.get_first_kernel().id for f in self.frontier.queue]
                        # print self.frontier.queue
                        # print "LC_DEBUG After Cluster Merging", self.ready_queue
                        # print lineno(), "CALLING UPDATE"
                        self.update_execution_pool_of_task_sets()
                        #restore frontier


                    else:
                        # print "LC_DEBUG After Cluster Merging", self.ready_queue
                        # print "LC_DEBUG_Frontier_ClusterMapGenerated", [f.get_first_kernel().id for f in self.frontier.queue], [c.get_task_ids() for c in cluster_map]
                        for cluster in cluster_map:
                            # print "LC_DEBUG_Possible_Dispatch: ",
                            # cluster.print_information()
                            dag = self.dags[cluster.dag_id]
                            cpu = -1
                            gpu = -1
                            if cluster.device_type == "cpu":
                                cpu = cluster.device_id
                            else:
                                gpu = cluster.device_id
                            if cluster.frontier_task: 
                                # print "LC_DEBUG_Dispatching Task ",
                                # cluster.print_information()
                                if cluster.chain_type == "succ":
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"


                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)

                                    self.dispatch_task_set(cpu, gpu, dispatching_task_set, dag)

                                else:
                                    # cluster.tasks.reverse()
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    self.dispatch_task_set(cpu, gpu, dispatching_task_set, dag)
                            else:
                                # print "LC_DEBUG_future: To be dispatched in the future",
                                # cluster.print_information()
                                if cluster.chain_type == "succ":
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    dispatching_task_set.to_be_scheduled = True
                                    dispatching_task_set.future_device_type = cluster.device_type
                                    dispatching_task_set.future_device_id = cluster.device_id
                                
                                
                                else:
                                    # cluster.tasks.reverse()
                                    if cluster.device_type == "cpu":
                                        cluster.tasks[0].Class ="ZERO"
                                    else:
                                        cluster.tasks[0].Class = "TEN"
                                    dispatching_task_set = dag.merge_task_list(cluster.tasks)
                                    dispatching_task_set.to_be_scheduled = True
                                    dispatching_task_set.future_device_type = cluster.device_type
                                    dispatching_task_set.future_device_id = cluster.device_id

                                if dispatching_task_set.future_device_type == "cpu":
                                    self.nCPU -= 1
                                else:
                                    self.mGPU -= 1

                    for b in blacklist_frontier:
                        if b not in self.frontier.queue and not b.is_supertask() and not b.is_dispatched:
                            self.frontier.queue.append(b)
        # self.makespan = self.time_stamp











    def schedule_workflows(self, dags, contract_function, width, depth):

        # print "Schedule Engine for Task Sets Initiated"
        
        if not self.online:
        
            self.dags = dags
        # if self.online:
        #     self.create_random_dags_online(0)

        # Compute ranks of tasks for all DAGs

        # print "Computing Ranks"

            r = Ranker(dags, self.ex_map)

            r.compute_rank_dags(self.rank_name, self.num_CPU_devices, self.num_GPU_devices)
        # for dag in dags:
        #    for node in dag.skeleton.nodes():
               # print "RANK ", node, dag.tasks[node].name, dag.tasks[node].rank

        # Set up device queues

        # print "Rechecking task component ids"
        dt_max_value = 0.0
        for dag in dags:
            if dt_max_value < dag.get_max_dt():
                dt_max_value = dag.get_max_dt()
            dag.assign_device_lookahead(self.available_device_lookahead)
            dag.assign_currently_executing(self.currently_executing)

            # for task_component in dag.G.nodes():
            #     print str(task_component.dag_id) + "-->" + str(task_component.id)

        # print "Setting up device queues"

        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        # print "Initializing Frontier of Task Components"
    
        for dag in self.dags:
            for task_component in dag.free_task_components:
                task_component.get_first_kernel().in_frontier = True
                self.frontier.put(task_component)

        # Scheduling iteration behaviour
        # print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
        #     self.mGPU) + "GPU devices"

        while (not all_processed(self.dags)):
            # print "DAG Pool", len(dags)
            # print "CURRENT FRONTIER TC_GEN",
            # print [tc.get_kernel_ids() for tc in self.frontier.queue]
            # print "Iteration Stats: nCPU " + str(self.nCPU),
            # print " mGPU " + str(self.mGPU),
            # print " Frontier Size " + str(len(self.frontier.queue)),
            # print "Currently Executing ", self.currently_executing,
            # print "Ready Queue", self.ready_queue,
            # print "Suspended", self.waiting_tasks

            if (self.nCPU == 0 and self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_sets()
            else:
                if (not self.frontier.empty()):
                    task_index_in_queue = 0
                    task_object = None
                    max = 0
                    for i in range(0, len(self.frontier.queue)):
                        task_rank = self.frontier.queue[i].get_first_kernel().rank
                        if task_rank > max:
                            task_object = self.frontier.queue[i]
                            max = task_rank
                            task_index_in_queue = i
                    dispatching_task_component = self.frontier.queue[task_index_in_queue]
                    # print "SELECTED TC_GEN of ", dispatching_task_component.get_kernel_ids()
                    dag = self.dags[dispatching_task_component.dag_id]
                    dispatching_task_set = None

                    if dispatching_task_component.contract:
                        if contract_function == "naive":
                            dispatching_task_set = dag.naive_contraction(dispatching_task_component, width, depth, self.frontier, self.nCPU, self.mGPU, dt_max_value, 1.0)
                        elif contract_function == "lsc":
                            dispatching_task_set = dag.local_scenario_contraction(dispatching_task_component, width, depth, self.frontier, self.nCPU, self.mGPU, dt_max_value, 1.0)
                        elif contract_function == "greedy_contraction":
                            dispatching_task_set = dag.greedy_gpu_contraction(dispatching_task_component, width, depth)
                        elif contract_function == "adaptive_naive_contraction":
                            dispatching_task_set = dag.adaptive_naive_contraction(dispatching_task_component,
                                                                                  2, self.frontier, self.nCPU,
                                                                                  self.mGPU, dt_max_value, 1.0)
                        else:
                            dispatching_task_set = dispatching_task_component

                    else:
                        dispatching_task_set = dispatching_task_component

                    cpu = -1
                    gpu = -1

                    # print "TC_GEN", self.nCPU, self.mGPU
                    if dispatching_task_set is None:
                        dispatching_task_set = dispatching_task_component
                    # print "Class of task ",dispatching_task_component.get_first_kernel().id,partition_class(dispatching_task_set)
                    if (partition_class(dispatching_task_set) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            # self.frontier.get()
                            self.frontier.queue.remove(dispatching_task_component)
                            # print "SCHEDULE: Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                # dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, self.dags[dispatching_task_set.dag_id])
                        else:
                            if not self.currently_executing['gpu']:
                                succ_component = None
                                for succ in dag.get_task_component_children(dispatching_task_set):
                                    if succ in self.waiting_tasks['gpu'].values():
                                        succ_component = succ
                                        break


                                gpu = self.waiting_tasks['gpu'].keys()[self.waiting_tasks['gpu'].values().index(succ_component)]
                                # self.frontier.get()
                                self.frontier.queue.remove(dispatching_task_component)
                                # print "Dispatching " + str(
                                    # list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                    # dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                                self.dispatch_task_set(cpu, gpu, dispatching_task_set, self.dags[dispatching_task_set.dag_id])
                            else:
                                self.update_execution_pool_of_task_sets()

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            # self.frontier.get()
                            self.frontier.queue.remove(dispatching_task_component)
                            # print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                # dispatching_task_set.dag_id) + " to CPU" + str(cpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, self.dags[dispatching_task_set.dag_id])
                        else:
                            if not self.currently_executing['cpu']:
                                succ_component = None
                                for succ in dag.get_task_component_children(dispatching_task_set):
                                    if succ in self.waiting_tasks['gpu'].values():
                                        succ_component = succ
                                        break
                                cpu = self.waiting_tasks['cpu'].keys()[self.waiting_tasks['cpu'].values().index(succ_component)]
                                # self.frontier.get()
                                self.frontier.queue.remove(dispatching_task_component)
                                # print "Dispatching " + str(
                                    # list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                    # dispatching_task_set.dag_id) + " to CPU " + str(cpu) + " at ",
                                self.dispatch_task_set(cpu, gpu, dispatching_task_set, self.dags[dispatching_task_set.dag_id])
                            else:
                                self.update_execution_pool_of_task_sets()
            self.makespan = self.time_stamp


    def schedule_task_sets(self, dags):

        print "Schedule Engine for Task Sets Initiated"

        # Compute ranks of tasks for all DAGs

        print "Computing Ranks"

        r = Ranker(dags, self.ex_map)
        # r.compute_rank_dags("DT_next_level")
        r.compute_rank_dags(self.rank_name)

        # Set up device queues
        print "Rechecking task component ids"
        for dag in dags:
            for task_component in dag.G.nodes():
                print str(task_component.dag_id) + "-->" + str(task_component.id)
        print "Setting up device queues"
        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier

        print "Initializing Frontier of Task Components"
        for dag in dags:
            for task_component in dag.free_task_components:
                self.frontier.put(task_component)

        # for tasks in self.frontier.queue:
        #     print tasks.get_kernel_classes()

        # Scheduling iteration behaviour
        print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
            self.mGPU) + "GPU devices"
        while (not all_processed(dags)):
            # print "nCPU " + str(self.nCPU)
            # print "mGPU " + str(self.mGPU)
            # print "Frontier Size " + str(len(self.frontier.queue))

            if (self.nCPU == 0 or self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_sets()
            else:
                if (not self.frontier.empty()):
                    dispatching_task_set = self.frontier.queue[0]
                    # dag = dags[dispatching_task_set.dag_id]
                    # dispatching_task_set = dag.contract_component(dispatching_task_set)
                    cpu = -1
                    gpu = -1
                    if (partition_class(dispatching_task_set) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to CPU" + str(cpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])
            self.makespan = self.time_stamp

    def schedule_task_sets_contraction(self, dags):

        print "Schedule Engine for Task Sets Initiated"

        # Compute ranks of tasks for all DAGs

        print "Computing Ranks"

        r = Ranker(dags, self.ex_map)
        r.compute_rank_dags(self.rank_name)

        # Set up device queues
        print "Rechecking task component ids"
        for dag in dags:
            for task_component in dag.G.nodes():
                print str(task_component.dag_id) + "-->" + str(task_component.id)
        print "Setting up device queues"
        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        for dag in dags:
            for task_component in dag.free_task_components:
                self.frontier.put(task_component)
                task_component.get_first_kernel().in_frontier = True

        print "Initializing Frontier of Task Components"
        '''
        for dag in dags:
            tc_list = []
            for task_component in dag.free_task_components:
                print "Free tasks for dag " + str(dag.dag_id),
                print list(task_component.get_kernel_ids())
                # tc_list = []
                tc_list.append(task_component)
            cc = dag.get_connected_components_two_level(tc_list)
            for component in cc:
                print "Component Size for dag " + str(dag.dag_id),
                print len(component.nodes())
                tc = dag.merge_task_list(component.nodes())
                self.frontier.put(tc)

        for dag in dags:
            print "DAG " + str(dag.dag_id)
            dag.print_information("upward_rank_ECO_DT")
        '''
        # for tasks in self.frontier.queue:
        #     print tasks.get_kernel_classes()

        # Scheduling iteration behaviour
        print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
            self.mGPU) + "GPU devices"
        while (not all_processed(dags)):
            print "Iteration Stats: nCPU " + str(self.nCPU),
            print " mGPU " + str(self.mGPU),
            print " Frontier Size " + str(len(self.frontier.queue))

            if (self.nCPU == 0 or self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_sets()
            else:
                if (not self.frontier.empty()):
                    dispatching_task_set = self.frontier.queue[0]
                    dag = dags[dispatching_task_set.dag_id]
                    dispatching_task_set = dag.naive_contraction(dispatching_task_set, 0, 1)
                    cpu = -1
                    gpu = -1
                    if (partition_class(dispatching_task_set) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to CPU" + str(cpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])
            self.makespan = self.time_stamp

    def schedule_task_sets_greedy_contraction_execution_time(self, dags):

        print "Schedule Engine for Task Sets Initiated"

        # Compute ranks of tasks for all DAGs

        print "Computing Ranks"

        r = Ranker(dags, self.ex_map)
        r.compute_rank_dags(self.rank_name)

        # Set up device queues
        print "Rechecking task component ids"
        for dag in dags:
            for task_component in dag.G.nodes():
                print str(task_component.dag_id) + "-->" + str(task_component.id)
        print "Setting up device queues"
        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        for dag in dags:
            dag.assign_device_lookahead(self.available_device_lookahead)
            for task_component in dag.free_task_components:
                self.frontier.put(task_component)

        print "Initializing Frontier of Task Components"
        '''
        for dag in dags:
            tc_list = []
            for task_component in dag.free_task_components:
                print "Free tasks for dag " + str(dag.dag_id),
                print list(task_component.get_kernel_ids())
                # tc_list = []
                tc_list.append(task_component)
            cc = dag.get_connected_components_two_level(tc_list)
            for component in cc:
                print "Component Size for dag " + str(dag.dag_id),
                print len(component.nodes())
                tc = dag.merge_task_list(component.nodes())
                self.frontier.put(tc)
    
        for dag in dags:
            print "DAG " + str(dag.dag_id)
            dag.print_information("upward_rank_ECO_DT")
        '''
        # for tasks in self.frontier.queue:
        #     print tasks.get_kernel_classes()

        # Scheduling iteration behaviour
        print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
            self.mGPU) + "GPU devices"
        while (not all_processed(dags)):
            print "Iteration Stats: nCPU " + str(self.nCPU),
            print " mGPU " + str(self.mGPU),
            print " Frontier Size " + str(len(self.frontier.queue))

            if (self.nCPU == 0 or self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_sets()
            else:
                if (not self.frontier.empty()):
                    dispatching_task_set = self.frontier.queue[0]
                    dag = dags[dispatching_task_set.dag_id]
                    dispatching_task_set = dag.greedy_contraction(dispatching_task_set, self.num_CPU_devices,
                                                                  self.num_GPU_devices)
                    cpu = -1
                    gpu = -1
                    if (partition_class(dispatching_task_set) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to CPU" + str(cpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])
            self.makespan = self.time_stamp

    def schedule_task_sets_contraction_execution_time(self, dags):

        print "Schedule Engine for Task Sets Initiated"

        # Compute ranks of tasks for all DAGs

        print "Computing Ranks"

        r = Ranker(dags, self.ex_map)
        r.compute_rank_dags(self.rank_name)

        # Set up device queues
        print "Rechecking task component ids"
        for dag in dags:
            for task_component in dag.G.nodes():
                print str(task_component.dag_id) + "-->" + str(task_component.id)
        print "Setting up device queues"
        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier
        for dag in dags:
            # dag.assign_device_lookahead(self.available_device_lookahead)
            for task_component in dag.free_task_components:
                self.frontier.put(task_component)

        print "Initializing Frontier of Task Components"
        '''
        for dag in dags:
            tc_list = []
            for task_component in dag.free_task_components:
                print "Free tasks for dag " + str(dag.dag_id),
                print list(task_component.get_kernel_ids())
                # tc_list = []
                tc_list.append(task_component)
            cc = dag.get_connected_components_two_level(tc_list)
            for component in cc:
                print "Component Size for dag " + str(dag.dag_id),
                print len(component.nodes())
                tc = dag.merge_task_list(component.nodes())
                self.frontier.put(tc)
    
        for dag in dags:
            print "DAG " + str(dag.dag_id)
            dag.print_information("upward_rank_ECO_DT")
        '''
        # for tasks in self.frontier.queue:
        #     print tasks.get_kernel_classes()

        # Scheduling iteration behaviour
        print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
            self.mGPU) + "GPU devices"
        while (not all_processed(dags)):
            print "Iteration Stats: nCPU " + str(self.nCPU),
            print " mGPU " + str(self.mGPU),
            print " Frontier Size " + str(len(self.frontier.queue))

            if (self.nCPU == 0 or self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_sets()
            else:
                if (not self.frontier.empty()):
                    dispatching_task_set = self.frontier.queue[0]
                    dag = dags[dispatching_task_set.dag_id]
                    dispatching_task_set = dag.contract_component_extime(dispatching_task_set, self.num_CPU_devices,
                                                                         self.num_GPU_devices)
                    cpu = -1
                    gpu = -1
                    if (partition_class(dispatching_task_set) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(list(dispatching_task_set.get_kernel_ids())) + " of DAG " + str(
                                dispatching_task_set.dag_id) + " to CPU" + str(cpu) + " at ",
                            self.dispatch_task_set(cpu, gpu, dispatching_task_set, dags[dispatching_task_set.dag_id])
            self.makespan = self.time_stamp


    def schedule_task_components_DT_contraction_level1(self, dags):

        print "Schedule Engine for Task Components Initiated"

        # Compute ranks of tasks for all DAGs

        print "Computing Ranks"

        r = Ranker(dags, ex_map)
        r.compute_rank_dags("upward_rank_ECO_DT")

        # Set up device queues
        print "Rechecking task component ids"
        for dag in dags:
            for task_component in dag.G.nodes():
                print str(task_component.dag_id) + "-->" + str(task_component.id)
        print "Setting up device queues"
        for i in range(0, self.nCPU):
            self.ready_queue['cpu'].append(i)
        for i in range(0, self.mGPU):
            self.ready_queue['gpu'].append(i)

        # Initialize frontier

        print "Initializing Frontier of Task Components"
        for dag in dags:
            for task_component in dag.free_task_components:
                tc_list = []
                tc_list.append(task_component)
            cc = dag.get_connected_components_k_level(tc_list, 1)
            for component in cc:
                tc = dag.merge_task_list(component.nodes())
                self.frontier.put(tc)

        # for tasks in self.frontier.queue:
        #     print tasks.get_kernel_classes()

        # Scheduling iteration behaviour
        print "Primary Scheduling Iteration Starts here with " + str(self.nCPU) + " CPU and " + str(
            self.mGPU) + "GPU devices"
        while (not all_processed(dags)):
            print "nCPU " + str(self.nCPU)
            print "mGPU " + str(self.mGPU)
            print "Frontier Size " + str(len(self.frontier.queue))

            if (self.nCPU == 0 or self.mGPU == 0 or self.frontier.empty()):
                self.update_execution_pool_of_task_components()
            else:
                if (not self.frontier.empty()):
                    dispatching_task_component = self.frontier.queue[0]

                    cpu = -1
                    gpu = -1
                    if (partition_class(dispatching_task_component) == "gpu"):
                        if self.mGPU > 0:
                            gpu = self.ready_queue['gpu'].popleft()
                            self.frontier.get()
                            print "Dispatching " + str(
                                list(dispatching_task_component.get_kernel_names())[0]) + " of DAG " + str(
                                dispatching_task_component.dag_id) + " to GPU " + str(gpu) + " at ",
                            self.dispatch_task_component(cpu, gpu, dispatching_task_component,
                                                         dags[dispatching_task_component.dag_id])

                    else:
                        if self.nCPU > 0:
                            cpu = self.ready_queue['cpu'].popleft()
                            self.dispatch_task_component(cpu, gpu, dispatching_task_component,
                                                         dags[dispatching_task_component.dag_id])
                            self.frontier.get()
                            print "Dispatching " + str(
                                list(dispatching_task_component.get_kernel_names())[0]) + " of DAG " + str(
                                dispatching_task_component.dag_id) + " to CPU" + str(cpu) + " at ",
        self.makespan = self.time_stamp

    def get_device_history(self):
        print "CPU History"
        for cpu_history in self.device_history['cpu']:
            print cpu_history
        print "GPU History"
        for gpu_history in self.device_history['gpu']:
            print gpu_history

    def get_device_utilization(self):
        devs = {'cpu': [], 'gpu': []}
        dev_utilization = {'cpu': [], 'gpu': []}

        for i in range(0, self.num_CPU_devices):
            d = []
            devs['cpu'].append(d)
        for i in range(0, self.num_GPU_devices):
            d = []
            devs['gpu'].append(d)
        for cpu_history in self.device_history['cpu']:
            device_id, task_name, task_dag_id, task_start_time, task_finish_time, task_execution_time = cpu_history
            time_stats = task_start_time, task_finish_time
            devs['cpu'][device_id].append(time_stats)
        for gpu_history in self.device_history['gpu']:
            device_id, task_name, task_dag_id, task_start_time, task_finish_time, task_execution_time = gpu_history
            time_stats = task_start_time, task_finish_time
            devs['gpu'][device_id].append(time_stats)
        for cpu_device_stats in devs['cpu']:
            sorted(cpu_device_stats, key=lambda x: x[1])
        for gpu_device_stats in devs['gpu']:
            sorted(gpu_device_stats, key=lambda x: x[1])
        # for cpu_device_stats in devs['cpu']:
        #     print cpu_device_stats
        # for gpu_device_stats in devs['gpu']:
        #     print gpu_device_stats
        for dtype in ['cpu', 'gpu']:
            for device_id in range(0, len(devs[dtype])):
                idle_time = 0.0
                if len(devs[dtype][device_id]) > 0:
                    start, final = devs[dtype][device_id][0]
                    for i in range(1, len(devs[dtype][device_id])):
                        s1, f1 = devs[dtype][device_id][i - 1]
                        s2, f2 = devs[dtype][device_id][i]
                        diff = s2 - f1
                        # print diff
                        idle_time += diff
                        final = f2
                    dev_utilization[dtype].append((start, final, idle_time))

        print devs
        for dtype in ['cpu', 'gpu']:
            for device_id in range(0, len(devs[dtype])):
                if len(devs[dtype][device_id]) > 0:
                    start, final, idle_time = dev_utilization[dtype][device_id]
                    utilization = (1.0 - idle_time / (final - start)) * 100
                    print dtype + str(device_id) + " --> Tasks dispatched: " + str(
                        len(devs[dtype][device_id])) + " start: " + str(start) + " finish: " + str(
                        final) + " idle time: " + str(idle_time) + " utilization: " + str(utilization) + "%"

    def dump_schedule_engine_stats(self, filename):
        f = open(filename, "w")
        original_stdout = sys.stdout
        sys.stdout = f
        print "Schedule Makespan: " + str(self.makespan)
        self.get_device_history()
        self.get_device_utilization()
        sys.stdout = original_stdout
        f.close()

    def get_device_history_of_task_components(self):
        print "CPU History"
        for cpu_history in self.device_history['cpu']:
            print cpu_history
        print "GPU History"
        for gpu_history in self.device_history['gpu']:
            print gpu_history

    def plot_gantt_chart(self, title='Gantt Chart', bar_width=0.2, showgrid_x=True, showgrid_y=True, height=600,
                         width=900, ):
        # devs = {"gpu": ['GPU 1', 'GPU 2'], "cpu": ['CPU 1', 'CPU 2']}
        devs = {"gpu": [], "cpu": []}
        for i in range(0, self.mGPU):
            dev_string = "GPU " + str(i)
            devs['gpu'].append(dev_string)
        for i in range(0, self.nCPU):
            dev_string = "CPU " + str(i)
            devs['cpu'].append(dev_string)
        # print devs['gpu'][0]
        xtasks = []
        for dtype in self.device_history:
            for devices in self.device_history[dtype]:
                dev, task_name, task_dag_id, task_start_time, task_finish_time, task_execution_time = devices
                # dev = int(dev)
                print '{} {} {} {}'.format(dev, task_name, task_start_time, task_finish_time)
                # print dtype
                print devs[dtype][dev]
                device = devs[dtype][dev]
                print device
                xtasks.append(dict(Task=device, Start=datetime.datetime.fromtimestamp(task_start_time),
                                   Finish=datetime.datetime.fromtimestamp(task_finish_time),
                                   Name='{} {}'.format(task_name, task_dag_id)))
                fig = ff.create_gantt(xtasks, index_col='Name', show_colorbar=True, group_tasks=True, title=title,
                                      bar_width=bar_width, showgrid_x=showgrid_x, showgrid_y=showgrid_y,
                                      height=height, width=width, colors=AC)
                layout = go.Layout(legend=dict(orientation="h"))

        return fig

def plot_gantt_chart_from_file(device_history, filename, nD, title='Gantt Chart', bar_width=0.2, showgrid_x=True, showgrid_y=True, height=600,
                     width=900):
    # devs = {"gpu": ['GPU 1', 'GPU 2'], "cpu": ['CPU 1', 'CPU 2']}
    import random
    import colorsys
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches

    def save_png(fig, filename):
        fig.savefig(filename)
        print "GANTT chart is saved at %s" % filename

    def get_N_HexCol(N=5):

        HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in xrange(N)]
        hex_out = []
        for rgb in HSV_tuples:
            rgb = map(lambda x: int(x * 255), colorsys.hsv_to_rgb(*rgb))
            hex_out.append("".join(map(lambda x: chr(x).encode('hex'), rgb)))
        return hex_out

    def get_N_random_HexColor(N=5):
        HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in xrange(N)]
        hex_out = []
        'rgb(31, 119, 180)'
        indexs = random.sample(range(0, 77), N)
        for i in indexs:
            r = int(ALL_COLORS[i][4:-1].split(",")[0])
            g = int(ALL_COLORS[i][4:-1].split(",")[1])
            b = int(ALL_COLORS[i][4:-1].split(",")[2])
            hex_out.append('#%02x%02x%02x' % (r, g, b))
        return hex_out

    devs = {"gpu": [], "cpu": []}
    for i in range(0, nD):
        dev_string = "GPU " + str(i)
        devs['gpu'].append(dev_string)
    for i in range(0, nD):
        dev_string = "CPU " + str(i)
        devs['cpu'].append(dev_string)
    # print devs['gpu'][0]
    xtasks = []
    xmax = 0
    device_info_list = []
    dev_time = {}
    for dtype in device_history:
        for devices in device_history[dtype]:
            dev, task_name, task_dag_id, task_start_time, task_finish_time, task_execution_time = devices
            kn = dtype + "_" + dev
            task_label = task_name + "(" + task_dag_id + ")"
            kernel_times = [task_label, task_start_time, task_finish_time]
            device_info_list.append(devices)
            if kn not in dev_time:
                dev_time[kn] = []
            if kn in dev_time:
                dev_time[kn].append(kernel_times)
            xmax = max(xmax, task_finish_time)

    colourMap = {}
    # colors = get_N_HexCol(len(device_info_list))
    colors = get_N_random_HexColor(len(device_info_list))

    c = 0


    for k in device_info_list:
        colourMap[k[2]] = colors[c]
        c = c + 1

    # legend_patches = []
    # for kn in colourMap:
    #     patch_color = "#" + colourMap[kn]
    #     legend_patches.append(patches.Patch(color=patch_color, label=str(k[2])))

    fig, ax = plt.subplots(figsize=(20, 10))
    device = 0

    for dev in dev_time:
        for k in dev_time[dev]:
            kname = k[0]
            # patch_color = "#" + colourMap[kname]
            patch_color = colourMap[kname]
            start = k[1]
            finish = k[2]
            y = 5 + device * 5
            x = start
            height = 5
            width = finish - start
            # print kname.split(",")[-1] + " : " + str(x) + "," + str(y) + "," + str(width) + "," + str(height)
            ax.add_patch(patches.Rectangle((x, y), width, height, facecolor=patch_color, edgecolor="#000000",
                                           label=kname.split(",")[-1]))
        device = device + 1
    plt.legend(loc=1)
    ax.autoscale(True)
    x_length = float(get_max(device_info_list))
    ax.set_xlim(0, 1.2 * x_length)
    ax.set_ylim(0, len(dev_time) * 10, True, True)
    labels = [item.get_text() for item in ax.get_yticklabels()]
    labels[0] = ""
    i = 1
    for dev in dev_time:
        labels[i] = (dev)
        i = i + 1

    y_ticks = np.arange(2.5, 2.5 + 5 * (1 + len(dev_time)), 5)

    plt.yticks(y_ticks.tolist(), labels)
    ax.set_yticklabels(labels)
    ax.set_xlabel('time ( in second )')
    ax.set_ylabel('devices')
    ax.set_yticklabels(labels)

    save_png(fig, filename)




def plot_gantt_chart_graph(device_history, filename):
    """
    Plots Gantt Chart and Saves as png.
    :param device_history: Dictionary Structure containing timestamps of every kernel on every device
    :type device_history: dict
    :param filename: Name of file where the gantt chart is saved. The plot is saved in gantt_charts folder.
    :type filename: String
    """
    import random
    import colorsys
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches

    def save_png(fig, filename):
        fig.savefig(filename)
        print "GANTT chart is saved at %s" % filename

    def get_N_HexCol(N=5):

        HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in xrange(N)]
        hex_out = []
        for rgb in HSV_tuples:
            rgb = map(lambda x: int(x * 255), colorsys.hsv_to_rgb(*rgb))
            hex_out.append("".join(map(lambda x: chr(x).encode('hex'), rgb)))
        return hex_out

    def get_N_random_HexColor(N=5):
        HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in xrange(N)]
        hex_out = []
        'rgb(31, 119, 180)'
        indexs = random.sample(range(0, 77), N)
        for i in indexs:
            r = int(ALL_COLORS[i][4:-1].split(",")[0])
            g = int(ALL_COLORS[i][4:-1].split(",")[1])
            b = int(ALL_COLORS[i][4:-1].split(",")[2])
            hex_out.append('#%02x%02x%02x' % (r, g, b))
        return hex_out

    def list_from_file(file):
        device_info_list = []
        dev_data = open(file, "r")
        for line in dev_data:
            if "HOST_EVENT" in line:
                d_list = line.split(" ")[1:]
                device_info_list.append(d_list)
        return device_info_list

    def list_from_dev_history(dev_history):
        device_info_list = []
        for his in dev_history:
            device_info_list.append(his.split(" ")[1:])
        return device_info_list

    def get_min(device_info_list):
        g_min = Decimal('Infinity')
        for item in device_info_list:
            n = Decimal(min(item[3:], key=lambda x: Decimal(x)))
            if g_min > n:
                g_min = n
        return g_min

    def get_max(device_info_list):
        g_max = -1
        for item in device_info_list:
            x = Decimal(max(item[3:], key=lambda x: Decimal(x)))
            if g_max < x:
                g_max = x
        return g_max

    def normalise_timestamp(device_info_list):
        min_t = get_min(device_info_list)
        for item in device_info_list:
            for i in range(len(item) - 3):
                item[i + 3] = Decimal(item[i + 3]) - min_t
        return device_info_list

    device_info_list = normalise_timestamp(list_from_dev_history(device_history))

    colourMap = {}
    # colors = get_N_HexCol(len(device_info_list))
    colors = get_N_random_HexColor(len(device_info_list))

    c = 0
    dev_time = {}
    for k in device_info_list:
        kn = k[0] + "_" + k[1]

        kernel_times = [k[2], k[3], k[-1]]
        if kn not in dev_time:
            dev_time[kn] = []
        if kn in dev_time:
            dev_time[kn].append(kernel_times)

    for k in device_info_list:
        colourMap[k[2]] = colors[c]
        c = c + 1

    # legend_patches = []
    # for kn in colourMap:
    #     patch_color = "#" + colourMap[kn]
    #     legend_patches.append(patches.Patch(color=patch_color, label=str(k[2])))

    fig, ax = plt.subplots(figsize=(20, 10))
    device = 0
    #print dev_time
    for dev in dev_time:
        for k in dev_time[dev]:
            kname = k[0]
            # patch_color = "#" + colourMap[kname]
            patch_color = colourMap[kname]
            start = k[1]
            finish = k[2]
            y = 5 + device * 5
            x = start
            height = 5
            width = finish - start
            # print kname.split(",")[-1] + " : " + str(x) + "," + str(y) + "," + str(width) + "," + str(height)
            ax.add_patch(patches.Rectangle((x, y), width, height, facecolor=patch_color, edgecolor="#000000",
                                           label=kname.split(",")[-1]))
        device = device + 1
    plt.legend(loc=1)
    ax.autoscale(True)
    x_length = float(get_max(device_info_list))
    ax.set_xlim(0, 1.2 * x_length)
    ax.set_ylim(0, len(dev_time) * 10, True, True)
    labels = [item.get_text() for item in ax.get_yticklabels()]
    labels[0] = ""
    i = 1
    for dev in dev_time:
        labels[i] = (dev)
        i = i + 1

    y_ticks = np.arange(2.5, 2.5 + 5 * (1 + len(dev_time)), 5)

    plt.yticks(y_ticks.tolist(), labels)
    ax.set_yticklabels(labels)
    ax.set_xlabel('time ( in second )')
    ax.set_ylabel('devices')
    ax.set_yticklabels(labels)

    save_png(fig, filename)


class CLTrainer:

    def __init__(self, ex_map, global_map,filter_list=[],name=None):
        self.key_featvector_map = {}
        self.test_key_featvector_map = {}
        self.test_key_target_map = {}
        self.accuracy = 0.0
        self.sample_features = []
        self.sample_targets = []
        self.test_keys = filter_list
        self.key_list = []
        self.model = None
        self.name=name
        ex_cpu, ex_gpu = ex_map
        
        for key in global_map.keys():
            
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")

            dimension = obtain_kernel_dimension(key)
            feat_dict = create_feat_dict(key, global_map)
            extime = 0.0
            

            
            # if value_int(feat_dict['Class']) < 5:
            #     feat_dict['Class'] = "CPU"
            #     extime = ex_cpu[key]
                
            # else:
            #     feat_dict['Class'] = "GPU"
            #     extime = ex_gpu[key]
            

            if ex_cpu[key] < ex_gpu[key]:
                extime = ex_cpu[key]
                feat_dict['Class']= "CPU"
            else:
                extime = ex_gpu[key]
                feat_dict['Class']= "GPU"

            simtask = SimTask(key, 0, 0, feat_dict, extime)
            feature_vector = []
            for feat in feat_dict.keys():
                if feat!= "Class":
                    feature_vector.append(float(feat_dict[feat]))
            
            if key not in filter_list:
                self.key_featvector_map[key] = feature_vector
                self.key_list.append(key)
                self.sample_features.append(np.asarray(feature_vector))
                self.sample_targets.append(feat_dict['Class']) 
            
            else:
                self.test_key_featvector_map[key] = np.asarray(feature_vector)
                self.test_key_target_map[key] = feat_dict['Class']        

            # self.sample_features = np.asarray(self.sample_features)
            # self.sample_targets = np.asarray(self.sample_targets)
          

    def predict_partition_classes(self):
        test_target_map = {}
        count = 0
        # print self.name
        file_name = "GraphsNewAccuracy/accuracy_" + self.name.split(".")[0]+".stats"
        # print file_name
        # pc_file = open(file_name,'w')
        for key in self.test_key_featvector_map.keys():
            test_feature_vector = self.test_key_featvector_map[key]
            
            test_partition_class = self.model.predict([test_feature_vector])
            if test_partition_class[0] == self.test_key_target_map[key]:
                count +=1

            print "Test Predicted vs Optimal Partition Class",key, test_partition_class[0], self.test_key_target_map[key]
            test_target_map[key] = test_partition_class[0]
            line = key + ":" + test_partition_class[0]+"\n"
            # print line
            # pc_file.write(line)
        self.accuracy = float(count)/float(len(self.test_key_featvector_map.keys()))
        # pc_file.close()
        return test_target_map

    
    def feature_selection(self, num_features = 2):
        from sklearn.feature_selection import SelectKBest
        from sklearn.feature_selection import chi2
        
        self.sample_features = SelectKBest(chi2, k=num_features).fit_transform(self.sample_features, self.sample_targets)
        print "Reduced sample features shape", self.sample_features.shape

    def split_dataset(self, train_percentage):
        from sklearn.model_selection import train_test_split
        train_x, test_x, train_y, test_y = train_test_split(self.sample_features, self.sample_targets, train_size=train_percentage)
        return train_x, test_x, train_y, test_y

    def model_accuracy(self, train_x, test_x, train_y, test_y):
        from sklearn.metrics import accuracy_score
        from sklearn.metrics import confusion_matrix

        predictions = self.model.predict(test_x)

        print "Training Accuracy: ", accuracy_score(train_y, self.model.predict(train_x))
        print "Testing Accuracy: ", accuracy_score(test_y, predictions)

    def model_cv_accuracy(self):
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(self.model, self.sample_features, self.sample_targets, cv=5)
        print "Cross Validation Accuracy: ", scores.mean(), "+/-", scores.std()*2

    def model_cv_accuracy_with_feature_selection(self, num_features):
        from sklearn.model_selection import cross_val_score
        from sklearn.feature_selection import SelectKBest
        from sklearn.feature_selection import chi2
        from sklearn.feature_selection import f_classif
        
        for num_feat in range(2, num_features):
            reduced_features = SelectKBest(chi2, k=num_features).fit_transform(self.sample_features, self.sample_targets)
            scores = cross_val_score(self.model, reduced_features, self.sample_targets, cv=10)
            print "Cross Validation Accuracy for ",num_feat, "features", scores.mean(), "+/-", scores.std()*2

    def model_accuracy_with_smote(self):
        from imblearn.over_sampling import SMOTE
        from sklearn.model_selection import cross_val_score
        self.sample_features = np.asarray(self.sample_features)
        self.sample_targets = np.asarray(self.sample_targets)
        from sklearn.ensemble import RandomForestClassifier
        self.model = RandomForestClassifier()

        sm = SMOTE(random_state=12, ratio = 'minority')

        total_features = self.sample_features
        total_targets = self.sample_targets

        for key in self.test_key_featvector_map.keys():
            np.vstacK(total_features,self.test_key_featvector_map[key])
            np.vstack(total_targets, self.test_key_target_map[key])
        
        # x_res, y_res = sm.fit_sample(self.sample_features, self.sample_targets)

        x_res, y_res = sm.fit_sample(total_features, total_targets)

        itercount = 0
        delete_indices = []
        for element in x_res:
            for key in self.test_key_featvector_map.keys():
                if np.array_equal(element, self.test_key_featvector_map[key]):
                    delete_indices.append(itercount)
            itercount +=1
        x_res = np.delete(x_res,delete_indices,0)
        y_res = np.delete(y_res,delete_indices,0)



        scores = cross_val_score(self.model, x_res, y_res, cv=10)
        print "Cross Validation Accuracy: ", scores.mean(), "+/-", scores.std()*2
        


    def train_classifier(self,classifier_name):
        self.filter()
        
        self.sample_features = np.asarray(self.sample_features)
        self.sample_targets = np.asarray(self.sample_targets)
        from imblearn.over_sampling import SMOTE
        sm = SMOTE(random_state=12, ratio = 'minority')
        
        if classifier_name == "RandomForest":
            from sklearn.ensemble import RandomForestClassifier
            self.model = RandomForestClassifier(n_estimators=1000)
            total_features = self.sample_features
            total_targets = self.sample_targets
            # print total_features.shape
            # print total_targets.shape
            for key in self.test_key_featvector_map.keys():
                # print self.test_key_featvector_map[key].shape
                total_features = np.vstack((total_features, self.test_key_featvector_map[key]))
                total_targets = np.append(total_targets, self.test_key_target_map[key])
        
            # x_res, y_res = sm.fit_sample(self.sample_features, self.sample_targets)

            x_res, y_res = sm.fit_sample(total_features, total_targets)
            # print x_res.shape
            itercount = 0
            delete_indices = []
            for element in x_res:
                for key in self.test_key_featvector_map.keys():
                    if np.array_equal(element, self.test_key_featvector_map[key]):
                        delete_indices.append(itercount)
                itercount +=1
        
            x_res = np.delete(x_res,delete_indices,0)
            y_res = np.delete(y_res,delete_indices,0)
            # print x_res.shape
            # x_res, y_res = sm.fit_sample(self.sample_features, self.sample_targets)
            self.model.fit(x_res, y_res)


    def train_test_classifier(self,classifier_name, train_percentage,cross_validation=False,num_features=4):

        self.sample_features = np.asarray(self.sample_features)
        self.sample_targets = np.asarray(self.sample_targets)

        if cross_validation:
            if classifier_name == "RandomForest":
                from sklearn.ensemble import RandomForestClassifier
                print "Training for ", num_features, " features"
                # self.feature_selection(num_features)
                self.model = RandomForestClassifier(n_estimators=1000)
                self.model_cv_accuracy_with_feature_selection(num_features)
            
            if classifier_name == "MLP":
                from sklearn.neural_network import MLPClassifier
                print "Training for ", num_features, " features"
                # self.feature_selection(num_features)
                self.model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
                self.model_cv_accuracy_with_feature_selection(num_features)
            
            if classifier_name == "LogisticRegression":
                from sklearn.linear_model import LogisticRegression
                print "Training for ", num_features, " features"
                # self.feature_selection(num_features)
                self.model = LogisticRegression()
                self.model_cv_accuracy_with_feature_selection(num_features)
            
              
        else:
            
            train_x, test_x, train_y, test_y = self.split_dataset(train_percentage)
            print "Train_x Shape :: ", train_x.shape
            print "Train_y Shape :: ", train_y.shape
            print "Test_x Shape :: ", test_x.shape
            print "Test_y Shape :: ", test_y.shape
            self.model = None

            if classifier_name == "RandomForest":
                from sklearn.ensemble import RandomForestClassifier
                self.model = RandomForestClassifier()
                self.model.fit(train_x, train_y)
                
            self.model_accuracy(train_x, test_x, train_y, test_y)

    
    def dataset_stats(self):
        print "Number of samples", len(self.sample_features)
        num_cpu_samples = 0
        num_gpu_samples = 0
        for target in self.sample_targets:
            if target == "CPU":
                num_cpu_samples +=1
            else:
                num_gpu_samples +=1
        

        print "Class counts (CPU, GPU) ", num_cpu_samples, num_gpu_samples
        

    def filter(self,work_item_range=[1024,2048,4096,8192]):
        
        new_key_list = []
        new_sample_features = []
        new_sample_targets = []
        
        for index in range(len(self.key_list)):
            # print index
            key = self.key_list[index]
            kernel_info = key.split("_")
            kernelName = kernel_info[0]
            worksize = kernel_info[1].strip("\n")
            if kernelName == 'uncoalesced' or kernelName == 'shared':
                kernelName = kernelName + "_copy"
                worksize = kernel_info[2].strip("\n")
            if kernelName == 'transpose':
                kernelName = kernelName + "_naive"
                worksize = kernel_info[2].strip("\n")
            
            work_item = int(worksize)
            # print work_item
            if work_item  in work_item_range:
                new_sample_features.append(self.sample_features[index])
                new_sample_targets.append(self.sample_targets[index])
            else:
                self.key_featvector_map.pop(key)    
        
        self.key_list = list(self.key_featvector_map.keys())
        self.sample_features = new_sample_features
        self.sample_targets = new_sample_targets
        

#######################################################################################################

if __name__ == '__main__':
    global_map = {}
    global_file_list = open(sys.argv[1], "r").readlines()
    global_map = make_dict(global_file_list)
    # print global_map
    # ex_map = dict()
    # Testing dag creation and rank functions
    # TODO: Parameterized DAG generation
    # DAG = create_dag(5, global_map, ex_map, 0)
    # DAG.print_information()
    # compute_rank_ECO_DT(DAG, 0)
    # DAG.print_ranks()
    ex_cpu = extract_dict_from_pickle("Stats/execCPU.pickle")
    ex_gpu = extract_dict_from_pickle("Stats/execGPU.pickle")
    ex_map = (ex_cpu, ex_gpu)

    # Testing schedule_engine with sample dags
    DC = DAGCreator()
    # dag_greedy_contraction = []
    # dag = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
    # dag_greedy_contraction.append(dag)
    # SE = ScheduleEngine(4, 4, "upward_rank_ECO_DT", ex_map)
    # SE.schedule_task_sets_greedy_contraction_execution_time(dag_greedy_contraction)
    # dag_name = "greedy_contraction_test" + ".png"
    # DC.dump_graph_start_finish_times(dag, dag_name)
    # dump_string = "greedy_contraction_test.stats"
    # SE.dump_schedule_engine_stats(dump_string)
    # dags_baseline = []
    # dag = DC.create_sample_dag(0, ex_map, "sample_dag.graph")

    # dag0 = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
    # dag1 = DC.create_dag_from_file("sample_dag2.graph", 1, global_map, ex_map)
    # dag2 = DC.create_dag_from_file("sample_dag2.graph", 2, global_map, ex_map)
    # dag3 = DC.create_dag_from_file("sample_dag2.graph", 3, global_map, ex_map)
    # # DC.dump_graph_names(dag, "sample_dag2_names.png")
    # # DC.dump_graph_class(dag, "sample_dag2_class.png")
    # # DC.dump_graph_DT_ECO(dag, "sample_dag2_DT_ECO.png")
    # # DC.dump_graph_execution_times(dag, "sample_dag2_extime.png")
    #
    # SE = ScheduleEngine(2, 2, "DT_next_level", ex_map)
    # SE.schedule_task_sets_contraction([dag0, dag1, dag2, dag3])
    # SE.dump_schedule_engine_stats("h2_sample_schedule3_2_contraction.stats")
    # counter = 0
    # for dag in [dag0, dag1, dag2, dag3]:
    #     dag_name = "H2_DAG_" + str(counter) + "_contraction2.png"
    #     DC.dump_graph_start_finish_times(dag, dag_name)
    #     counter += 1
    # fig = SE.plot_gantt_chart()
    # dag_name = "h2_test_multiple2_contraction"
    # py.image.save_as(fig, '%s.png' % (dag_name))
    #


    # =======================================================================================================
    # Heuristics 1 and 2 + Baseline + greedy

    f = open("Results/Experiment/makespans_multiple.txt", "a")
    for num_devices in range(2, 7):
        # dag0 = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
        # dag1 = DC.create_dag_from_file("sample_dag2.graph", 1, global_map, ex_map)
        # dag2 = DC.create_dag_from_file("sample_dag2.graph", 2, global_map, ex_map)
        # dag3 = DC.create_dag_from_file("sample_dag2.graph", 3, global_map, ex_map)
        #
        # dag0_c1 = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
        # dag1_c1 = DC.create_dag_from_file("sample_dag2.graph", 1, global_map, ex_map)
        # dag2_c1 = DC.create_dag_from_file("sample_dag2.graph", 2, global_map, ex_map)
        # dag3_c1 = DC.create_dag_from_file("sample_dag2.graph", 3, global_map, ex_map)
        #
        # dag0_c2 = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
        # dag1_c2 = DC.create_dag_from_file("sample_dag2.graph", 1, global_map, ex_map)
        # dag2_c2 = DC.create_dag_from_file("sample_dag2.graph", 2, global_map, ex_map)
        # dag3_c2 = DC.create_dag_from_file("sample_dag2.graph", 3, global_map, ex_map)
        #
        # dag0_c3 = DC.create_dag_from_file("sample_dag2.graph", 0, global_map, ex_map)
        # dag1_c3 = DC.create_dag_from_file("sample_dag2.graph", 1, global_map, ex_map)
        # dag2_c3 = DC.create_dag_from_file("sample_dag2.graph", 2, global_map, ex_map)
        # dag3_c3 = DC.create_dag_from_file("sample_dag2.graph", 3, global_map, ex_map)
        #

        dag0 = DC.create_dag_from_file("test1.graph", 0, global_map, ex_map)
        dag1 = DC.create_dag_from_file("test2.graph", 1, global_map, ex_map)
        dag2 = DC.create_dag_from_file("test3.graph", 2, global_map, ex_map)
        dag3 = DC.create_dag_from_file("test4.graph", 3, global_map, ex_map)

        dag0_c1 = DC.create_dag_from_file("test1.graph", 0, global_map, ex_map)
        dag1_c1 = DC.create_dag_from_file("test2.graph", 1, global_map, ex_map)
        dag2_c1 = DC.create_dag_from_file("test3.graph", 2, global_map, ex_map)
        dag3_c1 = DC.create_dag_from_file("test4.graph", 3, global_map, ex_map)

        dag0_c2 = DC.create_dag_from_file("test1.graph", 0, global_map, ex_map)
        dag1_c2 = DC.create_dag_from_file("test2.graph", 1, global_map, ex_map)
        dag2_c2 = DC.create_dag_from_file("test3.graph", 2, global_map, ex_map)
        dag3_c2 = DC.create_dag_from_file("test4.graph", 3, global_map, ex_map)

        dag0_c3 = DC.create_dag_from_file("test1.graph", 0, global_map, ex_map)
        dag1_c3 = DC.create_dag_from_file("test2.graph", 1, global_map, ex_map)
        dag2_c3 = DC.create_dag_from_file("test3.graph", 2, global_map, ex_map)
        dag3_c3 = DC.create_dag_from_file("test4.graph", 3, global_map, ex_map)

        dag_objects = [dag0, dag1, dag2, dag3]

        dag_objects_contraction = [dag0_c1, dag1_c1, dag2_c1, dag3_c1]

        dag_objects_contraction2 = [dag0_c2, dag1_c2, dag2_c2, dag3_c2]

        dag_objects_contraction3 = [dag0_c3, dag1_c3, dag2_c3, dag3_c3]
        counter = 0

        SE = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
        SE.schedule_task_sets(dag_objects)
        dump_string = "Results/Experiment/Result_baseline_dt_next_level_4DAGs" + "_" + str(num_devices) + "_CPU_" + str(
            num_devices) + "_GPU.stats"
        SE.dump_schedule_engine_stats(dump_string)

        SE_contraction = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
        SE_contraction.schedule_task_sets_contraction(dag_objects_contraction)
        dump_string_contraction = "Results/Experiment/result_contractionh1_dt_next_level_4DAGs_" + "_" + str(
            num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
        SE_contraction.dump_schedule_engine_stats(dump_string_contraction)

        SE_contraction2 = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
        SE_contraction2.schedule_task_sets_contraction_execution_time(dag_objects_contraction2)
        dump_string_contraction2 = "Results/Experiment/result_contractionh2_dt_next_level_4DAGs_" + "_" + str(
            num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
        SE_contraction2.dump_schedule_engine_stats(dump_string_contraction2)

        SE_contraction3 = ScheduleEngine(num_devices, num_devices, "upward_rank_ECO_DT", ex_map)
        SE_contraction3.schedule_task_sets_greedy_contraction_execution_time(dag_objects_contraction3)
        dump_string_contraction3 = "Results/Experiment/result_contractionh3_dt_next_level_4DAGs_" + "_" + str(
            num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
        SE_contraction3.dump_schedule_engine_stats(dump_string_contraction3)

        i = 0
        for dag in dag_objects:
            dag_name = "Results/Experiment/num_devices_" + str(num_devices) + "DAG" + str(i) + ".png"
            DC.dump_graph_start_finish_times(dag, dag_name)
            i = i + 1
        i = 0
        for dag in dag_objects_contraction:
            dag_name = "Results/Experiment/num_devices_" + str(num_devices) + "DAG" + str(i) + "_contractionH1.png"
            DC.dump_graph_start_finish_times(dag, dag_name)
            i = i + 1
        i = 0
        for dag in dag_objects_contraction2:
            dag_name = "Results/Experiment/num_devices_" + str(num_devices) + "DAG" + str(i) + "_contractionH2.png"
            DC.dump_graph_start_finish_times(dag, dag_name)
            i = i + 1

        i = 0
        for dag in dag_objects_contraction3:
            dag_name = "Results/Experiment/num_devices_" + str(num_devices) + "DAG" + str(i) + "_contractionH3.png"
            DC.dump_graph_start_finish_times(dag, dag_name)
            i = i + 1
        speedup1 = SE.time_stamp / SE_contraction.time_stamp
        speedup2 = SE.time_stamp / SE_contraction2.time_stamp
        speedup3 = SE.time_stamp / SE_contraction3.time_stamp

        print >> f, str(num_devices) + " CPUs and GPUs " + "for 4 DAGs" + "--> Baseline: " + str(
            SE.time_stamp) + " ContractionH1: " + str(SE_contraction.time_stamp) + " ContractionH2: " + str(
            SE_contraction2.time_stamp) + " ContractionH3: " + str(SE_contraction3.time_stamp) + " Speedup1: " + str(
            speedup1) + "Speedup2: " + str(speedup2) + " Speedup3: " + str(speedup3) + "\n"

    f.close()

    # py.iplot(fig)
    # DC.dump_graph_start_finish_times(dag, "sample_dag2_contraction_sftime.png")
    # dags = ["DAGs/dag_100_0.5_0.2_0.5.graph", "DAGs/dag_50_0.5_0.2_0.5.graph", "DAGs/dag_70_0.4_0.4_0.5.graph", "DAGs/dag_100_0.4_0.4_0.5.graph", "DAGs/dag_100_0.4_0.5_0.3.graph"]
    # counter = 0
    # Code for testing single dags
    # f = open("Results/Single/makespans.txt", "a")
    # for dag in dags:
    #     for num_devices in range(2, 7):
    #         dag_object = DC.create_dag_from_file(dag, 0, global_map, ex_map)
    #         dag_object_contraction = DC.create_dag_from_file(dag, 0, global_map, ex_map)
    #         SE = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
    #         SE.schedule_task_sets([dag_object])
    #
    #         dump_string = "Results/result_baseline_dt_next_level_1DAG_" + str(counter) + "_" + str(num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
    #         SE.dump_schedule_engine_stats(dump_string)
    #         DC.dump_graph(dag_object, dump_string[:-5] + ".png")
    #         SE_contraction = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
    #         SE_contraction.schedule_task_sets_contraction([dag_object_contraction])
    #         dump_string_contraction = "Results/result_contraction_dt_next_level_1DAG_" + str(counter) + "_" + str(num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
    #         SE_contraction.dump_schedule_engine_stats(dump_string)
    #         DC.dump_graph(dag_object_contraction, dump_string_contraction[:-5] + ".png")
    #         speedup = SE.time_stamp / SE_contraction.time_stamp
    #         print >> f, str(num_devices) + "CPUs and GPUs " + "for DAG" + str(counter) + "--> Baseline: " + str(SE.time_stamp) + " Contraction: " + str(SE_contraction.time_stamp) + "Speedup: " + str(speedup) + "\n"
    #     counter = counter + 1
    # f.close()
    # Code for testing multiple dags
    # f = open("Results/Multiple/makespans_multiple.txt", "a")
    # for num_devices in range(2, 7):
    #     dag_objects = []
    #     dag_objects_contraction = []
    #     counter = 0
    #     for dag in dags:
    #         dag_objects.append(DC.create_dag_from_file(dag, counter, global_map, ex_map))
    #         dag_objects_contraction.append(DC.create_dag_from_file(dag, counter, global_map, ex_map))
    #         counter = counter + 1
    #     SE = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
    #     SE.schedule_task_sets(dag_objects)
    #     dump_string = "Results/Multiple/Result_baseline_dt_next_level_5DAGs_" + str(counter) + "_" + str(num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
    #     SE.dump_schedule_engine_stats(dump_string)
    #     SE_contraction = ScheduleEngine(num_devices, num_devices, "DT_next_level", ex_map)
    #     SE_contraction.schedule_task_sets_contraction(dag_objects_contraction)
    #     dump_string_contraction = "Results/Multiple/result_contraction_dt_next_level_5DAGs_" + str(counter) + "_" + str(num_devices) + "_CPU_" + str(num_devices) + "_GPU.stats"
    #     SE_contraction.dump_schedule_engine_stats(dump_string)
    #     i = 0
    #     for d in dag_objects_contraction:
    #         DC.dump_graph(d, dump_string_contraction[:-5] + "_" + str(i) + ".png")
    #         i = i + 1
    #     speedup = SE.time_stamp / SE_contraction.time_stamp
    #     print >> f, str(num_devices) + "CPUs and GPUs " + "for 5 DAGs" + "--> Baseline: " + str(SE.time_stamp) + " Contraction: " + str(SE_contraction.time_stamp) + "Speedup: " + str(speedup) + "\n"
    #
    # f.close()

    # dag0 = DC.create_dag_from_file("DAGs/dag_100_0.5_0.2_0.5.graph", 0, global_map, ex_map)
    # dag1 = DC.create_dag_from_file("DAGs/dag_50_0.5_0.2_0.5.graph", 0, global_map, ex_map)
    # dag2 = DC.create_dag_from_file("DAGs/dag_70_0.4_0.4_0.5.graph", 0, global_map, ex_map)
    # dag3 = DC.create_dag_from_file("DAGs/dag_100_0.4_0.4_0.5.graph", 0, global_map, ex_map)
    # dag4 = DC.create_dag_from_file("DAGs/dag_100_0.4_0.5_0.3.graph", 0, global_map, ex_map)
    # dags_baseline.append(dag0)
    # dags_baseline.append(dag1)
    # dags_baseline.append(dag2)
    # dags_baseline.append(dag3)
    # dags_baseline.append(dag4)
    # SE1 = ScheduleEngine(4, 4, "DT_next_level", ex_map)
    # SE1.schedule_task_sets(dags_baseline)
    # SE1.dump_schedule_engine_stats("Results/result_baseline_dt_next_level_1DAGs4_4CPU_4GPU.stats")
    #
    # dags_contraction = []
    # dag0_c = DC.create_dag_from_file("DAGs/dag_100_0.5_0.2_0.5.graph", 0, global_map, ex_map)
    # dag1_c = DC.create_dag_from_file("DAGs/dag_50_0.5_0.2_0.5.graph", 0, global_map, ex_map)
    # dag2_c = DC.create_dag_from_file("DAGs/dag_70_0.4_0.4_0.5.graph", 0, global_map, ex_map)
    # dag3_c = DC.create_dag_from_file("DAGs/dag_100_0.4_0.4_0.5.graph", 0, global_map, ex_map)
    # dag4_c = DC.create_dag_from_file("DAGs/dag_100_0.4_0.5_0.3.graph", 0, global_map, ex_map)
    # dags_contraction.append(dag0_c)
    # dags_contraction.append(dag1_c)
    # dags_contraction.append(dag2_c)
    # dags_contraction.append(dag4_c)
    # dags_contraction.append(dag4_c)

    # SE2 = ScheduleEngine(4, 4, "DT_next_level", ex_map)
    # SE2.schedule_task_sets_contraction(dags_contraction)
    # SE2.dump_schedule_engine_stats("Results/result_contraction_dt_next_level_1DAGs3_4CPU_4GPU.stats")
    # for dag in dags_contraction:
    #     DC.visualize_graph(dag)
    # dag_test = DC.create_dag_randomly(4, ex_map, 100, 0.4, 0.5, 0.3)
    # DC.visualize_graph(dag_test)
    # DC = DAGCreator()
    # dag0 = DC.create_dag_from_file("DAGs/dag_100_0.5_0.2_0.5.graph", 0, global_map, ex_map)
    # dag1 = DC.create_dag_from_file("DAGs/dag_50_0.5_0.2_0.5.graph", 1, global_map, ex_map)
    # dag2 = DC.create_dag_from_file("DAGs/dag_70_0.4_0.4_0.5.graph", 2, global_map, ex_map)
    # dag3 = DC.create_dag_from_file("DAGs/dag_100_0.4_0.4_0.5.graph", 0, global_map, ex_map)
    # # dags.append(dag0)
    # # dags.append(dag1)
    # # dags.append(dag2)
    # dags.append(dag3)
    #
    # SE = ScheduleEngine(4, 4, "DT_next_level")
    # # SE.schedule_task_components(dags)
    # # SE.get_device_history_of_task_components()
    # # SE.schedule_task_components_DT_contraction_level1(dags)
    # SE.schedule_task_sets(dags)
    # SE.get_device_history()
    # SE.get_device_utilization()
    # for dag in dags:
    #     DC.visualize_graph(dag)
    # # SE.get_device_history_of_task_components()
    # dag0 = DC.create_dag_randomly(0, ex_map, 70, 0.4, 0.4, 0.5)
    # dag0 = DC.create_dag_from_file("DAGs/cnn.graph", 0, ex_map)
    # dag1 = DC.create_dag_from_file("DAGs/cnn.graph", 1, ex_map)
    # dags.append(dag0)
    # dags.append(dag1)
    # SE = ScheduleEngine(4, 4)
    # SE.schedule_task_components(dags)
    # SE.get_device_history_of_task_components()
    # r = Ranker(dags, ex_map)
    # r.compute_rank_dags("downward_rank_ECO_DT")
    # dag0.initialize_task_component_ranks("downward_rank_ECO_DT")
    # dag0.print_information("downward_rank_ECO_DT")
    # nodes = dag0.G.nodes()
    # print nodes
    # dag0.merge_independent_task_components(nodes[1], nodes[2])
    # dag_level = dag0.make_levels(dag0.G)
    # dag0.merge_task_list(dag_level[0])

    # cc = dag0.get_connected_components_k_level(dag0.free_task_components, 1)
    # dag0.print_cc_information(cc)
    # for component in cc:
    #     dag0.merge_task_list(component.nodes())
    # dag0.print_information("downward_rank_ECO_DT")
    # # dag1.print_information()

    # DC.visualize_graph(dag0)

    # dag0.add_dummy_node()
    # compute_rank_ECO_DT(dag0, 7)
    # compute_rank_EXTime(dag0, 7, ex_map)
    # dag0.print_ranks()
    # dag1.print_ranks()cluster
    # levels = dag0.make_levels()
    # print levels
'''
    # DAG 0
    dag0_skeleton = nx.DiGraph()
    dag0_skeleton.add_node(0)
    dag0_skeleton.add_node(1)
    dag0_skeleton.add_node(2)
    dag0_skeleton.add_node(3)
    dag0_skeleton.add_node(4)
    dag0_skeleton.add_edges_from([(0, 1), (0, 2), (0, 3), (1, 4), (2, 4), (3, 4)])
    dag0 = create_dag_from_graph(dag0_skeleton, global_map, ex_map, 0)

    # DAG 1
    dag1_skeleton = nx.DiGraph()
    dag1_skeleton.add_node(0)
    dag1_skeleton.add_node(1)
    dag1_skeleton.add_node(2)
    dag1_skeleton.add_node(3)
    dag1_skeleton.add_edges_from([(0, 1), (1, 2), (2, 3)])
    dag1 = create_dag_from_graph(dag1_skeleton, global_map, ex_map, 1)
    # dags.append(dag0)
    # dags.append(dag1)
    # SE = ScheduleEngine(2, 2)
    # SE.schedule(dags)
    # fig = SE.plot_gantt_chart()
    # dag_name = "test"
    # py.image.save_as(fig, '%s.png' % (dag_name))
    # py.iplot(fig)
'''
